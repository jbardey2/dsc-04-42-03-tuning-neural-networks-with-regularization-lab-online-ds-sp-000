{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Consumer Loan</td>\n",
       "      <td>Due to an accident resulting in a total loss o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Credit card</td>\n",
       "      <td>Capital One continues to list hidden charges f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Nationstar Mortgage took over my loan in XXXX ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>Safe Deposit Box, of XXXX XXXX XXXX XXXX. \\nXX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>XXXX XXXX XXXX XXXX XXXX XXXX issued a garnism...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Product                       Consumer complaint narrative\n",
       "0            Consumer Loan  Due to an accident resulting in a total loss o...\n",
       "1              Credit card  Capital One continues to list hidden charges f...\n",
       "2                 Mortgage  Nationstar Mortgage took over my loan in XXXX ...\n",
       "3  Bank account or service  Safe Deposit Box, of XXXX XXXX XXXX XXXX. \\nXX...\n",
       "4             Student loan  XXXX XXXX XXXX XXXX XXXX XXXX issued a garnism..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here\n",
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = df.Product\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Due to an accident resulting in a total loss o...\n",
       "1    Capital One continues to list hidden charges f...\n",
       "2    Nationstar Mortgage took over my loan in XXXX ...\n",
       "3    Safe Deposit Box, of XXXX XXXX XXXX XXXX. \\nXX...\n",
       "4    XXXX XXXX XXXX XXXX XXXX XXXX issued a garnism...\n",
       "Name: Consumer complaint narrative, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complaints[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints) \n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 5, ..., 1, 2, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "le = LabelEncoder()\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product)\n",
    "product_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_onehot = to_categorical(product_cat)\n",
    "product_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, product_onehot, test_size=1500, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 7)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "from keras import models, layers, optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(units=50, activation=\"relu\", input_shape=(2000,)))\n",
    "model.add(layers.Dense(units=25, activation=\"relu\"))\n",
    "model.add(layers.Dense(units=7, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jbard\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\jbard\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 107us/step - loss: 1.9352 - acc: 0.1933 - val_loss: 1.9262 - val_acc: 0.1970\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.9096 - acc: 0.2221 - val_loss: 1.9042 - val_acc: 0.2240\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.8868 - acc: 0.2419 - val_loss: 1.8810 - val_acc: 0.2490\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.8604 - acc: 0.2685 - val_loss: 1.8528 - val_acc: 0.2690\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.8286 - acc: 0.3009 - val_loss: 1.8209 - val_acc: 0.3010\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.7926 - acc: 0.3196 - val_loss: 1.7829 - val_acc: 0.3240\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.7526 - acc: 0.3443 - val_loss: 1.7434 - val_acc: 0.3540\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.7098 - acc: 0.3601 - val_loss: 1.7013 - val_acc: 0.3670\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.6654 - acc: 0.3739 - val_loss: 1.6574 - val_acc: 0.3730\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.6197 - acc: 0.3852 - val_loss: 1.6126 - val_acc: 0.3860\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.5735 - acc: 0.4007 - val_loss: 1.5675 - val_acc: 0.4070\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.5273 - acc: 0.4203 - val_loss: 1.5222 - val_acc: 0.4250\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.4803 - acc: 0.4439 - val_loss: 1.4772 - val_acc: 0.4390\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.4330 - acc: 0.4717 - val_loss: 1.4312 - val_acc: 0.4780\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.3857 - acc: 0.5020 - val_loss: 1.3857 - val_acc: 0.5080\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.3388 - acc: 0.5337 - val_loss: 1.3411 - val_acc: 0.5330\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.2928 - acc: 0.5603 - val_loss: 1.2975 - val_acc: 0.5550\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.2478 - acc: 0.5879 - val_loss: 1.2556 - val_acc: 0.5780\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.2043 - acc: 0.6092 - val_loss: 1.2133 - val_acc: 0.6020\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.1621 - acc: 0.6276 - val_loss: 1.1748 - val_acc: 0.6190\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1220 - acc: 0.6437 - val_loss: 1.1373 - val_acc: 0.6360\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0835 - acc: 0.6609 - val_loss: 1.1026 - val_acc: 0.6440\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0474 - acc: 0.6727 - val_loss: 1.0725 - val_acc: 0.6510\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0138 - acc: 0.6848 - val_loss: 1.0399 - val_acc: 0.6610\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9821 - acc: 0.6955 - val_loss: 1.0113 - val_acc: 0.6720\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9520 - acc: 0.7047 - val_loss: 0.9873 - val_acc: 0.6750\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9248 - acc: 0.7135 - val_loss: 0.9622 - val_acc: 0.6830\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8989 - acc: 0.7192 - val_loss: 0.9381 - val_acc: 0.6840\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8751 - acc: 0.7245 - val_loss: 0.9177 - val_acc: 0.6900\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8526 - acc: 0.7320 - val_loss: 0.8998 - val_acc: 0.6990\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8325 - acc: 0.7361 - val_loss: 0.8819 - val_acc: 0.7040\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8127 - acc: 0.7412 - val_loss: 0.8649 - val_acc: 0.7080\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7947 - acc: 0.7441 - val_loss: 0.8498 - val_acc: 0.7050\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.7775 - acc: 0.7521 - val_loss: 0.8375 - val_acc: 0.7060\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7619 - acc: 0.7547 - val_loss: 0.8263 - val_acc: 0.7170\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7466 - acc: 0.7583 - val_loss: 0.8135 - val_acc: 0.7090\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.7332 - acc: 0.7649 - val_loss: 0.8018 - val_acc: 0.7210\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7196 - acc: 0.7679 - val_loss: 0.7934 - val_acc: 0.7210\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7065 - acc: 0.7701 - val_loss: 0.7863 - val_acc: 0.7190\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.6958 - acc: 0.7752 - val_loss: 0.7740 - val_acc: 0.7210\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.6843 - acc: 0.7781 - val_loss: 0.7699 - val_acc: 0.7310\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.6738 - acc: 0.7832 - val_loss: 0.7628 - val_acc: 0.7340\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.6640 - acc: 0.7855 - val_loss: 0.7497 - val_acc: 0.7310\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.6541 - acc: 0.7864 - val_loss: 0.7469 - val_acc: 0.7340\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.6448 - acc: 0.7891 - val_loss: 0.7400 - val_acc: 0.7310\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.6363 - acc: 0.7891 - val_loss: 0.7323 - val_acc: 0.7360\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.6276 - acc: 0.7936 - val_loss: 0.7263 - val_acc: 0.7310\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.6191 - acc: 0.7981 - val_loss: 0.7236 - val_acc: 0.7340\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.6119 - acc: 0.7979 - val_loss: 0.7183 - val_acc: 0.7370\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.6044 - acc: 0.8007 - val_loss: 0.7127 - val_acc: 0.7380\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.5967 - acc: 0.8039 - val_loss: 0.7098 - val_acc: 0.7360\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.5905 - acc: 0.8059 - val_loss: 0.7057 - val_acc: 0.7420\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.5834 - acc: 0.8063 - val_loss: 0.6997 - val_acc: 0.7370\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.5765 - acc: 0.8093 - val_loss: 0.6979 - val_acc: 0.7430\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.5702 - acc: 0.8109 - val_loss: 0.6920 - val_acc: 0.7410\n",
      "Epoch 56/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.5646 - acc: 0.8120 - val_loss: 0.6918 - val_acc: 0.7500\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.5588 - acc: 0.8149 - val_loss: 0.6903 - val_acc: 0.7450\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.5529 - acc: 0.8155 - val_loss: 0.6850 - val_acc: 0.7490\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.5465 - acc: 0.8187 - val_loss: 0.6839 - val_acc: 0.7520\n",
      "Epoch 60/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.5417 - acc: 0.8207 - val_loss: 0.6820 - val_acc: 0.7480\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.5362 - acc: 0.8220 - val_loss: 0.6791 - val_acc: 0.7540\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.5305 - acc: 0.8219 - val_loss: 0.6720 - val_acc: 0.7450\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.5248 - acc: 0.8248 - val_loss: 0.6693 - val_acc: 0.7460\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.5205 - acc: 0.8260 - val_loss: 0.6694 - val_acc: 0.7510\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.5156 - acc: 0.8265 - val_loss: 0.6653 - val_acc: 0.7460\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.5111 - acc: 0.8297 - val_loss: 0.6693 - val_acc: 0.7540\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.5060 - acc: 0.8320 - val_loss: 0.6690 - val_acc: 0.7540\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.5013 - acc: 0.8333 - val_loss: 0.6651 - val_acc: 0.7590\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.4972 - acc: 0.8317 - val_loss: 0.6578 - val_acc: 0.7510\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.4922 - acc: 0.8364 - val_loss: 0.6559 - val_acc: 0.7520\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.4881 - acc: 0.8385 - val_loss: 0.6557 - val_acc: 0.7530\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.4836 - acc: 0.8404 - val_loss: 0.6537 - val_acc: 0.7560\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.4799 - acc: 0.8409 - val_loss: 0.6508 - val_acc: 0.7570\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.4754 - acc: 0.8423 - val_loss: 0.6534 - val_acc: 0.7540\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.4715 - acc: 0.8433 - val_loss: 0.6520 - val_acc: 0.7600\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.4675 - acc: 0.8445 - val_loss: 0.6502 - val_acc: 0.7530\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.4636 - acc: 0.8455 - val_loss: 0.6474 - val_acc: 0.7590\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.4596 - acc: 0.8469 - val_loss: 0.6480 - val_acc: 0.7550\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.4555 - acc: 0.8491 - val_loss: 0.6465 - val_acc: 0.7610\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.4520 - acc: 0.8497 - val_loss: 0.6460 - val_acc: 0.7590\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.4484 - acc: 0.8535 - val_loss: 0.6462 - val_acc: 0.7640\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.4447 - acc: 0.8539 - val_loss: 0.6467 - val_acc: 0.7610\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.4410 - acc: 0.8544 - val_loss: 0.6426 - val_acc: 0.7640\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.4373 - acc: 0.8573 - val_loss: 0.6438 - val_acc: 0.7630\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.4336 - acc: 0.8573 - val_loss: 0.6506 - val_acc: 0.7580\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.4307 - acc: 0.8581 - val_loss: 0.6440 - val_acc: 0.7610\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.4269 - acc: 0.8608 - val_loss: 0.6454 - val_acc: 0.7610\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.4236 - acc: 0.8620 - val_loss: 0.6404 - val_acc: 0.7570\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.4203 - acc: 0.8635 - val_loss: 0.6381 - val_acc: 0.7600\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.4170 - acc: 0.8640 - val_loss: 0.6424 - val_acc: 0.7630\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.4140 - acc: 0.8648 - val_loss: 0.6434 - val_acc: 0.7590\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.4108 - acc: 0.8683 - val_loss: 0.6387 - val_acc: 0.7630\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.4072 - acc: 0.8684 - val_loss: 0.6393 - val_acc: 0.7640\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.4043 - acc: 0.8691 - val_loss: 0.6381 - val_acc: 0.7660\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.4011 - acc: 0.8735 - val_loss: 0.6468 - val_acc: 0.7610\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.3983 - acc: 0.8723 - val_loss: 0.6354 - val_acc: 0.7620\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.3949 - acc: 0.8739 - val_loss: 0.6394 - val_acc: 0.7630\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.3921 - acc: 0.8748 - val_loss: 0.6356 - val_acc: 0.7660\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.3897 - acc: 0.8752 - val_loss: 0.6407 - val_acc: 0.7610\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.3867 - acc: 0.8752 - val_loss: 0.6355 - val_acc: 0.7620\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.3835 - acc: 0.8781 - val_loss: 0.6423 - val_acc: 0.7600\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.3807 - acc: 0.8795 - val_loss: 0.6378 - val_acc: 0.7650\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.3779 - acc: 0.8813 - val_loss: 0.6371 - val_acc: 0.7620\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.3760 - acc: 0.8783 - val_loss: 0.6396 - val_acc: 0.7640\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.3720 - acc: 0.8829 - val_loss: 0.6404 - val_acc: 0.7660\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.3694 - acc: 0.8825 - val_loss: 0.6418 - val_acc: 0.7680\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.3669 - acc: 0.8844 - val_loss: 0.6394 - val_acc: 0.7640\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.3646 - acc: 0.8860 - val_loss: 0.6424 - val_acc: 0.7650\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.3619 - acc: 0.8857 - val_loss: 0.6379 - val_acc: 0.7680\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.3589 - acc: 0.8881 - val_loss: 0.6370 - val_acc: 0.7640\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.3562 - acc: 0.8892 - val_loss: 0.6464 - val_acc: 0.7620\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.3542 - acc: 0.8901 - val_loss: 0.6396 - val_acc: 0.7630\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.3511 - acc: 0.8908 - val_loss: 0.6386 - val_acc: 0.7620\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.3485 - acc: 0.8917 - val_loss: 0.6418 - val_acc: 0.7660\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.3462 - acc: 0.8924 - val_loss: 0.6389 - val_acc: 0.7610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.3439 - acc: 0.8928 - val_loss: 0.6389 - val_acc: 0.7640\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.3410 - acc: 0.8952 - val_loss: 0.6404 - val_acc: 0.7640\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.3393 - acc: 0.8957 - val_loss: 0.6423 - val_acc: 0.7630\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.3361 - acc: 0.8977 - val_loss: 0.6401 - val_acc: 0.7600\n",
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.3342 - acc: 0.8981 - val_loss: 0.6374 - val_acc: 0.7660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26cb61d0f98>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_final, label_train_final, batch_size=256, epochs=120, validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 100us/step - loss: 1.9437 - acc: 0.1560 - val_loss: 1.9279 - val_acc: 0.1740\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.9156 - acc: 0.1839 - val_loss: 1.9104 - val_acc: 0.1970\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.8970 - acc: 0.2055 - val_loss: 1.8944 - val_acc: 0.2040\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.8796 - acc: 0.2199 - val_loss: 1.8784 - val_acc: 0.2160\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.8618 - acc: 0.2239 - val_loss: 1.8609 - val_acc: 0.2240\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.8427 - acc: 0.2319 - val_loss: 1.8416 - val_acc: 0.2290\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.8217 - acc: 0.2401 - val_loss: 1.8201 - val_acc: 0.2440\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.7987 - acc: 0.2561 - val_loss: 1.7959 - val_acc: 0.2740\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.7730 - acc: 0.2815 - val_loss: 1.7693 - val_acc: 0.2910\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.7446 - acc: 0.3003 - val_loss: 1.7395 - val_acc: 0.3170\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.7128 - acc: 0.3319 - val_loss: 1.7064 - val_acc: 0.3450\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.6773 - acc: 0.3632 - val_loss: 1.6703 - val_acc: 0.3750\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.6386 - acc: 0.3897 - val_loss: 1.6305 - val_acc: 0.4070\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.5969 - acc: 0.4235 - val_loss: 1.5888 - val_acc: 0.4390\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.5528 - acc: 0.4499 - val_loss: 1.5443 - val_acc: 0.4680\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.5069 - acc: 0.4756 - val_loss: 1.4982 - val_acc: 0.5070\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.4596 - acc: 0.5143 - val_loss: 1.4519 - val_acc: 0.5210\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.4118 - acc: 0.5353 - val_loss: 1.4060 - val_acc: 0.5390\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.3636 - acc: 0.5556 - val_loss: 1.3597 - val_acc: 0.5570\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.3162 - acc: 0.5771 - val_loss: 1.3149 - val_acc: 0.5820\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.2694 - acc: 0.6009 - val_loss: 1.2692 - val_acc: 0.6000\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.2242 - acc: 0.6163 - val_loss: 1.2263 - val_acc: 0.6120\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.1803 - acc: 0.6356 - val_loss: 1.1859 - val_acc: 0.6260\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.1389 - acc: 0.6509 - val_loss: 1.1469 - val_acc: 0.6380\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0996 - acc: 0.6676 - val_loss: 1.1111 - val_acc: 0.6460\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0622 - acc: 0.6785 - val_loss: 1.0768 - val_acc: 0.6610\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.0276 - acc: 0.6871 - val_loss: 1.0448 - val_acc: 0.6660\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9954 - acc: 0.6965 - val_loss: 1.0135 - val_acc: 0.6780\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9647 - acc: 0.7045 - val_loss: 0.9860 - val_acc: 0.6830\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9366 - acc: 0.7100 - val_loss: 0.9605 - val_acc: 0.6950\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9104 - acc: 0.7151 - val_loss: 0.9370 - val_acc: 0.6950\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8861 - acc: 0.7185 - val_loss: 0.9164 - val_acc: 0.6930\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8634 - acc: 0.7248 - val_loss: 0.8953 - val_acc: 0.6970\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8425 - acc: 0.7277 - val_loss: 0.8789 - val_acc: 0.6920\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8236 - acc: 0.7324 - val_loss: 0.8620 - val_acc: 0.6960\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8052 - acc: 0.7335 - val_loss: 0.8474 - val_acc: 0.6970\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.7886 - acc: 0.7383 - val_loss: 0.8319 - val_acc: 0.7010\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.7720 - acc: 0.742 - 1s 71us/step - loss: 0.7722 - acc: 0.7416 - val_loss: 0.8181 - val_acc: 0.7060\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7582 - acc: 0.7459 - val_loss: 0.8067 - val_acc: 0.7030\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.7446 - acc: 0.7483 - val_loss: 0.7985 - val_acc: 0.7000\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7316 - acc: 0.7507 - val_loss: 0.7845 - val_acc: 0.7120\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7193 - acc: 0.7545 - val_loss: 0.7791 - val_acc: 0.7020\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.7077 - acc: 0.7588 - val_loss: 0.7678 - val_acc: 0.7140\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.6975 - acc: 0.7607 - val_loss: 0.7593 - val_acc: 0.7130\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.6868 - acc: 0.7639 - val_loss: 0.7498 - val_acc: 0.7220\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.6810 - acc: 0.766 - 1s 80us/step - loss: 0.6772 - acc: 0.7684 - val_loss: 0.7452 - val_acc: 0.7170\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.6682 - acc: 0.7707 - val_loss: 0.7378 - val_acc: 0.7190\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.6594 - acc: 0.7711 - val_loss: 0.7312 - val_acc: 0.7230\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.6509 - acc: 0.7751 - val_loss: 0.7272 - val_acc: 0.7220\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.6427 - acc: 0.7771 - val_loss: 0.7203 - val_acc: 0.7260\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.6350 - acc: 0.7775 - val_loss: 0.7170 - val_acc: 0.7260\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.6279 - acc: 0.7792 - val_loss: 0.7107 - val_acc: 0.7260\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.6206 - acc: 0.7811 - val_loss: 0.7081 - val_acc: 0.7230\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.6140 - acc: 0.7853 - val_loss: 0.7015 - val_acc: 0.7310\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.6073 - acc: 0.7869 - val_loss: 0.6966 - val_acc: 0.7310\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.6003 - acc: 0.7892 - val_loss: 0.6951 - val_acc: 0.7300\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.5945 - acc: 0.7923 - val_loss: 0.6923 - val_acc: 0.7290\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.5883 - acc: 0.7943 - val_loss: 0.6898 - val_acc: 0.7300\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.5826 - acc: 0.7963 - val_loss: 0.6827 - val_acc: 0.7320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.5768 - acc: 0.7987 - val_loss: 0.6803 - val_acc: 0.7290\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.5715 - acc: 0.8004 - val_loss: 0.6796 - val_acc: 0.7300\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.5662 - acc: 0.7988 - val_loss: 0.6760 - val_acc: 0.7340\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.5606 - acc: 0.8029 - val_loss: 0.6733 - val_acc: 0.7360\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.5554 - acc: 0.8033 - val_loss: 0.6724 - val_acc: 0.7310\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.5509 - acc: 0.8029 - val_loss: 0.6681 - val_acc: 0.7320\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.5458 - acc: 0.8056 - val_loss: 0.6664 - val_acc: 0.7380\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.5415 - acc: 0.8080 - val_loss: 0.6679 - val_acc: 0.7390\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.5364 - acc: 0.8095 - val_loss: 0.6606 - val_acc: 0.7330\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.5316 - acc: 0.8124 - val_loss: 0.6624 - val_acc: 0.7350\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.5274 - acc: 0.8119 - val_loss: 0.6574 - val_acc: 0.7410\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.5229 - acc: 0.8135 - val_loss: 0.6567 - val_acc: 0.7400\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.5189 - acc: 0.8145 - val_loss: 0.6555 - val_acc: 0.7420\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.5147 - acc: 0.8156 - val_loss: 0.6537 - val_acc: 0.7430\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.5107 - acc: 0.8160 - val_loss: 0.6534 - val_acc: 0.7410\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.5063 - acc: 0.8195 - val_loss: 0.6532 - val_acc: 0.7400\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.5025 - acc: 0.8216 - val_loss: 0.6506 - val_acc: 0.7450\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.4985 - acc: 0.8205 - val_loss: 0.6494 - val_acc: 0.7430\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.4947 - acc: 0.8231 - val_loss: 0.6478 - val_acc: 0.7460\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.4907 - acc: 0.8253 - val_loss: 0.6484 - val_acc: 0.7430\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.4872 - acc: 0.8251 - val_loss: 0.6478 - val_acc: 0.7490\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.4834 - acc: 0.8292 - val_loss: 0.6472 - val_acc: 0.7420\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.4795 - acc: 0.8283 - val_loss: 0.6472 - val_acc: 0.7480\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.4760 - acc: 0.8311 - val_loss: 0.6420 - val_acc: 0.7430\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.4725 - acc: 0.8324 - val_loss: 0.6414 - val_acc: 0.7440\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.4692 - acc: 0.8348 - val_loss: 0.6441 - val_acc: 0.7480\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.4656 - acc: 0.8356 - val_loss: 0.6435 - val_acc: 0.7500\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.4623 - acc: 0.8367 - val_loss: 0.6422 - val_acc: 0.7490\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.4588 - acc: 0.8365 - val_loss: 0.6399 - val_acc: 0.7500\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.4554 - acc: 0.8399 - val_loss: 0.6369 - val_acc: 0.7500\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.4527 - acc: 0.8424 - val_loss: 0.6415 - val_acc: 0.7520\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.4491 - acc: 0.8435 - val_loss: 0.6389 - val_acc: 0.7510\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.4465 - acc: 0.8429 - val_loss: 0.6381 - val_acc: 0.7540\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.4425 - acc: 0.8449 - val_loss: 0.6389 - val_acc: 0.7520\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.4396 - acc: 0.8464 - val_loss: 0.6375 - val_acc: 0.7510\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.4363 - acc: 0.8483 - val_loss: 0.6362 - val_acc: 0.7560\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.4334 - acc: 0.8467 - val_loss: 0.6353 - val_acc: 0.7520\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.4303 - acc: 0.8520 - val_loss: 0.6343 - val_acc: 0.7550\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.4276 - acc: 0.8523 - val_loss: 0.6377 - val_acc: 0.7550\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.4245 - acc: 0.8547 - val_loss: 0.6346 - val_acc: 0.7560\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.4217 - acc: 0.8552 - val_loss: 0.6375 - val_acc: 0.7570\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.4187 - acc: 0.8540 - val_loss: 0.6370 - val_acc: 0.7580\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.4159 - acc: 0.8568 - val_loss: 0.6393 - val_acc: 0.7510\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.4130 - acc: 0.8568 - val_loss: 0.6384 - val_acc: 0.7550\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.4100 - acc: 0.8583 - val_loss: 0.6351 - val_acc: 0.7560\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.4077 - acc: 0.8605 - val_loss: 0.6332 - val_acc: 0.7550\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.4049 - acc: 0.8615 - val_loss: 0.6360 - val_acc: 0.7560\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.4022 - acc: 0.8621 - val_loss: 0.6325 - val_acc: 0.7560\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.3996 - acc: 0.8629 - val_loss: 0.6333 - val_acc: 0.7560\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.3966 - acc: 0.8647 - val_loss: 0.6359 - val_acc: 0.7570\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.3939 - acc: 0.8669 - val_loss: 0.6380 - val_acc: 0.7560\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.3915 - acc: 0.8681 - val_loss: 0.6323 - val_acc: 0.7540\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.3887 - acc: 0.8703 - val_loss: 0.6349 - val_acc: 0.7580\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.3860 - acc: 0.8709 - val_loss: 0.6370 - val_acc: 0.7540\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.3836 - acc: 0.8701 - val_loss: 0.6332 - val_acc: 0.7550\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.3808 - acc: 0.8729 - val_loss: 0.6421 - val_acc: 0.7570\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.3783 - acc: 0.8747 - val_loss: 0.6363 - val_acc: 0.7580\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.3760 - acc: 0.8736 - val_loss: 0.6363 - val_acc: 0.7610\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.3735 - acc: 0.8747 - val_loss: 0.6378 - val_acc: 0.7590\n",
      "Epoch 119/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.3709 - acc: 0.8763 - val_loss: 0.6373 - val_acc: 0.7560\n",
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.3687 - acc: 0.8769 - val_loss: 0.6380 - val_acc: 0.7560\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 39us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 39us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3652195799748103, 0.8797333333333334]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6327094724178314, 0.7653333331743876]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 1s 106us/step - loss: 1.9558 - acc: 0.1399 - val_loss: 1.9426 - val_acc: 0.1650\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.9241 - acc: 0.1936 - val_loss: 1.9188 - val_acc: 0.2100\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.9000 - acc: 0.2349 - val_loss: 1.8970 - val_acc: 0.2450\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.8770 - acc: 0.2684 - val_loss: 1.8755 - val_acc: 0.2680\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.8530 - acc: 0.2940 - val_loss: 1.8521 - val_acc: 0.2950\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.8268 - acc: 0.3167 - val_loss: 1.8258 - val_acc: 0.3250\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.7976 - acc: 0.3399 - val_loss: 1.7967 - val_acc: 0.3330\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.7648 - acc: 0.3571 - val_loss: 1.7632 - val_acc: 0.3630\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.7283 - acc: 0.3839 - val_loss: 1.7263 - val_acc: 0.3800\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.6878 - acc: 0.4009 - val_loss: 1.6856 - val_acc: 0.4070\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.6436 - acc: 0.4215 - val_loss: 1.6418 - val_acc: 0.4270\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.5963 - acc: 0.4411 - val_loss: 1.5955 - val_acc: 0.4520\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.5465 - acc: 0.4664 - val_loss: 1.5456 - val_acc: 0.4700\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.4949 - acc: 0.4937 - val_loss: 1.4968 - val_acc: 0.4950\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.4425 - acc: 0.5260 - val_loss: 1.4449 - val_acc: 0.5070\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.3900 - acc: 0.5483 - val_loss: 1.3943 - val_acc: 0.5350\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.3381 - acc: 0.5800 - val_loss: 1.3443 - val_acc: 0.5610\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.2874 - acc: 0.6063 - val_loss: 1.2955 - val_acc: 0.5890\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.2384 - acc: 0.6268 - val_loss: 1.2493 - val_acc: 0.6090\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.1913 - acc: 0.6484 - val_loss: 1.2066 - val_acc: 0.6210\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 1.1473 - acc: 0.6631 - val_loss: 1.1656 - val_acc: 0.6420\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.1056 - acc: 0.6740 - val_loss: 1.1274 - val_acc: 0.6520\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.0671 - acc: 0.6845 - val_loss: 1.0920 - val_acc: 0.6650\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.0309 - acc: 0.6937 - val_loss: 1.0597 - val_acc: 0.6690\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.9975 - acc: 0.7007 - val_loss: 1.0307 - val_acc: 0.6840\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9664 - acc: 0.7105 - val_loss: 1.0025 - val_acc: 0.6800\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.9371 - acc: 0.7144 - val_loss: 0.9760 - val_acc: 0.6860\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9097 - acc: 0.7237 - val_loss: 0.9528 - val_acc: 0.6830\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.8850 - acc: 0.7277 - val_loss: 0.9313 - val_acc: 0.6960\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8619 - acc: 0.7333 - val_loss: 0.9118 - val_acc: 0.7070\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.8402 - acc: 0.7360 - val_loss: 0.8932 - val_acc: 0.7070\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8201 - acc: 0.7431 - val_loss: 0.8767 - val_acc: 0.7110\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.8014 - acc: 0.7477 - val_loss: 0.8614 - val_acc: 0.7160\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.7835 - acc: 0.7495 - val_loss: 0.8477 - val_acc: 0.7060\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7674 - acc: 0.7553 - val_loss: 0.8320 - val_acc: 0.7120\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.7526 - acc: 0.7580 - val_loss: 0.8201 - val_acc: 0.7130\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.7379 - acc: 0.7603 - val_loss: 0.8112 - val_acc: 0.7170\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.7249 - acc: 0.7632 - val_loss: 0.7996 - val_acc: 0.7230\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.7122 - acc: 0.7655 - val_loss: 0.7896 - val_acc: 0.7220\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.6999 - acc: 0.7713 - val_loss: 0.7808 - val_acc: 0.7220\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.6888 - acc: 0.7728 - val_loss: 0.7721 - val_acc: 0.7210\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.6779 - acc: 0.7760 - val_loss: 0.7644 - val_acc: 0.7190\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.6676 - acc: 0.7793 - val_loss: 0.7571 - val_acc: 0.7290\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.6580 - acc: 0.7813 - val_loss: 0.7502 - val_acc: 0.7250\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.6488 - acc: 0.7841 - val_loss: 0.7439 - val_acc: 0.7250\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.6398 - acc: 0.7851 - val_loss: 0.7375 - val_acc: 0.7330\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.6314 - acc: 0.7872 - val_loss: 0.7333 - val_acc: 0.7300\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.6230 - acc: 0.7921 - val_loss: 0.7287 - val_acc: 0.7290\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.6156 - acc: 0.7925 - val_loss: 0.7208 - val_acc: 0.7390\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.6084 - acc: 0.7953 - val_loss: 0.7199 - val_acc: 0.7320\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.6009 - acc: 0.7967 - val_loss: 0.7137 - val_acc: 0.7350\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.5940 - acc: 0.7987 - val_loss: 0.7116 - val_acc: 0.7340\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.5876 - acc: 0.8024 - val_loss: 0.7065 - val_acc: 0.7430\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.5810 - acc: 0.8029 - val_loss: 0.7030 - val_acc: 0.7360\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.5750 - acc: 0.8064 - val_loss: 0.6985 - val_acc: 0.7350\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.5681 - acc: 0.8089 - val_loss: 0.7019 - val_acc: 0.7450\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.5631 - acc: 0.8097 - val_loss: 0.6919 - val_acc: 0.7410\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.5571 - acc: 0.8104 - val_loss: 0.6888 - val_acc: 0.7430\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.5515 - acc: 0.8111 - val_loss: 0.6861 - val_acc: 0.7440\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.5461 - acc: 0.8159 - val_loss: 0.6834 - val_acc: 0.7440\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 42us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 40us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5414692291259766, 0.818]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.691273176908493, 0.7499999998410543]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 115us/step - loss: 2.5705 - acc: 0.1901 - val_loss: 2.5595 - val_acc: 0.1960\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 2.5409 - acc: 0.2221 - val_loss: 2.5336 - val_acc: 0.2340\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 2.5096 - acc: 0.2448 - val_loss: 2.5034 - val_acc: 0.2660\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 2.4753 - acc: 0.2668 - val_loss: 2.4693 - val_acc: 0.2750\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 2.4371 - acc: 0.2860 - val_loss: 2.4300 - val_acc: 0.3120\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 2.3962 - acc: 0.3152 - val_loss: 2.3885 - val_acc: 0.3220\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 2.3524 - acc: 0.3372 - val_loss: 2.3424 - val_acc: 0.3610\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 2.3063 - acc: 0.3675 - val_loss: 2.2969 - val_acc: 0.3800\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 2.2580 - acc: 0.4008 - val_loss: 2.2492 - val_acc: 0.4070\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 2.2086 - acc: 0.4328 - val_loss: 2.2014 - val_acc: 0.4420\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 2.1580 - acc: 0.4675 - val_loss: 2.1525 - val_acc: 0.4610\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 2.1068 - acc: 0.4977 - val_loss: 2.1047 - val_acc: 0.4870\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 2.0552 - acc: 0.5249 - val_loss: 2.0531 - val_acc: 0.5280\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 2.0041 - acc: 0.5541 - val_loss: 2.0024 - val_acc: 0.5500\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.9539 - acc: 0.5781 - val_loss: 1.9544 - val_acc: 0.5690\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.9051 - acc: 0.6020 - val_loss: 1.9074 - val_acc: 0.5740\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.8577 - acc: 0.6200 - val_loss: 1.8615 - val_acc: 0.6060\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.8120 - acc: 0.6412 - val_loss: 1.8175 - val_acc: 0.6150\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.7673 - acc: 0.6531 - val_loss: 1.7755 - val_acc: 0.6370\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.7245 - acc: 0.6680 - val_loss: 1.7356 - val_acc: 0.6400\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.6846 - acc: 0.6768 - val_loss: 1.6979 - val_acc: 0.6500\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.6468 - acc: 0.6861 - val_loss: 1.6638 - val_acc: 0.6620\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.6124 - acc: 0.6960 - val_loss: 1.6310 - val_acc: 0.6650\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.5802 - acc: 0.7035 - val_loss: 1.6018 - val_acc: 0.6740\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.5502 - acc: 0.7099 - val_loss: 1.5737 - val_acc: 0.6790\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.5223 - acc: 0.7156 - val_loss: 1.5482 - val_acc: 0.6920\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.4962 - acc: 0.7204 - val_loss: 1.5237 - val_acc: 0.6960\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.4721 - acc: 0.7241 - val_loss: 1.5021 - val_acc: 0.7020\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.4485 - acc: 0.7287 - val_loss: 1.4839 - val_acc: 0.6990\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.4275 - acc: 0.7311 - val_loss: 1.4633 - val_acc: 0.7090\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.4077 - acc: 0.7348 - val_loss: 1.4452 - val_acc: 0.7080\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.3891 - acc: 0.7396 - val_loss: 1.4305 - val_acc: 0.7180\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 1.3719 - acc: 0.7427 - val_loss: 1.4144 - val_acc: 0.7110\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.3549 - acc: 0.7476 - val_loss: 1.4004 - val_acc: 0.7120\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.3397 - acc: 0.7508 - val_loss: 1.3871 - val_acc: 0.7130\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.3246 - acc: 0.7543 - val_loss: 1.3757 - val_acc: 0.7130\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.3107 - acc: 0.7560 - val_loss: 1.3634 - val_acc: 0.7130\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.2981 - acc: 0.7583 - val_loss: 1.3529 - val_acc: 0.7210\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.2847 - acc: 0.7645 - val_loss: 1.3425 - val_acc: 0.7230\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.2732 - acc: 0.7631 - val_loss: 1.3327 - val_acc: 0.7250\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.2618 - acc: 0.7705 - val_loss: 1.3253 - val_acc: 0.7200\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.2509 - acc: 0.7700 - val_loss: 1.3154 - val_acc: 0.7330\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.2400 - acc: 0.7748 - val_loss: 1.3069 - val_acc: 0.7250\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.2302 - acc: 0.7725 - val_loss: 1.2989 - val_acc: 0.7330\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.2207 - acc: 0.7781 - val_loss: 1.2930 - val_acc: 0.7380\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.2112 - acc: 0.7801 - val_loss: 1.2844 - val_acc: 0.7360\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.2019 - acc: 0.7800 - val_loss: 1.2776 - val_acc: 0.7350\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.1930 - acc: 0.7825 - val_loss: 1.2703 - val_acc: 0.7310\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.1845 - acc: 0.7852 - val_loss: 1.2642 - val_acc: 0.7330\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.1760 - acc: 0.7865 - val_loss: 1.2587 - val_acc: 0.7390\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.1680 - acc: 0.7875 - val_loss: 1.2514 - val_acc: 0.7340\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.1602 - acc: 0.7892 - val_loss: 1.2462 - val_acc: 0.7340\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 1.1526 - acc: 0.7952 - val_loss: 1.2402 - val_acc: 0.7330\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.1448 - acc: 0.7937 - val_loss: 1.2352 - val_acc: 0.7350\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1376 - acc: 0.7948 - val_loss: 1.2293 - val_acc: 0.7410\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.1303 - acc: 0.7981 - val_loss: 1.2252 - val_acc: 0.7370\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.1231 - acc: 0.7975 - val_loss: 1.2198 - val_acc: 0.7370\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.1167 - acc: 0.7991 - val_loss: 1.2168 - val_acc: 0.7360\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.1098 - acc: 0.8040 - val_loss: 1.2119 - val_acc: 0.7440\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.1033 - acc: 0.8015 - val_loss: 1.2066 - val_acc: 0.7480\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.0968 - acc: 0.8068 - val_loss: 1.2012 - val_acc: 0.7390\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.0902 - acc: 0.8075 - val_loss: 1.1970 - val_acc: 0.7380\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.0843 - acc: 0.8097 - val_loss: 1.1947 - val_acc: 0.7370\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.0785 - acc: 0.8088 - val_loss: 1.1881 - val_acc: 0.7460\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.0722 - acc: 0.8113 - val_loss: 1.1847 - val_acc: 0.7420\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.0663 - acc: 0.8145 - val_loss: 1.1819 - val_acc: 0.7390\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.0606 - acc: 0.8144 - val_loss: 1.1771 - val_acc: 0.7460\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.0547 - acc: 0.8165 - val_loss: 1.1741 - val_acc: 0.7440\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.0493 - acc: 0.8201 - val_loss: 1.1707 - val_acc: 0.7460\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0438 - acc: 0.8203 - val_loss: 1.1679 - val_acc: 0.7410\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0381 - acc: 0.8225 - val_loss: 1.1652 - val_acc: 0.7470\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0329 - acc: 0.8221 - val_loss: 1.1612 - val_acc: 0.7450\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0276 - acc: 0.8253 - val_loss: 1.1559 - val_acc: 0.7550\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0222 - acc: 0.8257 - val_loss: 1.1534 - val_acc: 0.7450\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.0171 - acc: 0.8277 - val_loss: 1.1505 - val_acc: 0.7500\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.0121 - acc: 0.8301 - val_loss: 1.1481 - val_acc: 0.7500\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.0071 - acc: 0.8319 - val_loss: 1.1444 - val_acc: 0.7460\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.0019 - acc: 0.8323 - val_loss: 1.1408 - val_acc: 0.7450\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9970 - acc: 0.8323 - val_loss: 1.1379 - val_acc: 0.7480\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.9923 - acc: 0.8363 - val_loss: 1.1353 - val_acc: 0.7490\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.9873 - acc: 0.8379 - val_loss: 1.1330 - val_acc: 0.7520\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.9825 - acc: 0.8392 - val_loss: 1.1284 - val_acc: 0.7510\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.9780 - acc: 0.8384 - val_loss: 1.1269 - val_acc: 0.7550\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9735 - acc: 0.8393 - val_loss: 1.1243 - val_acc: 0.7470\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9690 - acc: 0.8425 - val_loss: 1.1225 - val_acc: 0.7510\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9644 - acc: 0.8421 - val_loss: 1.1178 - val_acc: 0.7500\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9601 - acc: 0.8432 - val_loss: 1.1161 - val_acc: 0.7550\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9556 - acc: 0.8463 - val_loss: 1.1137 - val_acc: 0.7480\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9511 - acc: 0.8456 - val_loss: 1.1102 - val_acc: 0.7550\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9470 - acc: 0.8492 - val_loss: 1.1089 - val_acc: 0.7550\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9430 - acc: 0.8497 - val_loss: 1.1055 - val_acc: 0.7500\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.9384 - acc: 0.8503 - val_loss: 1.1049 - val_acc: 0.7520\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9344 - acc: 0.8500 - val_loss: 1.1012 - val_acc: 0.7570\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.9306 - acc: 0.8537 - val_loss: 1.0988 - val_acc: 0.7590\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9262 - acc: 0.8563 - val_loss: 1.0974 - val_acc: 0.7500\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.9221 - acc: 0.8532 - val_loss: 1.0945 - val_acc: 0.7550\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9179 - acc: 0.8557 - val_loss: 1.0926 - val_acc: 0.7580\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9139 - acc: 0.8584 - val_loss: 1.0938 - val_acc: 0.7460\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9104 - acc: 0.8587 - val_loss: 1.0920 - val_acc: 0.7540\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.9065 - acc: 0.8585 - val_loss: 1.0886 - val_acc: 0.7530\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9027 - acc: 0.8603 - val_loss: 1.0851 - val_acc: 0.7610\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8986 - acc: 0.8619 - val_loss: 1.0822 - val_acc: 0.7530\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8950 - acc: 0.8609 - val_loss: 1.0805 - val_acc: 0.7550\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8912 - acc: 0.8643 - val_loss: 1.0779 - val_acc: 0.7510\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8873 - acc: 0.8657 - val_loss: 1.0761 - val_acc: 0.7560\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8835 - acc: 0.8649 - val_loss: 1.0747 - val_acc: 0.7590\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8799 - acc: 0.8649 - val_loss: 1.0720 - val_acc: 0.7570\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8761 - acc: 0.8679 - val_loss: 1.0710 - val_acc: 0.7620\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8730 - acc: 0.8680 - val_loss: 1.0685 - val_acc: 0.7510\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8693 - acc: 0.8687 - val_loss: 1.0663 - val_acc: 0.7570\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8655 - acc: 0.8699 - val_loss: 1.0655 - val_acc: 0.7490\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8625 - acc: 0.8697 - val_loss: 1.0638 - val_acc: 0.7560\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8589 - acc: 0.8708 - val_loss: 1.0633 - val_acc: 0.7520\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8551 - acc: 0.8723 - val_loss: 1.0584 - val_acc: 0.7590\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8520 - acc: 0.8731 - val_loss: 1.0608 - val_acc: 0.7460\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8487 - acc: 0.8729 - val_loss: 1.0579 - val_acc: 0.7500\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8452 - acc: 0.8729 - val_loss: 1.0549 - val_acc: 0.7620\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.8420 - acc: 0.8756 - val_loss: 1.0537 - val_acc: 0.7670\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8383 - acc: 0.8772 - val_loss: 1.0550 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8353 - acc: 0.8772 - val_loss: 1.0489 - val_acc: 0.7650\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'y', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 116us/step - loss: 15.9311 - acc: 0.1967 - val_loss: 15.5291 - val_acc: 0.1990\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 15.1681 - acc: 0.2193 - val_loss: 14.7798 - val_acc: 0.2120\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 14.4261 - acc: 0.2249 - val_loss: 14.0517 - val_acc: 0.2200\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 13.7058 - acc: 0.2364 - val_loss: 13.3440 - val_acc: 0.2340\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 13.0062 - acc: 0.2512 - val_loss: 12.6560 - val_acc: 0.2580\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 12.3259 - acc: 0.2713 - val_loss: 11.9866 - val_acc: 0.2930\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 11.6636 - acc: 0.3059 - val_loss: 11.3356 - val_acc: 0.3140\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 11.0198 - acc: 0.3355 - val_loss: 10.7030 - val_acc: 0.3290\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 10.3960 - acc: 0.3568 - val_loss: 10.0905 - val_acc: 0.3600\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 9.7938 - acc: 0.3761 - val_loss: 9.5006 - val_acc: 0.3720\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 9.2143 - acc: 0.3832 - val_loss: 8.9331 - val_acc: 0.3920\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 8.6583 - acc: 0.3999 - val_loss: 8.3895 - val_acc: 0.4030\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 8.1255 - acc: 0.4105 - val_loss: 7.8690 - val_acc: 0.4140\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 7.6159 - acc: 0.4217 - val_loss: 7.3720 - val_acc: 0.4320\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 7.1300 - acc: 0.4384 - val_loss: 6.8983 - val_acc: 0.4510\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 6.6682 - acc: 0.4595 - val_loss: 6.4484 - val_acc: 0.4660\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 6.2293 - acc: 0.4797 - val_loss: 6.0224 - val_acc: 0.4790\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 5.8140 - acc: 0.4996 - val_loss: 5.6198 - val_acc: 0.5030\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 5.4220 - acc: 0.5260 - val_loss: 5.2399 - val_acc: 0.5270\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 5.0518 - acc: 0.5516 - val_loss: 4.8825 - val_acc: 0.5530\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 4.7045 - acc: 0.5671 - val_loss: 4.5473 - val_acc: 0.5610\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 4.3803 - acc: 0.5813 - val_loss: 4.2339 - val_acc: 0.5850\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 4.0785 - acc: 0.5940 - val_loss: 3.9443 - val_acc: 0.5960\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 3.7992 - acc: 0.6028 - val_loss: 3.6766 - val_acc: 0.6030\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 3.5417 - acc: 0.6132 - val_loss: 3.4303 - val_acc: 0.6040\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 3.3062 - acc: 0.6199 - val_loss: 3.2078 - val_acc: 0.6050\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 3.0916 - acc: 0.6239 - val_loss: 3.0032 - val_acc: 0.6120\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 2.8984 - acc: 0.6264 - val_loss: 2.8187 - val_acc: 0.6160\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 2.7253 - acc: 0.6331 - val_loss: 2.6575 - val_acc: 0.6310\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 2.5734 - acc: 0.6380 - val_loss: 2.5162 - val_acc: 0.6210\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 2.4416 - acc: 0.6437 - val_loss: 2.3929 - val_acc: 0.6260\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 2.3292 - acc: 0.6439 - val_loss: 2.2912 - val_acc: 0.6250\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 2.2360 - acc: 0.6473 - val_loss: 2.2073 - val_acc: 0.6330\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 2.1616 - acc: 0.6517 - val_loss: 2.1407 - val_acc: 0.6310\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 2.1032 - acc: 0.6547 - val_loss: 2.0908 - val_acc: 0.6310\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 2.0597 - acc: 0.6569 - val_loss: 2.0549 - val_acc: 0.6330\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 2.0271 - acc: 0.6575 - val_loss: 2.0243 - val_acc: 0.6450\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 2.0007 - acc: 0.6619 - val_loss: 1.9992 - val_acc: 0.6410\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.9776 - acc: 0.6643 - val_loss: 1.9774 - val_acc: 0.6420\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.9569 - acc: 0.6664 - val_loss: 1.9565 - val_acc: 0.6510\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.9374 - acc: 0.6673 - val_loss: 1.9388 - val_acc: 0.6520\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.9194 - acc: 0.6721 - val_loss: 1.9228 - val_acc: 0.6470\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.9022 - acc: 0.6732 - val_loss: 1.9052 - val_acc: 0.6550\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.8862 - acc: 0.6744 - val_loss: 1.8895 - val_acc: 0.6590\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.8706 - acc: 0.6764 - val_loss: 1.8722 - val_acc: 0.6580\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.8556 - acc: 0.6781 - val_loss: 1.8587 - val_acc: 0.6620\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.8413 - acc: 0.6804 - val_loss: 1.8442 - val_acc: 0.6640\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.8274 - acc: 0.6845 - val_loss: 1.8344 - val_acc: 0.6580\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.8141 - acc: 0.6839 - val_loss: 1.8180 - val_acc: 0.6630\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.8007 - acc: 0.6864 - val_loss: 1.8056 - val_acc: 0.6710\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.7886 - acc: 0.6868 - val_loss: 1.7954 - val_acc: 0.6630\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.7764 - acc: 0.6883 - val_loss: 1.7854 - val_acc: 0.6590\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.7644 - acc: 0.6887 - val_loss: 1.7681 - val_acc: 0.6670\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.7529 - acc: 0.6893 - val_loss: 1.7590 - val_acc: 0.6770\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.7413 - acc: 0.6915 - val_loss: 1.7484 - val_acc: 0.6690\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.7307 - acc: 0.6939 - val_loss: 1.7369 - val_acc: 0.6730\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.7197 - acc: 0.6939 - val_loss: 1.7265 - val_acc: 0.6710\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.7093 - acc: 0.6952 - val_loss: 1.7143 - val_acc: 0.6740\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.6992 - acc: 0.6969 - val_loss: 1.7055 - val_acc: 0.6760\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.6891 - acc: 0.6971 - val_loss: 1.6939 - val_acc: 0.6760\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.6791 - acc: 0.6980 - val_loss: 1.6852 - val_acc: 0.6780\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.6695 - acc: 0.6980 - val_loss: 1.6768 - val_acc: 0.6760\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.6603 - acc: 0.6993 - val_loss: 1.6658 - val_acc: 0.6750\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.6508 - acc: 0.7004 - val_loss: 1.6586 - val_acc: 0.6660\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.6417 - acc: 0.7016 - val_loss: 1.6479 - val_acc: 0.6750\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.6325 - acc: 0.7009 - val_loss: 1.6406 - val_acc: 0.6770\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.6235 - acc: 0.7020 - val_loss: 1.6323 - val_acc: 0.6700\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.6148 - acc: 0.7033 - val_loss: 1.6232 - val_acc: 0.6880\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.6066 - acc: 0.7035 - val_loss: 1.6147 - val_acc: 0.6710\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.5977 - acc: 0.7041 - val_loss: 1.6059 - val_acc: 0.6740\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.5894 - acc: 0.7041 - val_loss: 1.5969 - val_acc: 0.6740\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.5816 - acc: 0.7048 - val_loss: 1.5896 - val_acc: 0.6800\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.5732 - acc: 0.7067 - val_loss: 1.5830 - val_acc: 0.6780\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.5656 - acc: 0.7065 - val_loss: 1.5740 - val_acc: 0.6730\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.5577 - acc: 0.7056 - val_loss: 1.5689 - val_acc: 0.6790\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.5501 - acc: 0.7073 - val_loss: 1.5583 - val_acc: 0.6810\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 1.5423 - acc: 0.7079 - val_loss: 1.5523 - val_acc: 0.6890\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.5353 - acc: 0.7087 - val_loss: 1.5502 - val_acc: 0.6760\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.5275 - acc: 0.7088 - val_loss: 1.5390 - val_acc: 0.6870\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.5205 - acc: 0.7095 - val_loss: 1.5298 - val_acc: 0.6790\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.5131 - acc: 0.7100 - val_loss: 1.5223 - val_acc: 0.6810\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.5061 - acc: 0.7095 - val_loss: 1.5139 - val_acc: 0.6850\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.4990 - acc: 0.7109 - val_loss: 1.5097 - val_acc: 0.6790\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.4921 - acc: 0.7111 - val_loss: 1.5043 - val_acc: 0.6920\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.4855 - acc: 0.7111 - val_loss: 1.4941 - val_acc: 0.6870\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.4785 - acc: 0.7125 - val_loss: 1.4897 - val_acc: 0.6820\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.4718 - acc: 0.7121 - val_loss: 1.4844 - val_acc: 0.6800\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.4658 - acc: 0.7119 - val_loss: 1.4791 - val_acc: 0.6800\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.4588 - acc: 0.7128 - val_loss: 1.4739 - val_acc: 0.6790\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.4529 - acc: 0.7136 - val_loss: 1.4650 - val_acc: 0.6830\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.4463 - acc: 0.7147 - val_loss: 1.4586 - val_acc: 0.6900\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.4399 - acc: 0.7132 - val_loss: 1.4501 - val_acc: 0.6890\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.4335 - acc: 0.7156 - val_loss: 1.4454 - val_acc: 0.6890\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.4279 - acc: 0.7148 - val_loss: 1.4392 - val_acc: 0.6910\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.4213 - acc: 0.7144 - val_loss: 1.4324 - val_acc: 0.6890\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.4154 - acc: 0.7139 - val_loss: 1.4258 - val_acc: 0.6900\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.4094 - acc: 0.7160 - val_loss: 1.4215 - val_acc: 0.6920\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.4034 - acc: 0.7163 - val_loss: 1.4159 - val_acc: 0.6870\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.3979 - acc: 0.7167 - val_loss: 1.4159 - val_acc: 0.6850\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.3924 - acc: 0.7167 - val_loss: 1.4055 - val_acc: 0.6960\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.3863 - acc: 0.7160 - val_loss: 1.3968 - val_acc: 0.6920\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.3804 - acc: 0.7187 - val_loss: 1.3944 - val_acc: 0.6870\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.3748 - acc: 0.7185 - val_loss: 1.3897 - val_acc: 0.6930\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.3697 - acc: 0.7201 - val_loss: 1.3848 - val_acc: 0.6880\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.3640 - acc: 0.7188 - val_loss: 1.3789 - val_acc: 0.6920\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.3586 - acc: 0.7200 - val_loss: 1.3701 - val_acc: 0.6960\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.3528 - acc: 0.7193 - val_loss: 1.3668 - val_acc: 0.6990\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.3477 - acc: 0.7191 - val_loss: 1.3610 - val_acc: 0.6950\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.3425 - acc: 0.7209 - val_loss: 1.3559 - val_acc: 0.6980\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.3373 - acc: 0.7200 - val_loss: 1.3517 - val_acc: 0.6990\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.3317 - acc: 0.7209 - val_loss: 1.3451 - val_acc: 0.6920\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.3270 - acc: 0.7211 - val_loss: 1.3399 - val_acc: 0.6980\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.3217 - acc: 0.7215 - val_loss: 1.3374 - val_acc: 0.6930\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.3166 - acc: 0.7236 - val_loss: 1.3327 - val_acc: 0.6960\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.3121 - acc: 0.7227 - val_loss: 1.3258 - val_acc: 0.6970\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.3067 - acc: 0.7223 - val_loss: 1.3239 - val_acc: 0.7020\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.3020 - acc: 0.7223 - val_loss: 1.3182 - val_acc: 0.7040\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.2978 - acc: 0.7225 - val_loss: 1.3137 - val_acc: 0.6990\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.2922 - acc: 0.7228 - val_loss: 1.3084 - val_acc: 0.7040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.2881 - acc: 0.7236 - val_loss: 1.3075 - val_acc: 0.6930\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 1s 129us/step - loss: 16.0209 - acc: 0.1444 - val_loss: 15.6098 - val_acc: 0.1680\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 15.2521 - acc: 0.1859 - val_loss: 14.8609 - val_acc: 0.2110\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 14.5113 - acc: 0.2215 - val_loss: 14.1344 - val_acc: 0.2520\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 13.7918 - acc: 0.2571 - val_loss: 13.4265 - val_acc: 0.2850\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 13.0906 - acc: 0.2903 - val_loss: 12.7364 - val_acc: 0.3080\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 12.4078 - acc: 0.3244 - val_loss: 12.0651 - val_acc: 0.3330\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 11.7447 - acc: 0.3557 - val_loss: 11.4146 - val_acc: 0.3640\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 11.1032 - acc: 0.3857 - val_loss: 10.7851 - val_acc: 0.4080\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 10.4836 - acc: 0.4151 - val_loss: 10.1777 - val_acc: 0.4390\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 9.8860 - acc: 0.4349 - val_loss: 9.5914 - val_acc: 0.4700\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 9.3098 - acc: 0.4595 - val_loss: 9.0264 - val_acc: 0.4810\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 8.7543 - acc: 0.4791 - val_loss: 8.4829 - val_acc: 0.4900\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 8.2209 - acc: 0.4975 - val_loss: 7.9616 - val_acc: 0.5040\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 7.7110 - acc: 0.5085 - val_loss: 7.4645 - val_acc: 0.5070\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 7.2239 - acc: 0.5195 - val_loss: 6.9895 - val_acc: 0.5160\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 6.7593 - acc: 0.5301 - val_loss: 6.5370 - val_acc: 0.5230\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 6.3181 - acc: 0.5397 - val_loss: 6.1081 - val_acc: 0.5300\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 5.8994 - acc: 0.5492 - val_loss: 5.7014 - val_acc: 0.5410\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 5.5031 - acc: 0.5597 - val_loss: 5.3187 - val_acc: 0.5370\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 5.1297 - acc: 0.5664 - val_loss: 4.9567 - val_acc: 0.5480\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 4.7782 - acc: 0.5728 - val_loss: 4.6166 - val_acc: 0.5590\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 4.4502 - acc: 0.5793 - val_loss: 4.3013 - val_acc: 0.5620\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 4.1446 - acc: 0.5851 - val_loss: 4.0071 - val_acc: 0.5790\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 3.8609 - acc: 0.5912 - val_loss: 3.7349 - val_acc: 0.5910\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 3.5993 - acc: 0.5988 - val_loss: 3.4845 - val_acc: 0.5900\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 3.3596 - acc: 0.6056 - val_loss: 3.2568 - val_acc: 0.5920\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 3.1417 - acc: 0.6108 - val_loss: 3.0498 - val_acc: 0.6040\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 2.9445 - acc: 0.6189 - val_loss: 2.8640 - val_acc: 0.6010\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 2.7686 - acc: 0.6236 - val_loss: 2.6988 - val_acc: 0.6090\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 2.6139 - acc: 0.6281 - val_loss: 2.5558 - val_acc: 0.6160\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 2.4803 - acc: 0.6289 - val_loss: 2.4324 - val_acc: 0.6130\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 2.3667 - acc: 0.6349 - val_loss: 2.3297 - val_acc: 0.6170\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 2.2729 - acc: 0.6373 - val_loss: 2.2461 - val_acc: 0.6140\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 2.1979 - acc: 0.6364 - val_loss: 2.1800 - val_acc: 0.6340\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 2.1407 - acc: 0.6467 - val_loss: 2.1295 - val_acc: 0.6370\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 2.0984 - acc: 0.6471 - val_loss: 2.0941 - val_acc: 0.6350\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 2.0671 - acc: 0.6483 - val_loss: 2.0663 - val_acc: 0.6400\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 2.0419 - acc: 0.6547 - val_loss: 2.0465 - val_acc: 0.6320\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 2.0206 - acc: 0.6529 - val_loss: 2.0230 - val_acc: 0.6350\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 2.0009 - acc: 0.6552 - val_loss: 2.0054 - val_acc: 0.6460\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.9824 - acc: 0.6595 - val_loss: 1.9880 - val_acc: 0.6420\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.9652 - acc: 0.6612 - val_loss: 1.9705 - val_acc: 0.6520\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.9485 - acc: 0.6631 - val_loss: 1.9541 - val_acc: 0.6500\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.9326 - acc: 0.6660 - val_loss: 1.9388 - val_acc: 0.6600\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.9171 - acc: 0.6671 - val_loss: 1.9242 - val_acc: 0.6620\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.9029 - acc: 0.6727 - val_loss: 1.9092 - val_acc: 0.6520\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.8890 - acc: 0.6771 - val_loss: 1.8958 - val_acc: 0.6530\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.8752 - acc: 0.6768 - val_loss: 1.8830 - val_acc: 0.6560\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.8623 - acc: 0.6775 - val_loss: 1.8703 - val_acc: 0.6590\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.8495 - acc: 0.6813 - val_loss: 1.8571 - val_acc: 0.6600\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.8369 - acc: 0.6817 - val_loss: 1.8450 - val_acc: 0.6560\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.8249 - acc: 0.6819 - val_loss: 1.8326 - val_acc: 0.6550\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.8131 - acc: 0.6816 - val_loss: 1.8238 - val_acc: 0.6620\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.8015 - acc: 0.6843 - val_loss: 1.8123 - val_acc: 0.6630\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.7903 - acc: 0.6868 - val_loss: 1.7987 - val_acc: 0.6640\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.7794 - acc: 0.6855 - val_loss: 1.7888 - val_acc: 0.6620\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.7683 - acc: 0.6871 - val_loss: 1.7787 - val_acc: 0.6670\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.7578 - acc: 0.6861 - val_loss: 1.7689 - val_acc: 0.6710\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.7476 - acc: 0.6857 - val_loss: 1.7594 - val_acc: 0.6720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.7374 - acc: 0.6879 - val_loss: 1.7468 - val_acc: 0.6700\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.7270 - acc: 0.6887 - val_loss: 1.7383 - val_acc: 0.6760\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.7125 - acc: 0.692 - 0s 64us/step - loss: 1.7175 - acc: 0.6895 - val_loss: 1.7321 - val_acc: 0.6740\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.7083 - acc: 0.6901 - val_loss: 1.7194 - val_acc: 0.6620\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.6984 - acc: 0.6903 - val_loss: 1.7124 - val_acc: 0.6660\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.6892 - acc: 0.6915 - val_loss: 1.6995 - val_acc: 0.6750\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.6795 - acc: 0.6917 - val_loss: 1.6967 - val_acc: 0.6680\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.6711 - acc: 0.6940 - val_loss: 1.6839 - val_acc: 0.6750\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.6623 - acc: 0.6915 - val_loss: 1.6725 - val_acc: 0.6780\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.6531 - acc: 0.6927 - val_loss: 1.6651 - val_acc: 0.6710\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.6445 - acc: 0.6940 - val_loss: 1.6589 - val_acc: 0.6800\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.6361 - acc: 0.6940 - val_loss: 1.6497 - val_acc: 0.6760\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.6279 - acc: 0.6949 - val_loss: 1.6408 - val_acc: 0.6740\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.6197 - acc: 0.6957 - val_loss: 1.6342 - val_acc: 0.6780\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.6117 - acc: 0.6964 - val_loss: 1.6235 - val_acc: 0.6740\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.6033 - acc: 0.6956 - val_loss: 1.6168 - val_acc: 0.6790\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.5960 - acc: 0.6957 - val_loss: 1.6128 - val_acc: 0.6690\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.5880 - acc: 0.6973 - val_loss: 1.6005 - val_acc: 0.6790\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.5801 - acc: 0.6961 - val_loss: 1.5968 - val_acc: 0.6760\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.5731 - acc: 0.6975 - val_loss: 1.5882 - val_acc: 0.6710\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.5653 - acc: 0.6984 - val_loss: 1.5827 - val_acc: 0.6740\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.5581 - acc: 0.6999 - val_loss: 1.5699 - val_acc: 0.6760\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.5500 - acc: 0.6981 - val_loss: 1.5653 - val_acc: 0.6720\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.5431 - acc: 0.7001 - val_loss: 1.5566 - val_acc: 0.6830\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.5361 - acc: 0.6995 - val_loss: 1.5539 - val_acc: 0.6760\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.5292 - acc: 0.7012 - val_loss: 1.5448 - val_acc: 0.6820\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.5219 - acc: 0.7025 - val_loss: 1.5369 - val_acc: 0.6810\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.5146 - acc: 0.7033 - val_loss: 1.5307 - val_acc: 0.6760\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.5086 - acc: 0.7012 - val_loss: 1.5225 - val_acc: 0.6830\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.5015 - acc: 0.7025 - val_loss: 1.5253 - val_acc: 0.6760\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 1.4948 - acc: 0.7043 - val_loss: 1.5126 - val_acc: 0.6800\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.4887 - acc: 0.7035 - val_loss: 1.5038 - val_acc: 0.6820\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.4815 - acc: 0.7041 - val_loss: 1.4990 - val_acc: 0.6830\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 1.4751 - acc: 0.7041 - val_loss: 1.4940 - val_acc: 0.6850\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.4689 - acc: 0.7047 - val_loss: 1.4854 - val_acc: 0.6870\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.4621 - acc: 0.7061 - val_loss: 1.4796 - val_acc: 0.6900\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.4562 - acc: 0.7057 - val_loss: 1.4731 - val_acc: 0.6810\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.4499 - acc: 0.7039 - val_loss: 1.4659 - val_acc: 0.6860\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.4432 - acc: 0.7047 - val_loss: 1.4609 - val_acc: 0.6880\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.4376 - acc: 0.7072 - val_loss: 1.4576 - val_acc: 0.6820\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.4318 - acc: 0.7077 - val_loss: 1.4548 - val_acc: 0.6870\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.4254 - acc: 0.7073 - val_loss: 1.4431 - val_acc: 0.6850\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.4192 - acc: 0.7055 - val_loss: 1.4374 - val_acc: 0.6860\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.4139 - acc: 0.7077 - val_loss: 1.4326 - val_acc: 0.6890\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.4079 - acc: 0.7100 - val_loss: 1.4283 - val_acc: 0.6850\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.4025 - acc: 0.7079 - val_loss: 1.4228 - val_acc: 0.6920\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.3967 - acc: 0.7080 - val_loss: 1.4174 - val_acc: 0.6940\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.3909 - acc: 0.7112 - val_loss: 1.4091 - val_acc: 0.6900\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.3853 - acc: 0.7091 - val_loss: 1.4071 - val_acc: 0.6870\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.3803 - acc: 0.7097 - val_loss: 1.3993 - val_acc: 0.6860\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.3750 - acc: 0.7104 - val_loss: 1.3953 - val_acc: 0.6940\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.3694 - acc: 0.7105 - val_loss: 1.3922 - val_acc: 0.6900\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.3642 - acc: 0.7119 - val_loss: 1.3920 - val_acc: 0.6940\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.3591 - acc: 0.7105 - val_loss: 1.3839 - val_acc: 0.6980\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.3535 - acc: 0.7128 - val_loss: 1.3747 - val_acc: 0.6940\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.3486 - acc: 0.7109 - val_loss: 1.3698 - val_acc: 0.6910\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.3439 - acc: 0.7117 - val_loss: 1.3650 - val_acc: 0.6960\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.3390 - acc: 0.7113 - val_loss: 1.3613 - val_acc: 0.6940\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.3339 - acc: 0.7120 - val_loss: 1.3568 - val_acc: 0.6950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.3287 - acc: 0.7127 - val_loss: 1.3512 - val_acc: 0.6960\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.3238 - acc: 0.7127 - val_loss: 1.3469 - val_acc: 0.6960\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.3188 - acc: 0.7141 - val_loss: 1.3421 - val_acc: 0.6960\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.3142 - acc: 0.7147 - val_loss: 1.3372 - val_acc: 0.6980\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.3096 - acc: 0.7140 - val_loss: 1.3323 - val_acc: 0.6970\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.3048 - acc: 0.7144 - val_loss: 1.3276 - val_acc: 0.6970\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.3003 - acc: 0.7155 - val_loss: 1.3250 - val_acc: 0.7030\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.2956 - acc: 0.7143 - val_loss: 1.3235 - val_acc: 0.6980\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.2913 - acc: 0.7159 - val_loss: 1.3172 - val_acc: 0.6970\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.2869 - acc: 0.7153 - val_loss: 1.3132 - val_acc: 0.6990\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.2824 - acc: 0.7164 - val_loss: 1.3089 - val_acc: 0.6970\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.2788 - acc: 0.7169 - val_loss: 1.3016 - val_acc: 0.6990\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.2735 - acc: 0.7181 - val_loss: 1.2994 - val_acc: 0.7030\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.2696 - acc: 0.7185 - val_loss: 1.2979 - val_acc: 0.7110\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.2653 - acc: 0.7187 - val_loss: 1.2895 - val_acc: 0.7090\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.2603 - acc: 0.7188 - val_loss: 1.2869 - val_acc: 0.7030\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.2569 - acc: 0.7188 - val_loss: 1.2838 - val_acc: 0.7000\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.2522 - acc: 0.7195 - val_loss: 1.2781 - val_acc: 0.7060\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.2479 - acc: 0.7207 - val_loss: 1.2752 - val_acc: 0.7080\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.2443 - acc: 0.7205 - val_loss: 1.2756 - val_acc: 0.7070\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.2405 - acc: 0.7215 - val_loss: 1.2673 - val_acc: 0.7080\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.2364 - acc: 0.7205 - val_loss: 1.2635 - val_acc: 0.7020\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.2326 - acc: 0.7224 - val_loss: 1.2590 - val_acc: 0.7060\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.2280 - acc: 0.7221 - val_loss: 1.2622 - val_acc: 0.7040\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.2248 - acc: 0.7221 - val_loss: 1.2526 - val_acc: 0.7050\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.2214 - acc: 0.7232 - val_loss: 1.2495 - val_acc: 0.7060\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.2177 - acc: 0.7223 - val_loss: 1.2479 - val_acc: 0.7140\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.2138 - acc: 0.7225 - val_loss: 1.2414 - val_acc: 0.7120\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.2104 - acc: 0.7225 - val_loss: 1.2425 - val_acc: 0.7140\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.2069 - acc: 0.7239 - val_loss: 1.2385 - val_acc: 0.7100\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.2028 - acc: 0.7232 - val_loss: 1.2343 - val_acc: 0.7100\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1999 - acc: 0.7233 - val_loss: 1.2282 - val_acc: 0.7100\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.1958 - acc: 0.7249 - val_loss: 1.2256 - val_acc: 0.7080\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.1921 - acc: 0.7251 - val_loss: 1.2299 - val_acc: 0.7110\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.1894 - acc: 0.7235 - val_loss: 1.2197 - val_acc: 0.7100\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.1864 - acc: 0.7231 - val_loss: 1.2255 - val_acc: 0.7130\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.1827 - acc: 0.7247 - val_loss: 1.2160 - val_acc: 0.7150\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.1798 - acc: 0.7271 - val_loss: 1.2100 - val_acc: 0.7120\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.1761 - acc: 0.7271 - val_loss: 1.2121 - val_acc: 0.7130\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1733 - acc: 0.7272 - val_loss: 1.2036 - val_acc: 0.7140\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.1703 - acc: 0.7255 - val_loss: 1.2082 - val_acc: 0.7140\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.1670 - acc: 0.7256 - val_loss: 1.2009 - val_acc: 0.7140\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.1642 - acc: 0.7276 - val_loss: 1.2025 - val_acc: 0.7130\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.1613 - acc: 0.7263 - val_loss: 1.1944 - val_acc: 0.7100\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.1580 - acc: 0.7260 - val_loss: 1.1911 - val_acc: 0.7110\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.1550 - acc: 0.7265 - val_loss: 1.1907 - val_acc: 0.7110\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1524 - acc: 0.7279 - val_loss: 1.1893 - val_acc: 0.7180\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.1498 - acc: 0.7283 - val_loss: 1.1831 - val_acc: 0.7150\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1473 - acc: 0.7276 - val_loss: 1.1852 - val_acc: 0.7170\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.1443 - acc: 0.7257 - val_loss: 1.1829 - val_acc: 0.7110\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1416 - acc: 0.7284 - val_loss: 1.1769 - val_acc: 0.7110\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1387 - acc: 0.7283 - val_loss: 1.1749 - val_acc: 0.7130\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.1364 - acc: 0.7288 - val_loss: 1.1754 - val_acc: 0.7190\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.1336 - acc: 0.7297 - val_loss: 1.1705 - val_acc: 0.7130\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1316 - acc: 0.7304 - val_loss: 1.1665 - val_acc: 0.7130\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.1287 - acc: 0.7304 - val_loss: 1.1712 - val_acc: 0.7150\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.1268 - acc: 0.7309 - val_loss: 1.1717 - val_acc: 0.7170\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1251 - acc: 0.7301 - val_loss: 1.1620 - val_acc: 0.7160\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1219 - acc: 0.7300 - val_loss: 1.1600 - val_acc: 0.7180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1193 - acc: 0.7313 - val_loss: 1.1593 - val_acc: 0.7140\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.1165 - acc: 0.7304 - val_loss: 1.1554 - val_acc: 0.7120\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1147 - acc: 0.7308 - val_loss: 1.1509 - val_acc: 0.7140\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1119 - acc: 0.7313 - val_loss: 1.1516 - val_acc: 0.7200\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.1099 - acc: 0.7332 - val_loss: 1.1479 - val_acc: 0.7170\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1079 - acc: 0.7331 - val_loss: 1.1477 - val_acc: 0.7160\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.1060 - acc: 0.7336 - val_loss: 1.1431 - val_acc: 0.7160\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.1041 - acc: 0.7313 - val_loss: 1.1464 - val_acc: 0.7110\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1024 - acc: 0.7345 - val_loss: 1.1422 - val_acc: 0.7200\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0994 - acc: 0.7336 - val_loss: 1.1387 - val_acc: 0.7150\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0972 - acc: 0.7339 - val_loss: 1.1363 - val_acc: 0.7180\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0955 - acc: 0.7347 - val_loss: 1.1350 - val_acc: 0.7210\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0934 - acc: 0.7359 - val_loss: 1.1374 - val_acc: 0.7150\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0920 - acc: 0.7359 - val_loss: 1.1338 - val_acc: 0.7150\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0903 - acc: 0.7361 - val_loss: 1.1372 - val_acc: 0.7190\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0885 - acc: 0.7349 - val_loss: 1.1290 - val_acc: 0.7150\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0852 - acc: 0.7369 - val_loss: 1.1254 - val_acc: 0.7150\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0842 - acc: 0.7385 - val_loss: 1.1250 - val_acc: 0.7150\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0821 - acc: 0.7369 - val_loss: 1.1274 - val_acc: 0.7230\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0807 - acc: 0.7376 - val_loss: 1.1205 - val_acc: 0.7190\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0784 - acc: 0.7383 - val_loss: 1.1185 - val_acc: 0.7190\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0761 - acc: 0.7369 - val_loss: 1.1197 - val_acc: 0.7130\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0752 - acc: 0.7379 - val_loss: 1.1218 - val_acc: 0.7120\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.0734 - acc: 0.7395 - val_loss: 1.1174 - val_acc: 0.7110\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 1.0714 - acc: 0.7392 - val_loss: 1.1125 - val_acc: 0.7160\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.0699 - acc: 0.7383 - val_loss: 1.1222 - val_acc: 0.7090\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.0682 - acc: 0.7397 - val_loss: 1.1092 - val_acc: 0.7220\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.0664 - acc: 0.7405 - val_loss: 1.1090 - val_acc: 0.7160\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.0643 - acc: 0.7413 - val_loss: 1.1215 - val_acc: 0.7220\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.0634 - acc: 0.7400 - val_loss: 1.1091 - val_acc: 0.7180\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.0612 - acc: 0.7392 - val_loss: 1.1055 - val_acc: 0.7170\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0605 - acc: 0.7411 - val_loss: 1.1030 - val_acc: 0.7160\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.0589 - acc: 0.7408 - val_loss: 1.1085 - val_acc: 0.7220\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 1.0567 - acc: 0.7439 - val_loss: 1.1000 - val_acc: 0.7180\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 1.0553 - acc: 0.7417 - val_loss: 1.1015 - val_acc: 0.7120\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.0532 - acc: 0.7449 - val_loss: 1.0998 - val_acc: 0.7250\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.0524 - acc: 0.7440 - val_loss: 1.0955 - val_acc: 0.7180\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 1.0504 - acc: 0.7420 - val_loss: 1.0978 - val_acc: 0.7120\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.0497 - acc: 0.7436 - val_loss: 1.0942 - val_acc: 0.7240\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.0476 - acc: 0.7435 - val_loss: 1.0960 - val_acc: 0.7220\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0459 - acc: 0.7439 - val_loss: 1.0914 - val_acc: 0.7150\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0448 - acc: 0.7451 - val_loss: 1.0880 - val_acc: 0.7180\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0428 - acc: 0.7445 - val_loss: 1.0884 - val_acc: 0.7230\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0419 - acc: 0.7444 - val_loss: 1.0860 - val_acc: 0.7140\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0404 - acc: 0.7469 - val_loss: 1.0921 - val_acc: 0.7130\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0394 - acc: 0.7459 - val_loss: 1.0829 - val_acc: 0.7250\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0380 - acc: 0.7465 - val_loss: 1.0882 - val_acc: 0.7240\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0365 - acc: 0.7439 - val_loss: 1.0839 - val_acc: 0.7120\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.0357 - acc: 0.7463 - val_loss: 1.0938 - val_acc: 0.7150\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.0341 - acc: 0.7468 - val_loss: 1.0835 - val_acc: 0.7260\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0320 - acc: 0.7464 - val_loss: 1.0762 - val_acc: 0.7210\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0306 - acc: 0.7480 - val_loss: 1.0753 - val_acc: 0.7120\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0292 - acc: 0.7479 - val_loss: 1.0769 - val_acc: 0.7190\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0292 - acc: 0.7471 - val_loss: 1.0758 - val_acc: 0.7260\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0270 - acc: 0.7483 - val_loss: 1.0742 - val_acc: 0.7200\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0262 - acc: 0.7476 - val_loss: 1.0707 - val_acc: 0.7170\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0247 - acc: 0.7485 - val_loss: 1.0745 - val_acc: 0.7290\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0234 - acc: 0.7481 - val_loss: 1.0695 - val_acc: 0.7280\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0219 - acc: 0.7499 - val_loss: 1.0714 - val_acc: 0.7250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0213 - acc: 0.7504 - val_loss: 1.0667 - val_acc: 0.7180\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.0198 - acc: 0.7499 - val_loss: 1.0776 - val_acc: 0.7310\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0193 - acc: 0.7496 - val_loss: 1.0747 - val_acc: 0.7270\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0176 - acc: 0.7504 - val_loss: 1.0649 - val_acc: 0.7130\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0162 - acc: 0.7496 - val_loss: 1.0734 - val_acc: 0.7120\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0154 - acc: 0.7513 - val_loss: 1.0628 - val_acc: 0.7320\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0138 - acc: 0.7499 - val_loss: 1.0593 - val_acc: 0.7240\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0124 - acc: 0.7503 - val_loss: 1.0603 - val_acc: 0.7170\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0113 - acc: 0.7517 - val_loss: 1.0623 - val_acc: 0.7210\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0110 - acc: 0.7491 - val_loss: 1.0628 - val_acc: 0.7290\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0096 - acc: 0.7512 - val_loss: 1.0573 - val_acc: 0.7160\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0078 - acc: 0.7515 - val_loss: 1.0622 - val_acc: 0.7270\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0069 - acc: 0.7507 - val_loss: 1.0546 - val_acc: 0.7190\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0060 - acc: 0.7519 - val_loss: 1.0561 - val_acc: 0.7180\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0052 - acc: 0.7520 - val_loss: 1.0615 - val_acc: 0.7280\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0043 - acc: 0.7529 - val_loss: 1.0527 - val_acc: 0.7300\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0027 - acc: 0.7505 - val_loss: 1.0520 - val_acc: 0.7240\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0021 - acc: 0.7528 - val_loss: 1.0485 - val_acc: 0.7220\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0003 - acc: 0.7551 - val_loss: 1.0684 - val_acc: 0.7300\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0000 - acc: 0.7533 - val_loss: 1.0615 - val_acc: 0.7300\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9991 - acc: 0.7549 - val_loss: 1.0486 - val_acc: 0.7320\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9975 - acc: 0.7537 - val_loss: 1.0509 - val_acc: 0.7300\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9969 - acc: 0.7533 - val_loss: 1.0468 - val_acc: 0.7230\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9971 - acc: 0.7544 - val_loss: 1.0633 - val_acc: 0.7280\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9948 - acc: 0.7551 - val_loss: 1.0445 - val_acc: 0.7160\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9930 - acc: 0.7551 - val_loss: 1.0430 - val_acc: 0.7170\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9929 - acc: 0.7543 - val_loss: 1.0446 - val_acc: 0.7280\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9923 - acc: 0.7564 - val_loss: 1.0416 - val_acc: 0.7240\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9905 - acc: 0.7576 - val_loss: 1.0470 - val_acc: 0.7330\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9903 - acc: 0.7563 - val_loss: 1.0393 - val_acc: 0.7270\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9888 - acc: 0.7563 - val_loss: 1.0595 - val_acc: 0.7300\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.9889 - acc: 0.7556 - val_loss: 1.0368 - val_acc: 0.7260\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.9863 - acc: 0.7580 - val_loss: 1.0434 - val_acc: 0.7290\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.9864 - acc: 0.7559 - val_loss: 1.0384 - val_acc: 0.7190\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.9849 - acc: 0.7564 - val_loss: 1.0361 - val_acc: 0.7310\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9842 - acc: 0.7573 - val_loss: 1.0352 - val_acc: 0.7270\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9831 - acc: 0.7581 - val_loss: 1.0372 - val_acc: 0.7300\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9823 - acc: 0.7583 - val_loss: 1.0458 - val_acc: 0.7280\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9823 - acc: 0.7561 - val_loss: 1.0311 - val_acc: 0.7240\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9801 - acc: 0.7575 - val_loss: 1.0316 - val_acc: 0.7300\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9800 - acc: 0.7589 - val_loss: 1.0321 - val_acc: 0.7330\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9791 - acc: 0.7585 - val_loss: 1.0326 - val_acc: 0.7270\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9782 - acc: 0.7581 - val_loss: 1.0305 - val_acc: 0.7310\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9772 - acc: 0.7596 - val_loss: 1.0303 - val_acc: 0.7360\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9763 - acc: 0.7600 - val_loss: 1.0309 - val_acc: 0.7260\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9763 - acc: 0.7573 - val_loss: 1.0265 - val_acc: 0.7250\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9746 - acc: 0.7600 - val_loss: 1.0271 - val_acc: 0.7200\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9739 - acc: 0.7591 - val_loss: 1.0243 - val_acc: 0.7190\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9738 - acc: 0.7600 - val_loss: 1.0252 - val_acc: 0.7230\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.9721 - acc: 0.7611 - val_loss: 1.0252 - val_acc: 0.7170\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.9711 - acc: 0.7603 - val_loss: 1.0225 - val_acc: 0.7270\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.9702 - acc: 0.7613 - val_loss: 1.0221 - val_acc: 0.7250\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.9695 - acc: 0.7584 - val_loss: 1.0224 - val_acc: 0.7320\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.9693 - acc: 0.7601 - val_loss: 1.0287 - val_acc: 0.7360\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9682 - acc: 0.7620 - val_loss: 1.0236 - val_acc: 0.7320\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.9675 - acc: 0.7623 - val_loss: 1.0226 - val_acc: 0.7280\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9663 - acc: 0.7621 - val_loss: 1.0193 - val_acc: 0.7290\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.9660 - acc: 0.7612 - val_loss: 1.0263 - val_acc: 0.7220\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9663 - acc: 0.7595 - val_loss: 1.0176 - val_acc: 0.7320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9648 - acc: 0.7625 - val_loss: 1.0147 - val_acc: 0.7260\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9637 - acc: 0.7615 - val_loss: 1.0154 - val_acc: 0.7330\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9626 - acc: 0.7617 - val_loss: 1.0197 - val_acc: 0.7300\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.9624 - acc: 0.7636 - val_loss: 1.0165 - val_acc: 0.7340\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9608 - acc: 0.7639 - val_loss: 1.0160 - val_acc: 0.7290\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9604 - acc: 0.7629 - val_loss: 1.0124 - val_acc: 0.7200\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9598 - acc: 0.7627 - val_loss: 1.0206 - val_acc: 0.7380\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9596 - acc: 0.7625 - val_loss: 1.0181 - val_acc: 0.7330\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9586 - acc: 0.7639 - val_loss: 1.0164 - val_acc: 0.7360\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9579 - acc: 0.7635 - val_loss: 1.0152 - val_acc: 0.7370\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9565 - acc: 0.7653 - val_loss: 1.0123 - val_acc: 0.7300\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9564 - acc: 0.7648 - val_loss: 1.0075 - val_acc: 0.7270\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9557 - acc: 0.7648 - val_loss: 1.0069 - val_acc: 0.7290\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9547 - acc: 0.7636 - val_loss: 1.0097 - val_acc: 0.7350\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9546 - acc: 0.7628 - val_loss: 1.0113 - val_acc: 0.7330\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9545 - acc: 0.7637 - val_loss: 1.0070 - val_acc: 0.7230\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9532 - acc: 0.7649 - val_loss: 1.0053 - val_acc: 0.7260\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9523 - acc: 0.7629 - val_loss: 1.0042 - val_acc: 0.7220\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9519 - acc: 0.7655 - val_loss: 1.0042 - val_acc: 0.7260\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9514 - acc: 0.7649 - val_loss: 1.0038 - val_acc: 0.7230\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9500 - acc: 0.7636 - val_loss: 1.0037 - val_acc: 0.7340\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9500 - acc: 0.7659 - val_loss: 1.0030 - val_acc: 0.7210\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9485 - acc: 0.7647 - val_loss: 1.0035 - val_acc: 0.7230\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9485 - acc: 0.7648 - val_loss: 1.0025 - val_acc: 0.7320\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9469 - acc: 0.7655 - val_loss: 1.0038 - val_acc: 0.7360\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9469 - acc: 0.7659 - val_loss: 1.0005 - val_acc: 0.7300\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9466 - acc: 0.7652 - val_loss: 0.9990 - val_acc: 0.7340\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9454 - acc: 0.7644 - val_loss: 1.0022 - val_acc: 0.7380\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9446 - acc: 0.7680 - val_loss: 1.0011 - val_acc: 0.7340\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9449 - acc: 0.7653 - val_loss: 0.9990 - val_acc: 0.7340\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9440 - acc: 0.7680 - val_loss: 1.0085 - val_acc: 0.7340\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9429 - acc: 0.7656 - val_loss: 0.9970 - val_acc: 0.7290\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9420 - acc: 0.7669 - val_loss: 0.9993 - val_acc: 0.7290\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9434 - acc: 0.7660 - val_loss: 1.0041 - val_acc: 0.7410\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9419 - acc: 0.7664 - val_loss: 0.9971 - val_acc: 0.7340\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9403 - acc: 0.7652 - val_loss: 0.9988 - val_acc: 0.7300\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9394 - acc: 0.7680 - val_loss: 0.9923 - val_acc: 0.7340\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9386 - acc: 0.7667 - val_loss: 0.9992 - val_acc: 0.7410\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9387 - acc: 0.7663 - val_loss: 0.9991 - val_acc: 0.7390\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9382 - acc: 0.7675 - val_loss: 0.9998 - val_acc: 0.7400\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9378 - acc: 0.7684 - val_loss: 0.9907 - val_acc: 0.7340\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9362 - acc: 0.7684 - val_loss: 1.0261 - val_acc: 0.7300\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9378 - acc: 0.7661 - val_loss: 0.9931 - val_acc: 0.7350\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9355 - acc: 0.7665 - val_loss: 0.9986 - val_acc: 0.7390\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9350 - acc: 0.7704 - val_loss: 1.0034 - val_acc: 0.7340\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9348 - acc: 0.7661 - val_loss: 0.9987 - val_acc: 0.7410\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9346 - acc: 0.7683 - val_loss: 0.9890 - val_acc: 0.7340\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9329 - acc: 0.7677 - val_loss: 0.9873 - val_acc: 0.7350\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9322 - acc: 0.7699 - val_loss: 0.9867 - val_acc: 0.7310\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9324 - acc: 0.7680 - val_loss: 1.0007 - val_acc: 0.7240\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9332 - acc: 0.7660 - val_loss: 0.9971 - val_acc: 0.7340\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9319 - acc: 0.7700 - val_loss: 0.9877 - val_acc: 0.7350\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9303 - acc: 0.7693 - val_loss: 1.0016 - val_acc: 0.7380\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9296 - acc: 0.7684 - val_loss: 0.9884 - val_acc: 0.7300\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9302 - acc: 0.7691 - val_loss: 0.9968 - val_acc: 0.7360\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9296 - acc: 0.7689 - val_loss: 0.9864 - val_acc: 0.7300\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9289 - acc: 0.7679 - val_loss: 0.9831 - val_acc: 0.7370\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9288 - acc: 0.7684 - val_loss: 0.9847 - val_acc: 0.7370\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9262 - acc: 0.7712 - val_loss: 0.9836 - val_acc: 0.7360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9271 - acc: 0.7701 - val_loss: 0.9867 - val_acc: 0.7400\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9268 - acc: 0.7701 - val_loss: 1.0038 - val_acc: 0.7200\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9262 - acc: 0.7709 - val_loss: 0.9891 - val_acc: 0.7390\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9257 - acc: 0.7705 - val_loss: 0.9864 - val_acc: 0.7310\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9241 - acc: 0.7717 - val_loss: 0.9826 - val_acc: 0.7360\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9243 - acc: 0.7696 - val_loss: 1.0087 - val_acc: 0.7290\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9240 - acc: 0.7712 - val_loss: 0.9807 - val_acc: 0.7310\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9237 - acc: 0.7724 - val_loss: 0.9824 - val_acc: 0.7410\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9226 - acc: 0.7727 - val_loss: 0.9866 - val_acc: 0.7270\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9222 - acc: 0.7703 - val_loss: 0.9802 - val_acc: 0.7380\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9222 - acc: 0.7711 - val_loss: 0.9794 - val_acc: 0.7400\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9218 - acc: 0.7727 - val_loss: 0.9785 - val_acc: 0.7350\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9208 - acc: 0.7721 - val_loss: 0.9762 - val_acc: 0.7380\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9202 - acc: 0.7711 - val_loss: 0.9835 - val_acc: 0.7350\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9194 - acc: 0.7717 - val_loss: 0.9808 - val_acc: 0.7400\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9191 - acc: 0.7713 - val_loss: 0.9765 - val_acc: 0.7330\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9193 - acc: 0.7696 - val_loss: 0.9791 - val_acc: 0.7360\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9183 - acc: 0.7704 - val_loss: 0.9774 - val_acc: 0.7330\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9173 - acc: 0.7725 - val_loss: 0.9794 - val_acc: 0.7400\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9160 - acc: 0.7731 - val_loss: 0.9794 - val_acc: 0.7410\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9169 - acc: 0.7713 - val_loss: 0.9802 - val_acc: 0.7320\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9155 - acc: 0.7725 - val_loss: 0.9764 - val_acc: 0.7410\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9166 - acc: 0.7728 - val_loss: 0.9753 - val_acc: 0.7410\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9146 - acc: 0.7719 - val_loss: 0.9755 - val_acc: 0.7400\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9141 - acc: 0.7739 - val_loss: 0.9757 - val_acc: 0.7360\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9144 - acc: 0.7715 - val_loss: 0.9822 - val_acc: 0.7310\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9131 - acc: 0.7733 - val_loss: 0.9720 - val_acc: 0.7370\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9132 - acc: 0.7729 - val_loss: 0.9752 - val_acc: 0.7370\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9127 - acc: 0.7735 - val_loss: 0.9817 - val_acc: 0.7350\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9127 - acc: 0.7725 - val_loss: 0.9745 - val_acc: 0.7340\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9116 - acc: 0.7739 - val_loss: 0.9712 - val_acc: 0.7430\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9113 - acc: 0.7721 - val_loss: 0.9725 - val_acc: 0.7420\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9103 - acc: 0.7739 - val_loss: 0.9749 - val_acc: 0.7430\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9109 - acc: 0.7720 - val_loss: 0.9720 - val_acc: 0.7400\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9114 - acc: 0.7733 - val_loss: 0.9685 - val_acc: 0.7380\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9091 - acc: 0.7744 - val_loss: 0.9705 - val_acc: 0.7380\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9095 - acc: 0.7740 - val_loss: 0.9721 - val_acc: 0.7350\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9085 - acc: 0.7731 - val_loss: 0.9796 - val_acc: 0.7410\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9085 - acc: 0.7755 - val_loss: 0.9700 - val_acc: 0.7410\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9076 - acc: 0.7720 - val_loss: 0.9780 - val_acc: 0.7400\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9076 - acc: 0.7749 - val_loss: 0.9684 - val_acc: 0.7370\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9067 - acc: 0.7736 - val_loss: 1.0113 - val_acc: 0.7350\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9077 - acc: 0.7736 - val_loss: 0.9730 - val_acc: 0.7440\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9060 - acc: 0.7755 - val_loss: 0.9687 - val_acc: 0.7390\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9053 - acc: 0.7741 - val_loss: 0.9703 - val_acc: 0.7360\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9065 - acc: 0.7756 - val_loss: 0.9642 - val_acc: 0.7370\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9050 - acc: 0.7743 - val_loss: 0.9669 - val_acc: 0.7400\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9040 - acc: 0.7748 - val_loss: 0.9688 - val_acc: 0.7460\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9042 - acc: 0.7757 - val_loss: 0.9776 - val_acc: 0.7310\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9043 - acc: 0.7747 - val_loss: 0.9690 - val_acc: 0.7310\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9039 - acc: 0.7749 - val_loss: 0.9681 - val_acc: 0.7370\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9035 - acc: 0.7756 - val_loss: 0.9794 - val_acc: 0.7410\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9031 - acc: 0.7760 - val_loss: 0.9849 - val_acc: 0.7360\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9031 - acc: 0.7764 - val_loss: 0.9696 - val_acc: 0.7380\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9021 - acc: 0.7732 - val_loss: 0.9865 - val_acc: 0.7330\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9025 - acc: 0.7740 - val_loss: 0.9729 - val_acc: 0.7450\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9013 - acc: 0.7767 - val_loss: 0.9626 - val_acc: 0.7390\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8994 - acc: 0.7756 - val_loss: 0.9656 - val_acc: 0.7430\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8996 - acc: 0.7765 - val_loss: 0.9634 - val_acc: 0.7400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8989 - acc: 0.7749 - val_loss: 0.9661 - val_acc: 0.7340\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8987 - acc: 0.7780 - val_loss: 0.9809 - val_acc: 0.7410\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8989 - acc: 0.7768 - val_loss: 0.9663 - val_acc: 0.7450\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8978 - acc: 0.7757 - val_loss: 0.9615 - val_acc: 0.7340\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8967 - acc: 0.7765 - val_loss: 0.9628 - val_acc: 0.7390\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8971 - acc: 0.7761 - val_loss: 0.9639 - val_acc: 0.7380\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8974 - acc: 0.7765 - val_loss: 0.9595 - val_acc: 0.7390\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8963 - acc: 0.7761 - val_loss: 0.9596 - val_acc: 0.7390\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8952 - acc: 0.7768 - val_loss: 0.9619 - val_acc: 0.7460\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8955 - acc: 0.7788 - val_loss: 0.9574 - val_acc: 0.7400\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8964 - acc: 0.7781 - val_loss: 0.9567 - val_acc: 0.7360\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8944 - acc: 0.7779 - val_loss: 0.9587 - val_acc: 0.7410\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8939 - acc: 0.7767 - val_loss: 0.9651 - val_acc: 0.7380\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8938 - acc: 0.7775 - val_loss: 0.9607 - val_acc: 0.7380\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8940 - acc: 0.7763 - val_loss: 0.9722 - val_acc: 0.7450\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8928 - acc: 0.7780 - val_loss: 0.9726 - val_acc: 0.7430\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8938 - acc: 0.7756 - val_loss: 0.9667 - val_acc: 0.7360\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8930 - acc: 0.7775 - val_loss: 1.0027 - val_acc: 0.7270\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8935 - acc: 0.7797 - val_loss: 0.9562 - val_acc: 0.7420\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8924 - acc: 0.7793 - val_loss: 0.9567 - val_acc: 0.7350\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8910 - acc: 0.7788 - val_loss: 0.9638 - val_acc: 0.7460\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8914 - acc: 0.7768 - val_loss: 0.9591 - val_acc: 0.7380\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8904 - acc: 0.7795 - val_loss: 0.9567 - val_acc: 0.7390\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8898 - acc: 0.7784 - val_loss: 0.9574 - val_acc: 0.7390\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8902 - acc: 0.7783 - val_loss: 0.9539 - val_acc: 0.7370\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8900 - acc: 0.7785 - val_loss: 0.9561 - val_acc: 0.7460\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8883 - acc: 0.7792 - val_loss: 0.9622 - val_acc: 0.7380\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8891 - acc: 0.7781 - val_loss: 0.9567 - val_acc: 0.7430\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8882 - acc: 0.7805 - val_loss: 0.9516 - val_acc: 0.7420\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8875 - acc: 0.7784 - val_loss: 0.9594 - val_acc: 0.7400\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8879 - acc: 0.7801 - val_loss: 0.9550 - val_acc: 0.7420\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8871 - acc: 0.7804 - val_loss: 0.9524 - val_acc: 0.7410\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8872 - acc: 0.7801 - val_loss: 0.9515 - val_acc: 0.7370\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8860 - acc: 0.7812 - val_loss: 0.9569 - val_acc: 0.7390\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8860 - acc: 0.7797 - val_loss: 0.9541 - val_acc: 0.7390\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8862 - acc: 0.7795 - val_loss: 0.9511 - val_acc: 0.7410\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8860 - acc: 0.7783 - val_loss: 0.9531 - val_acc: 0.7420\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8839 - acc: 0.7804 - val_loss: 0.9508 - val_acc: 0.7430\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8839 - acc: 0.7805 - val_loss: 0.9528 - val_acc: 0.7420\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8838 - acc: 0.7824 - val_loss: 0.9509 - val_acc: 0.7390\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8828 - acc: 0.7823 - val_loss: 0.9485 - val_acc: 0.7380\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8827 - acc: 0.7808 - val_loss: 0.9488 - val_acc: 0.7420\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8830 - acc: 0.7801 - val_loss: 0.9550 - val_acc: 0.7400\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8833 - acc: 0.7808 - val_loss: 0.9697 - val_acc: 0.7460\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8834 - acc: 0.7791 - val_loss: 0.9473 - val_acc: 0.7410\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8826 - acc: 0.7805 - val_loss: 0.9487 - val_acc: 0.7430\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8814 - acc: 0.7800 - val_loss: 0.9644 - val_acc: 0.7450\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8822 - acc: 0.7800 - val_loss: 0.9463 - val_acc: 0.7410\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8805 - acc: 0.7828 - val_loss: 0.9548 - val_acc: 0.7410\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8799 - acc: 0.7803 - val_loss: 0.9467 - val_acc: 0.7380\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8805 - acc: 0.7804 - val_loss: 0.9453 - val_acc: 0.7420\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8797 - acc: 0.7828 - val_loss: 0.9552 - val_acc: 0.7390\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8794 - acc: 0.7807 - val_loss: 0.9540 - val_acc: 0.7390\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8787 - acc: 0.7819 - val_loss: 0.9470 - val_acc: 0.7380\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8790 - acc: 0.7816 - val_loss: 0.9469 - val_acc: 0.7450\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8789 - acc: 0.7812 - val_loss: 0.9506 - val_acc: 0.7430\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8793 - acc: 0.7813 - val_loss: 0.9438 - val_acc: 0.7410\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8782 - acc: 0.7819 - val_loss: 0.9448 - val_acc: 0.7460\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8771 - acc: 0.7819 - val_loss: 0.9543 - val_acc: 0.7420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8768 - acc: 0.7824 - val_loss: 0.9439 - val_acc: 0.7410\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8756 - acc: 0.7821 - val_loss: 0.9499 - val_acc: 0.7480\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8760 - acc: 0.7835 - val_loss: 0.9503 - val_acc: 0.7390\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8749 - acc: 0.7809 - val_loss: 0.9607 - val_acc: 0.7420\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8754 - acc: 0.7815 - val_loss: 0.9652 - val_acc: 0.7460\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8758 - acc: 0.7813 - val_loss: 0.9430 - val_acc: 0.7420\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8758 - acc: 0.7796 - val_loss: 0.9478 - val_acc: 0.7420\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8749 - acc: 0.7828 - val_loss: 0.9472 - val_acc: 0.7450\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8739 - acc: 0.7832 - val_loss: 0.9475 - val_acc: 0.7440\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8745 - acc: 0.7813 - val_loss: 0.9422 - val_acc: 0.7430\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8729 - acc: 0.7805 - val_loss: 0.9461 - val_acc: 0.7450\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8730 - acc: 0.7808 - val_loss: 0.9503 - val_acc: 0.7410\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8728 - acc: 0.7835 - val_loss: 0.9437 - val_acc: 0.7450\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8725 - acc: 0.7812 - val_loss: 0.9422 - val_acc: 0.7500\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8727 - acc: 0.7832 - val_loss: 0.9477 - val_acc: 0.7440\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8710 - acc: 0.7824 - val_loss: 0.9443 - val_acc: 0.7430\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8719 - acc: 0.7823 - val_loss: 0.9496 - val_acc: 0.7470\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8717 - acc: 0.7817 - val_loss: 0.9530 - val_acc: 0.7540\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8716 - acc: 0.7831 - val_loss: 0.9422 - val_acc: 0.7450\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8723 - acc: 0.7841 - val_loss: 0.9500 - val_acc: 0.7420\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8718 - acc: 0.7825 - val_loss: 0.9437 - val_acc: 0.7490\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8713 - acc: 0.7833 - val_loss: 0.9423 - val_acc: 0.7460\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8700 - acc: 0.7827 - val_loss: 0.9389 - val_acc: 0.7420\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8702 - acc: 0.7840 - val_loss: 0.9476 - val_acc: 0.7540\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8682 - acc: 0.7836 - val_loss: 0.9597 - val_acc: 0.7440\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8716 - acc: 0.7833 - val_loss: 0.9389 - val_acc: 0.7430\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8691 - acc: 0.7837 - val_loss: 0.9385 - val_acc: 0.7420\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8692 - acc: 0.7827 - val_loss: 0.9392 - val_acc: 0.7400\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8685 - acc: 0.7843 - val_loss: 0.9378 - val_acc: 0.7500\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8678 - acc: 0.7853 - val_loss: 0.9423 - val_acc: 0.7420\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8684 - acc: 0.7847 - val_loss: 0.9555 - val_acc: 0.7440\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8684 - acc: 0.7852 - val_loss: 0.9451 - val_acc: 0.7450\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8661 - acc: 0.7851 - val_loss: 0.9402 - val_acc: 0.7460\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8685 - acc: 0.7839 - val_loss: 0.9627 - val_acc: 0.7380\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8678 - acc: 0.7840 - val_loss: 0.9714 - val_acc: 0.7440\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8664 - acc: 0.7861 - val_loss: 0.9412 - val_acc: 0.7430\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8666 - acc: 0.7843 - val_loss: 0.9446 - val_acc: 0.7420\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8660 - acc: 0.7844 - val_loss: 0.9496 - val_acc: 0.7390\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8646 - acc: 0.7875 - val_loss: 0.9357 - val_acc: 0.7430\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8661 - acc: 0.7836 - val_loss: 0.9393 - val_acc: 0.7420\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8671 - acc: 0.7824 - val_loss: 0.9400 - val_acc: 0.7490\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8651 - acc: 0.7857 - val_loss: 0.9359 - val_acc: 0.7460\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8638 - acc: 0.7867 - val_loss: 0.9358 - val_acc: 0.7490\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8644 - acc: 0.7841 - val_loss: 0.9363 - val_acc: 0.7490\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8628 - acc: 0.7856 - val_loss: 0.9416 - val_acc: 0.7560\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8655 - acc: 0.7855 - val_loss: 0.9398 - val_acc: 0.7510\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8619 - acc: 0.7848 - val_loss: 0.9394 - val_acc: 0.7480\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8624 - acc: 0.7867 - val_loss: 0.9344 - val_acc: 0.7520\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8636 - acc: 0.7841 - val_loss: 0.9405 - val_acc: 0.7570\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8619 - acc: 0.7856 - val_loss: 0.9346 - val_acc: 0.7440\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8627 - acc: 0.7868 - val_loss: 0.9355 - val_acc: 0.7470\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8622 - acc: 0.7859 - val_loss: 0.9662 - val_acc: 0.7410\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8619 - acc: 0.7868 - val_loss: 0.9412 - val_acc: 0.7540\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8613 - acc: 0.7871 - val_loss: 0.9416 - val_acc: 0.7440\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8604 - acc: 0.7875 - val_loss: 0.9434 - val_acc: 0.7450\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8603 - acc: 0.7883 - val_loss: 0.9443 - val_acc: 0.7450\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8615 - acc: 0.7868 - val_loss: 0.9454 - val_acc: 0.7500\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8594 - acc: 0.7871 - val_loss: 0.9346 - val_acc: 0.7500\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8600 - acc: 0.7856 - val_loss: 0.9403 - val_acc: 0.7490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 532/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8599 - acc: 0.7871 - val_loss: 0.9376 - val_acc: 0.7490\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8575 - acc: 0.7873 - val_loss: 0.9400 - val_acc: 0.7510\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8587 - acc: 0.7884 - val_loss: 0.9334 - val_acc: 0.7490\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8594 - acc: 0.7863 - val_loss: 0.9334 - val_acc: 0.7460\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8594 - acc: 0.7863 - val_loss: 0.9436 - val_acc: 0.7450\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8577 - acc: 0.7885 - val_loss: 0.9609 - val_acc: 0.7500\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8599 - acc: 0.7888 - val_loss: 0.9356 - val_acc: 0.7460\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8579 - acc: 0.7879 - val_loss: 0.9440 - val_acc: 0.7470\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8570 - acc: 0.7879 - val_loss: 0.9497 - val_acc: 0.7520\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8582 - acc: 0.7861 - val_loss: 0.9368 - val_acc: 0.7500\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8572 - acc: 0.7881 - val_loss: 0.9323 - val_acc: 0.7470\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8562 - acc: 0.7860 - val_loss: 0.9551 - val_acc: 0.7380\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8584 - acc: 0.7855 - val_loss: 0.9315 - val_acc: 0.7470\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8567 - acc: 0.7849 - val_loss: 0.9315 - val_acc: 0.7470\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8553 - acc: 0.7893 - val_loss: 0.9453 - val_acc: 0.7480\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8557 - acc: 0.7889 - val_loss: 1.0297 - val_acc: 0.7170\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8606 - acc: 0.7848 - val_loss: 0.9334 - val_acc: 0.7450\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8538 - acc: 0.7896 - val_loss: 0.9588 - val_acc: 0.7520\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8566 - acc: 0.7912 - val_loss: 0.9322 - val_acc: 0.7510\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8541 - acc: 0.7891 - val_loss: 0.9301 - val_acc: 0.7520\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8558 - acc: 0.7900 - val_loss: 0.9294 - val_acc: 0.7450\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8533 - acc: 0.7909 - val_loss: 0.9318 - val_acc: 0.7520\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8543 - acc: 0.7892 - val_loss: 0.9366 - val_acc: 0.7430\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8523 - acc: 0.7880 - val_loss: 0.9396 - val_acc: 0.7510\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8524 - acc: 0.7899 - val_loss: 0.9388 - val_acc: 0.7430\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8530 - acc: 0.7891 - val_loss: 0.9290 - val_acc: 0.7500\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8532 - acc: 0.7879 - val_loss: 0.9333 - val_acc: 0.7520\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8514 - acc: 0.7911 - val_loss: 0.9409 - val_acc: 0.7500\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8527 - acc: 0.7893 - val_loss: 0.9420 - val_acc: 0.7460\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8519 - acc: 0.7904 - val_loss: 0.9336 - val_acc: 0.7520\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8517 - acc: 0.7900 - val_loss: 0.9431 - val_acc: 0.7440\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8514 - acc: 0.7908 - val_loss: 0.9264 - val_acc: 0.7500\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8517 - acc: 0.7911 - val_loss: 0.9421 - val_acc: 0.7410\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8506 - acc: 0.7889 - val_loss: 0.9295 - val_acc: 0.7560\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8501 - acc: 0.7901 - val_loss: 0.9275 - val_acc: 0.7520\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8504 - acc: 0.7900 - val_loss: 0.9330 - val_acc: 0.7530\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8496 - acc: 0.7903 - val_loss: 0.9271 - val_acc: 0.7510\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8483 - acc: 0.7901 - val_loss: 0.9349 - val_acc: 0.7490\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8513 - acc: 0.7911 - val_loss: 0.9293 - val_acc: 0.7470\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8502 - acc: 0.7892 - val_loss: 0.9391 - val_acc: 0.7590\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8506 - acc: 0.7920 - val_loss: 0.9277 - val_acc: 0.7530\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8484 - acc: 0.7883 - val_loss: 0.9547 - val_acc: 0.7510\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8484 - acc: 0.7901 - val_loss: 0.9315 - val_acc: 0.7510\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8481 - acc: 0.7904 - val_loss: 0.9283 - val_acc: 0.7540\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8489 - acc: 0.7897 - val_loss: 0.9415 - val_acc: 0.7520\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8482 - acc: 0.7893 - val_loss: 0.9272 - val_acc: 0.7500\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8473 - acc: 0.7877 - val_loss: 0.9346 - val_acc: 0.7460\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8479 - acc: 0.7905 - val_loss: 0.9289 - val_acc: 0.7540\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8474 - acc: 0.7913 - val_loss: 0.9332 - val_acc: 0.7490\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8467 - acc: 0.7904 - val_loss: 0.9345 - val_acc: 0.7450\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8467 - acc: 0.7915 - val_loss: 0.9300 - val_acc: 0.7550\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8454 - acc: 0.7911 - val_loss: 0.9369 - val_acc: 0.7580\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8473 - acc: 0.7909 - val_loss: 0.9264 - val_acc: 0.7550\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8454 - acc: 0.7893 - val_loss: 0.9533 - val_acc: 0.7430\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8473 - acc: 0.7903 - val_loss: 0.9332 - val_acc: 0.7530\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8457 - acc: 0.7879 - val_loss: 0.9296 - val_acc: 0.7550\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8446 - acc: 0.7899 - val_loss: 0.9251 - val_acc: 0.7500\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8444 - acc: 0.7928 - val_loss: 0.9400 - val_acc: 0.7460\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8464 - acc: 0.7908 - val_loss: 0.9737 - val_acc: 0.7470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 591/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8448 - acc: 0.7931 - val_loss: 0.9400 - val_acc: 0.7540\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8453 - acc: 0.7893 - val_loss: 0.9246 - val_acc: 0.7460\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8441 - acc: 0.7929 - val_loss: 0.9292 - val_acc: 0.7520\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8453 - acc: 0.7908 - val_loss: 0.9359 - val_acc: 0.7590\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8449 - acc: 0.7913 - val_loss: 0.9310 - val_acc: 0.7540\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8436 - acc: 0.7917 - val_loss: 0.9239 - val_acc: 0.7490\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8422 - acc: 0.7929 - val_loss: 0.9466 - val_acc: 0.7410\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8435 - acc: 0.7919 - val_loss: 0.9370 - val_acc: 0.7540\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8418 - acc: 0.7919 - val_loss: 0.9278 - val_acc: 0.7550\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8418 - acc: 0.7937 - val_loss: 0.9414 - val_acc: 0.7510\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8415 - acc: 0.7925 - val_loss: 0.9354 - val_acc: 0.7610\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8418 - acc: 0.7929 - val_loss: 0.9274 - val_acc: 0.7520\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8406 - acc: 0.7907 - val_loss: 0.9709 - val_acc: 0.7350\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8432 - acc: 0.7911 - val_loss: 0.9268 - val_acc: 0.7590\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8406 - acc: 0.7924 - val_loss: 0.9263 - val_acc: 0.7500\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8412 - acc: 0.7929 - val_loss: 0.9359 - val_acc: 0.7530\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8436 - acc: 0.7915 - val_loss: 0.9216 - val_acc: 0.7520\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8408 - acc: 0.7947 - val_loss: 0.9223 - val_acc: 0.7510\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8397 - acc: 0.7941 - val_loss: 0.9247 - val_acc: 0.7500\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8413 - acc: 0.7941 - val_loss: 0.9321 - val_acc: 0.7440\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8396 - acc: 0.7920 - val_loss: 0.9274 - val_acc: 0.7500\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8393 - acc: 0.7933 - val_loss: 0.9528 - val_acc: 0.7440\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8414 - acc: 0.7949 - val_loss: 0.9224 - val_acc: 0.7560\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8396 - acc: 0.7944 - val_loss: 0.9377 - val_acc: 0.7560\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8409 - acc: 0.7947 - val_loss: 0.9284 - val_acc: 0.7520\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8389 - acc: 0.7921 - val_loss: 0.9235 - val_acc: 0.7550\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8384 - acc: 0.7941 - val_loss: 0.9351 - val_acc: 0.7470\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8383 - acc: 0.7924 - val_loss: 0.9532 - val_acc: 0.7370\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8374 - acc: 0.7939 - val_loss: 0.9196 - val_acc: 0.7470\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8370 - acc: 0.7932 - val_loss: 0.9224 - val_acc: 0.7540\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8372 - acc: 0.7928 - val_loss: 0.9218 - val_acc: 0.7550\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8356 - acc: 0.7952 - val_loss: 0.9592 - val_acc: 0.7300\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8391 - acc: 0.7948 - val_loss: 0.9297 - val_acc: 0.7500\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8370 - acc: 0.7932 - val_loss: 0.9201 - val_acc: 0.7500\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8367 - acc: 0.7955 - val_loss: 0.9309 - val_acc: 0.7580\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8351 - acc: 0.7960 - val_loss: 0.9218 - val_acc: 0.7570\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8365 - acc: 0.7935 - val_loss: 0.9370 - val_acc: 0.7410\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8381 - acc: 0.7952 - val_loss: 0.9628 - val_acc: 0.7290\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8371 - acc: 0.7916 - val_loss: 0.9197 - val_acc: 0.7540\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8347 - acc: 0.7945 - val_loss: 0.9352 - val_acc: 0.7440\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8357 - acc: 0.7964 - val_loss: 0.9632 - val_acc: 0.7420\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8359 - acc: 0.7947 - val_loss: 0.9211 - val_acc: 0.7530\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8358 - acc: 0.7937 - val_loss: 0.9405 - val_acc: 0.7420\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8368 - acc: 0.7928 - val_loss: 0.9177 - val_acc: 0.7570\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8336 - acc: 0.7951 - val_loss: 0.9518 - val_acc: 0.7530\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8368 - acc: 0.7943 - val_loss: 0.9361 - val_acc: 0.7590\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8342 - acc: 0.7960 - val_loss: 0.9257 - val_acc: 0.7520\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8335 - acc: 0.7965 - val_loss: 0.9404 - val_acc: 0.7440\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8354 - acc: 0.7957 - val_loss: 0.9221 - val_acc: 0.7570\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8331 - acc: 0.7959 - val_loss: 0.9217 - val_acc: 0.7560\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8359 - acc: 0.7940 - val_loss: 0.9288 - val_acc: 0.7530\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8327 - acc: 0.7956 - val_loss: 0.9356 - val_acc: 0.7500\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8333 - acc: 0.7955 - val_loss: 0.9276 - val_acc: 0.7520\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8331 - acc: 0.7969 - val_loss: 0.9299 - val_acc: 0.7480\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8319 - acc: 0.7953 - val_loss: 0.9450 - val_acc: 0.7480\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8318 - acc: 0.7972 - val_loss: 0.9511 - val_acc: 0.7430\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8343 - acc: 0.7929 - val_loss: 0.9254 - val_acc: 0.7480\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8312 - acc: 0.7963 - val_loss: 0.9413 - val_acc: 0.7410\n",
      "Epoch 649/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8333 - acc: 0.7979 - val_loss: 0.9164 - val_acc: 0.7550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 650/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8318 - acc: 0.7953 - val_loss: 0.9275 - val_acc: 0.7560\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8324 - acc: 0.7949 - val_loss: 0.9339 - val_acc: 0.7560\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8310 - acc: 0.7973 - val_loss: 0.9295 - val_acc: 0.7540\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8314 - acc: 0.7971 - val_loss: 0.9199 - val_acc: 0.7580\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8299 - acc: 0.7988 - val_loss: 0.9335 - val_acc: 0.7540\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8307 - acc: 0.7969 - val_loss: 0.9528 - val_acc: 0.7490\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8314 - acc: 0.7965 - val_loss: 0.9177 - val_acc: 0.7510\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8284 - acc: 0.7956 - val_loss: 0.9210 - val_acc: 0.7570\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8303 - acc: 0.7949 - val_loss: 0.9253 - val_acc: 0.7550\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8293 - acc: 0.7952 - val_loss: 0.9615 - val_acc: 0.7320\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8318 - acc: 0.7968 - val_loss: 0.9151 - val_acc: 0.7600\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8284 - acc: 0.8000 - val_loss: 0.9173 - val_acc: 0.7530\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8299 - acc: 0.7941 - val_loss: 0.9244 - val_acc: 0.7530\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8303 - acc: 0.7945 - val_loss: 0.9178 - val_acc: 0.7480\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8298 - acc: 0.7976 - val_loss: 0.9174 - val_acc: 0.7550\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8273 - acc: 0.7955 - val_loss: 0.9159 - val_acc: 0.7610\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8286 - acc: 0.7957 - val_loss: 0.9212 - val_acc: 0.7600\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8278 - acc: 0.7972 - val_loss: 0.9458 - val_acc: 0.7450\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8292 - acc: 0.7964 - val_loss: 0.9292 - val_acc: 0.7510\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8299 - acc: 0.7968 - val_loss: 0.9242 - val_acc: 0.7510\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8297 - acc: 0.7992 - val_loss: 0.9151 - val_acc: 0.7590\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8270 - acc: 0.7969 - val_loss: 0.9238 - val_acc: 0.7530\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8277 - acc: 0.7984 - val_loss: 0.9563 - val_acc: 0.7430\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8266 - acc: 0.7972 - val_loss: 0.9259 - val_acc: 0.7490\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8263 - acc: 0.7991 - val_loss: 0.9218 - val_acc: 0.7530\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8258 - acc: 0.7985 - val_loss: 0.9288 - val_acc: 0.7490\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8272 - acc: 0.8007 - val_loss: 0.9172 - val_acc: 0.7540\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8266 - acc: 0.7981 - val_loss: 0.9337 - val_acc: 0.7480\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8287 - acc: 0.8015 - val_loss: 0.9195 - val_acc: 0.7570\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8259 - acc: 0.7991 - val_loss: 0.9259 - val_acc: 0.7530\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8264 - acc: 0.7995 - val_loss: 0.9221 - val_acc: 0.7580\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.8261 - acc: 0.7989 - val_loss: 0.9369 - val_acc: 0.7410\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8260 - acc: 0.7983 - val_loss: 0.9184 - val_acc: 0.7530\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8254 - acc: 0.7980 - val_loss: 0.9237 - val_acc: 0.7570\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8260 - acc: 0.7991 - val_loss: 0.9187 - val_acc: 0.7510\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.8254 - acc: 0.8007 - val_loss: 0.9175 - val_acc: 0.7560\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8250 - acc: 0.7999 - val_loss: 0.9372 - val_acc: 0.7440\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8261 - acc: 0.7980 - val_loss: 0.9243 - val_acc: 0.7470\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8248 - acc: 0.7992 - val_loss: 0.9363 - val_acc: 0.7510\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.8256 - acc: 0.7979 - val_loss: 0.9375 - val_acc: 0.7460\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.8251 - acc: 0.7983 - val_loss: 0.9255 - val_acc: 0.7480\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8256 - acc: 0.8019 - val_loss: 0.9227 - val_acc: 0.7490\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8228 - acc: 0.8007 - val_loss: 0.9213 - val_acc: 0.7520\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8246 - acc: 0.7984 - val_loss: 0.9215 - val_acc: 0.7520\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8233 - acc: 0.8003 - val_loss: 0.9119 - val_acc: 0.7580\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8247 - acc: 0.7996 - val_loss: 0.9312 - val_acc: 0.7560\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8252 - acc: 0.7987 - val_loss: 0.9199 - val_acc: 0.7470\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8266 - acc: 0.7985 - val_loss: 0.9400 - val_acc: 0.7500\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8237 - acc: 0.7995 - val_loss: 0.9153 - val_acc: 0.7600\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8221 - acc: 0.8017 - val_loss: 0.9379 - val_acc: 0.7500\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8241 - acc: 0.7985 - val_loss: 0.9253 - val_acc: 0.7550\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8227 - acc: 0.8008 - val_loss: 0.9249 - val_acc: 0.7530\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8217 - acc: 0.7999 - val_loss: 0.9140 - val_acc: 0.7570\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8216 - acc: 0.7987 - val_loss: 0.9319 - val_acc: 0.7490\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8218 - acc: 0.8001 - val_loss: 0.9190 - val_acc: 0.7570\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8211 - acc: 0.8027 - val_loss: 0.9310 - val_acc: 0.7550\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.8234 - acc: 0.7979 - val_loss: 0.9371 - val_acc: 0.7600\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8224 - acc: 0.8008 - val_loss: 0.9120 - val_acc: 0.7590\n",
      "Epoch 708/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.8205 - acc: 0.8007 - val_loss: 0.9184 - val_acc: 0.7610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 709/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8203 - acc: 0.7995 - val_loss: 0.9178 - val_acc: 0.7610\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8201 - acc: 0.8007 - val_loss: 0.9167 - val_acc: 0.7550\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8215 - acc: 0.8028 - val_loss: 0.9411 - val_acc: 0.7510\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8201 - acc: 0.8015 - val_loss: 0.9125 - val_acc: 0.7580\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8203 - acc: 0.8029 - val_loss: 0.9207 - val_acc: 0.7530\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8202 - acc: 0.8032 - val_loss: 0.9158 - val_acc: 0.7580\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8246 - acc: 0.7997 - val_loss: 0.9253 - val_acc: 0.7540\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8215 - acc: 0.7992 - val_loss: 0.9193 - val_acc: 0.7530\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.8213 - acc: 0.8008 - val_loss: 0.9167 - val_acc: 0.7540\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8219 - acc: 0.7992 - val_loss: 0.9321 - val_acc: 0.7560\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8208 - acc: 0.8013 - val_loss: 0.9326 - val_acc: 0.7530\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8189 - acc: 0.8029 - val_loss: 0.9187 - val_acc: 0.7500\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8191 - acc: 0.8009 - val_loss: 0.9102 - val_acc: 0.7620\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8195 - acc: 0.8012 - val_loss: 0.9218 - val_acc: 0.7480\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8167 - acc: 0.8013 - val_loss: 0.9109 - val_acc: 0.7580\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8207 - acc: 0.8019 - val_loss: 0.9125 - val_acc: 0.7620\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.8185 - acc: 0.8008 - val_loss: 0.9577 - val_acc: 0.7350\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.8211 - acc: 0.8025 - val_loss: 0.9322 - val_acc: 0.7470\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.8171 - acc: 0.8016 - val_loss: 0.9531 - val_acc: 0.7460\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8198 - acc: 0.8007 - val_loss: 0.9182 - val_acc: 0.7610\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8176 - acc: 0.8021 - val_loss: 0.9163 - val_acc: 0.7500\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8159 - acc: 0.8047 - val_loss: 0.9235 - val_acc: 0.7470\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.8164 - acc: 0.8027 - val_loss: 0.9333 - val_acc: 0.7480\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8199 - acc: 0.8037 - val_loss: 0.9124 - val_acc: 0.7530\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8182 - acc: 0.8029 - val_loss: 0.9148 - val_acc: 0.7560\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8178 - acc: 0.8024 - val_loss: 0.9418 - val_acc: 0.7360\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8191 - acc: 0.8024 - val_loss: 0.9186 - val_acc: 0.7550\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8188 - acc: 0.8033 - val_loss: 0.9225 - val_acc: 0.7540\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8187 - acc: 0.8019 - val_loss: 0.9474 - val_acc: 0.7440\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8166 - acc: 0.8024 - val_loss: 0.9141 - val_acc: 0.7580\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8167 - acc: 0.8041 - val_loss: 0.9173 - val_acc: 0.7550\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8153 - acc: 0.8047 - val_loss: 0.9137 - val_acc: 0.7560\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8157 - acc: 0.8027 - val_loss: 0.9190 - val_acc: 0.7520\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.8156 - acc: 0.8056 - val_loss: 0.9132 - val_acc: 0.7600\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.8167 - acc: 0.8045 - val_loss: 0.9161 - val_acc: 0.7530\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8156 - acc: 0.8032 - val_loss: 0.9132 - val_acc: 0.7520\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8166 - acc: 0.8020 - val_loss: 0.9132 - val_acc: 0.7600\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8165 - acc: 0.8023 - val_loss: 0.9107 - val_acc: 0.7550\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8161 - acc: 0.8044 - val_loss: 0.9222 - val_acc: 0.7560\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8151 - acc: 0.8035 - val_loss: 0.9390 - val_acc: 0.7470\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8142 - acc: 0.8060 - val_loss: 0.9226 - val_acc: 0.7460\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8164 - acc: 0.8028 - val_loss: 0.9162 - val_acc: 0.7590\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8157 - acc: 0.8027 - val_loss: 0.9121 - val_acc: 0.7560\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8150 - acc: 0.8040 - val_loss: 0.9218 - val_acc: 0.7590\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8132 - acc: 0.8064 - val_loss: 0.9162 - val_acc: 0.7570\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.8193 - acc: 0.8009 - val_loss: 0.9169 - val_acc: 0.7540\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.8150 - acc: 0.8035 - val_loss: 0.9258 - val_acc: 0.7420\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8162 - acc: 0.8015 - val_loss: 0.9115 - val_acc: 0.7640\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.8150 - acc: 0.8020 - val_loss: 0.9121 - val_acc: 0.7630\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.8127 - acc: 0.8048 - val_loss: 0.9161 - val_acc: 0.7580\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 0.8120 - acc: 0.8045 - val_loss: 0.9159 - val_acc: 0.7550\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 0.8164 - acc: 0.8032 - val_loss: 0.9156 - val_acc: 0.7590\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.8146 - acc: 0.8043 - val_loss: 0.9135 - val_acc: 0.7610\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8126 - acc: 0.8032 - val_loss: 0.9179 - val_acc: 0.7570\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8135 - acc: 0.8064 - val_loss: 0.9099 - val_acc: 0.7560\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8122 - acc: 0.8063 - val_loss: 0.9321 - val_acc: 0.7500\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8129 - acc: 0.8031 - val_loss: 0.9152 - val_acc: 0.7570\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8124 - acc: 0.8057 - val_loss: 0.9143 - val_acc: 0.7510\n",
      "Epoch 767/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.8143 - acc: 0.8059 - val_loss: 0.9304 - val_acc: 0.7420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 768/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.8133 - acc: 0.8045 - val_loss: 0.9171 - val_acc: 0.7570\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8114 - acc: 0.8057 - val_loss: 0.9346 - val_acc: 0.7520\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.8132 - acc: 0.8023 - val_loss: 0.9214 - val_acc: 0.7560\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8115 - acc: 0.8036 - val_loss: 0.9209 - val_acc: 0.7550\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8125 - acc: 0.8027 - val_loss: 0.9427 - val_acc: 0.7330\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8125 - acc: 0.8029 - val_loss: 0.9266 - val_acc: 0.7480\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8163 - acc: 0.8055 - val_loss: 0.9359 - val_acc: 0.7430\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8140 - acc: 0.8068 - val_loss: 0.9367 - val_acc: 0.7450\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8155 - acc: 0.8015 - val_loss: 0.9095 - val_acc: 0.7590\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8113 - acc: 0.8059 - val_loss: 0.9129 - val_acc: 0.7560\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8146 - acc: 0.8039 - val_loss: 0.9076 - val_acc: 0.7620\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8120 - acc: 0.8060 - val_loss: 0.9092 - val_acc: 0.7610\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.8101 - acc: 0.8053 - val_loss: 0.9259 - val_acc: 0.7540\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8124 - acc: 0.8067 - val_loss: 0.9233 - val_acc: 0.7430\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8123 - acc: 0.8057 - val_loss: 0.9098 - val_acc: 0.7640\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8131 - acc: 0.8053 - val_loss: 0.9124 - val_acc: 0.7540\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8136 - acc: 0.8052 - val_loss: 0.9381 - val_acc: 0.7330\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8109 - acc: 0.8073 - val_loss: 0.9135 - val_acc: 0.7580\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8110 - acc: 0.8063 - val_loss: 0.9158 - val_acc: 0.7600\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8092 - acc: 0.8061 - val_loss: 0.9167 - val_acc: 0.7520\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8103 - acc: 0.8067 - val_loss: 0.9153 - val_acc: 0.7560\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8087 - acc: 0.8077 - val_loss: 0.9235 - val_acc: 0.7570\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8101 - acc: 0.8071 - val_loss: 0.9146 - val_acc: 0.7560\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8078 - acc: 0.8067 - val_loss: 0.9082 - val_acc: 0.7580\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8103 - acc: 0.8056 - val_loss: 0.9291 - val_acc: 0.7510\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8119 - acc: 0.8052 - val_loss: 0.9376 - val_acc: 0.7510\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8106 - acc: 0.8049 - val_loss: 0.9144 - val_acc: 0.7540\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8111 - acc: 0.8063 - val_loss: 0.9253 - val_acc: 0.7470\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.8110 - acc: 0.8065 - val_loss: 0.9214 - val_acc: 0.7590\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8090 - acc: 0.8085 - val_loss: 0.9147 - val_acc: 0.7560\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8087 - acc: 0.8076 - val_loss: 0.9161 - val_acc: 0.7550\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8106 - acc: 0.8081 - val_loss: 0.9520 - val_acc: 0.7390\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8111 - acc: 0.8063 - val_loss: 0.9315 - val_acc: 0.7410\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.8106 - acc: 0.8053 - val_loss: 0.9116 - val_acc: 0.7620\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8062 - acc: 0.8073 - val_loss: 0.9503 - val_acc: 0.7410\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8083 - acc: 0.8056 - val_loss: 0.9106 - val_acc: 0.7580\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.8096 - acc: 0.8073 - val_loss: 0.9227 - val_acc: 0.7450\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.8088 - acc: 0.8060 - val_loss: 0.9254 - val_acc: 0.7510\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8074 - acc: 0.8067 - val_loss: 0.9098 - val_acc: 0.7610\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8107 - acc: 0.8059 - val_loss: 0.9151 - val_acc: 0.7550\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8076 - acc: 0.8052 - val_loss: 0.9282 - val_acc: 0.7500\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.8102 - acc: 0.8061 - val_loss: 0.9244 - val_acc: 0.7490\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8123 - acc: 0.8083 - val_loss: 0.9414 - val_acc: 0.7370\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.8098 - acc: 0.8079 - val_loss: 0.9097 - val_acc: 0.7620\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.8092 - acc: 0.8048 - val_loss: 0.9333 - val_acc: 0.7560\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8081 - acc: 0.8085 - val_loss: 0.9051 - val_acc: 0.7620\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8064 - acc: 0.8044 - val_loss: 0.9386 - val_acc: 0.7520\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8091 - acc: 0.8069 - val_loss: 0.9453 - val_acc: 0.7380\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8110 - acc: 0.8068 - val_loss: 0.9092 - val_acc: 0.7580\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8085 - acc: 0.8073 - val_loss: 0.9434 - val_acc: 0.7350\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8063 - acc: 0.8075 - val_loss: 0.9447 - val_acc: 0.7550\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8088 - acc: 0.8053 - val_loss: 0.9639 - val_acc: 0.7530\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8068 - acc: 0.8089 - val_loss: 0.9285 - val_acc: 0.7580\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8054 - acc: 0.8099 - val_loss: 0.9295 - val_acc: 0.7560\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8061 - acc: 0.8081 - val_loss: 0.9072 - val_acc: 0.7600\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8086 - acc: 0.8079 - val_loss: 0.9199 - val_acc: 0.7560\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.8066 - acc: 0.8076 - val_loss: 0.9324 - val_acc: 0.7520\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.8057 - acc: 0.8084 - val_loss: 0.9207 - val_acc: 0.7520\n",
      "Epoch 826/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8058 - acc: 0.8093 - val_loss: 0.9208 - val_acc: 0.7550\n",
      "Epoch 827/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.8053 - acc: 0.8083 - val_loss: 0.9186 - val_acc: 0.7520\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8103 - acc: 0.8057 - val_loss: 0.9089 - val_acc: 0.7610A: 0s - loss: 0.8111 - acc: 0.805\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.8057 - acc: 0.8112 - val_loss: 0.9287 - val_acc: 0.7470\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8055 - acc: 0.8105 - val_loss: 0.9259 - val_acc: 0.7500\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.8052 - acc: 0.8073 - val_loss: 0.9410 - val_acc: 0.7430\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8044 - acc: 0.8076 - val_loss: 0.9190 - val_acc: 0.7610\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.8035 - acc: 0.8081 - val_loss: 0.9057 - val_acc: 0.7630\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8033 - acc: 0.8104 - val_loss: 0.9172 - val_acc: 0.7550\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8053 - acc: 0.8093 - val_loss: 0.9109 - val_acc: 0.7550\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8038 - acc: 0.8143 - val_loss: 0.9122 - val_acc: 0.7540\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8040 - acc: 0.8107 - val_loss: 0.9335 - val_acc: 0.7560\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8064 - acc: 0.8087 - val_loss: 0.9330 - val_acc: 0.7440\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8055 - acc: 0.8125 - val_loss: 0.9129 - val_acc: 0.7600\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8038 - acc: 0.8092 - val_loss: 0.9144 - val_acc: 0.7530\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8042 - acc: 0.8104 - val_loss: 0.9302 - val_acc: 0.7520\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8038 - acc: 0.8113 - val_loss: 0.9282 - val_acc: 0.7490\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8030 - acc: 0.8099 - val_loss: 0.9290 - val_acc: 0.7440\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8052 - acc: 0.8085 - val_loss: 0.9060 - val_acc: 0.7600\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8051 - acc: 0.8097 - val_loss: 0.9076 - val_acc: 0.7560\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8040 - acc: 0.8123 - val_loss: 0.9137 - val_acc: 0.7530\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8013 - acc: 0.8109 - val_loss: 0.9061 - val_acc: 0.7610\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8013 - acc: 0.8124 - val_loss: 0.9492 - val_acc: 0.7480\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8090 - acc: 0.8076 - val_loss: 0.9373 - val_acc: 0.7460\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8065 - acc: 0.8059 - val_loss: 0.9450 - val_acc: 0.7540\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8036 - acc: 0.8104 - val_loss: 0.9401 - val_acc: 0.7520\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8045 - acc: 0.8109 - val_loss: 0.9080 - val_acc: 0.7610\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8004 - acc: 0.8093 - val_loss: 0.9095 - val_acc: 0.7600\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8042 - acc: 0.8075 - val_loss: 0.9214 - val_acc: 0.7510\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8005 - acc: 0.8129 - val_loss: 0.9099 - val_acc: 0.7580\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8031 - acc: 0.8073 - val_loss: 0.9088 - val_acc: 0.7600\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.8038 - acc: 0.8099 - val_loss: 0.9344 - val_acc: 0.7500\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.8041 - acc: 0.8125 - val_loss: 0.9098 - val_acc: 0.7580\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8023 - acc: 0.8081 - val_loss: 0.9082 - val_acc: 0.7590\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.7998 - acc: 0.8112 - val_loss: 0.9196 - val_acc: 0.7510\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8030 - acc: 0.8084 - val_loss: 0.9172 - val_acc: 0.7590\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8001 - acc: 0.8083 - val_loss: 0.9221 - val_acc: 0.7550\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8014 - acc: 0.8111 - val_loss: 0.9124 - val_acc: 0.7570\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8032 - acc: 0.8080 - val_loss: 0.9084 - val_acc: 0.7590\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8009 - acc: 0.8103 - val_loss: 0.9963 - val_acc: 0.7310\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8053 - acc: 0.8073 - val_loss: 0.9180 - val_acc: 0.7490\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8022 - acc: 0.8097 - val_loss: 0.9585 - val_acc: 0.7420\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8013 - acc: 0.8097 - val_loss: 0.9371 - val_acc: 0.7380\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8010 - acc: 0.8105 - val_loss: 0.9164 - val_acc: 0.7540\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8037 - acc: 0.8073 - val_loss: 0.9181 - val_acc: 0.7580\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8006 - acc: 0.8116 - val_loss: 0.9232 - val_acc: 0.7550\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7991 - acc: 0.8123 - val_loss: 0.9188 - val_acc: 0.7560\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8010 - acc: 0.8097 - val_loss: 0.9179 - val_acc: 0.7540\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8004 - acc: 0.8104 - val_loss: 0.9432 - val_acc: 0.7530\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.7989 - acc: 0.8099 - val_loss: 0.9074 - val_acc: 0.7570\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8038 - acc: 0.8087 - val_loss: 0.9418 - val_acc: 0.7580\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8013 - acc: 0.8085 - val_loss: 0.9091 - val_acc: 0.7620\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.7999 - acc: 0.8123 - val_loss: 0.9044 - val_acc: 0.7640\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8005 - acc: 0.8076 - val_loss: 0.9132 - val_acc: 0.7570\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8011 - acc: 0.8121 - val_loss: 0.9411 - val_acc: 0.7490\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8048 - acc: 0.8085 - val_loss: 0.9076 - val_acc: 0.7610\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7979 - acc: 0.8099 - val_loss: 0.9036 - val_acc: 0.7590\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.7994 - acc: 0.8140 - val_loss: 0.9112 - val_acc: 0.7580\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.7994 - acc: 0.8103 - val_loss: 0.9289 - val_acc: 0.7530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 885/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8013 - acc: 0.8113 - val_loss: 0.9305 - val_acc: 0.7480\n",
      "Epoch 886/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8030 - acc: 0.8093 - val_loss: 0.9095 - val_acc: 0.7610\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.7998 - acc: 0.8121 - val_loss: 0.9268 - val_acc: 0.7570\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7971 - acc: 0.8123 - val_loss: 0.9294 - val_acc: 0.7520\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.7990 - acc: 0.8112 - val_loss: 0.9051 - val_acc: 0.7610\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8002 - acc: 0.8089 - val_loss: 0.9133 - val_acc: 0.7570\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7996 - acc: 0.8113 - val_loss: 0.9128 - val_acc: 0.7540\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7992 - acc: 0.8076 - val_loss: 0.9147 - val_acc: 0.7550\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7995 - acc: 0.8116 - val_loss: 0.9500 - val_acc: 0.7500\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8008 - acc: 0.8089 - val_loss: 0.9521 - val_acc: 0.7490\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8037 - acc: 0.8073 - val_loss: 0.9219 - val_acc: 0.7500\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7964 - acc: 0.8099 - val_loss: 0.9069 - val_acc: 0.7570\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7950 - acc: 0.8141 - val_loss: 0.9114 - val_acc: 0.7590\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7955 - acc: 0.8155 - val_loss: 0.9192 - val_acc: 0.7570\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7966 - acc: 0.8115 - val_loss: 0.9261 - val_acc: 0.7430\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7963 - acc: 0.8128 - val_loss: 0.9788 - val_acc: 0.7370\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7992 - acc: 0.8107 - val_loss: 0.9190 - val_acc: 0.7540\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8017 - acc: 0.8107 - val_loss: 0.9085 - val_acc: 0.7580\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.7941 - acc: 0.8141 - val_loss: 0.9063 - val_acc: 0.7600\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.7973 - acc: 0.8147 - val_loss: 0.9165 - val_acc: 0.7530\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7955 - acc: 0.8155 - val_loss: 0.9076 - val_acc: 0.7610\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7978 - acc: 0.8115 - val_loss: 0.9093 - val_acc: 0.7520\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.7990 - acc: 0.8123 - val_loss: 0.9185 - val_acc: 0.7480\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.7980 - acc: 0.8112 - val_loss: 0.9166 - val_acc: 0.7560\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7994 - acc: 0.8115 - val_loss: 0.9120 - val_acc: 0.7590\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7969 - acc: 0.8113 - val_loss: 0.9385 - val_acc: 0.7510\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7997 - acc: 0.8121 - val_loss: 0.9252 - val_acc: 0.7590\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.7989 - acc: 0.8136 - val_loss: 0.9153 - val_acc: 0.7550\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7968 - acc: 0.8105 - val_loss: 0.9175 - val_acc: 0.7550\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.7994 - acc: 0.8111 - val_loss: 0.9195 - val_acc: 0.7540\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7969 - acc: 0.8131 - val_loss: 0.9091 - val_acc: 0.7620\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7982 - acc: 0.8141 - val_loss: 0.9182 - val_acc: 0.7500\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.7936 - acc: 0.8144 - val_loss: 0.9045 - val_acc: 0.7580\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.7985 - acc: 0.8121 - val_loss: 0.9341 - val_acc: 0.7470\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8007 - acc: 0.8119 - val_loss: 0.9773 - val_acc: 0.7240\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7981 - acc: 0.8120 - val_loss: 0.9075 - val_acc: 0.7570\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.7956 - acc: 0.8139 - val_loss: 0.9342 - val_acc: 0.7540\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7978 - acc: 0.8108 - val_loss: 0.9202 - val_acc: 0.7500\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7945 - acc: 0.8151 - val_loss: 0.9516 - val_acc: 0.7260\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7977 - acc: 0.8135 - val_loss: 0.9105 - val_acc: 0.7560\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7948 - acc: 0.8129 - val_loss: 0.9556 - val_acc: 0.7410\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7931 - acc: 0.8139 - val_loss: 0.9079 - val_acc: 0.7580\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8052 - acc: 0.8100 - val_loss: 0.9079 - val_acc: 0.7590\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7930 - acc: 0.8172 - val_loss: 0.9046 - val_acc: 0.7650\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.7927 - acc: 0.8141 - val_loss: 0.9184 - val_acc: 0.7530\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7960 - acc: 0.8124 - val_loss: 0.9135 - val_acc: 0.7550\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7924 - acc: 0.8153 - val_loss: 0.9201 - val_acc: 0.7580\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7956 - acc: 0.8147 - val_loss: 0.9078 - val_acc: 0.7560\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.7977 - acc: 0.8133 - val_loss: 0.9305 - val_acc: 0.7510\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7925 - acc: 0.8135 - val_loss: 0.9304 - val_acc: 0.7460\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7947 - acc: 0.8120 - val_loss: 0.9525 - val_acc: 0.7370\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7965 - acc: 0.8107 - val_loss: 0.9043 - val_acc: 0.7650\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7941 - acc: 0.8135 - val_loss: 0.9133 - val_acc: 0.7590\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7950 - acc: 0.8144 - val_loss: 0.9194 - val_acc: 0.7450\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7902 - acc: 0.8155 - val_loss: 0.9091 - val_acc: 0.7570\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7930 - acc: 0.8163 - val_loss: 0.9018 - val_acc: 0.7610\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7908 - acc: 0.8156 - val_loss: 0.9220 - val_acc: 0.7580\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7919 - acc: 0.8155 - val_loss: 0.9368 - val_acc: 0.7470\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7944 - acc: 0.8145 - val_loss: 0.9109 - val_acc: 0.7600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 944/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7940 - acc: 0.8136 - val_loss: 0.9080 - val_acc: 0.7560\n",
      "Epoch 945/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7925 - acc: 0.8147 - val_loss: 0.9057 - val_acc: 0.7580\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.7971 - acc: 0.8148 - val_loss: 0.9345 - val_acc: 0.7540\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.7921 - acc: 0.8140 - val_loss: 0.9401 - val_acc: 0.7510\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7932 - acc: 0.8189 - val_loss: 0.9154 - val_acc: 0.7570\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7902 - acc: 0.8173 - val_loss: 0.9021 - val_acc: 0.7660\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7934 - acc: 0.8157 - val_loss: 0.9099 - val_acc: 0.7540\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7919 - acc: 0.8153 - val_loss: 0.9164 - val_acc: 0.7550\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7958 - acc: 0.8124 - val_loss: 0.9246 - val_acc: 0.7470\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7942 - acc: 0.8165 - val_loss: 0.9240 - val_acc: 0.7470\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7943 - acc: 0.8124 - val_loss: 0.9111 - val_acc: 0.7540\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7907 - acc: 0.8159 - val_loss: 0.9515 - val_acc: 0.7360\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7934 - acc: 0.8157 - val_loss: 0.9139 - val_acc: 0.7570\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.7958 - acc: 0.8125 - val_loss: 0.9064 - val_acc: 0.7640\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7900 - acc: 0.8159 - val_loss: 0.9149 - val_acc: 0.7500\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7931 - acc: 0.8153 - val_loss: 0.9218 - val_acc: 0.7560\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8034 - acc: 0.8091 - val_loss: 0.9384 - val_acc: 0.7520\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7912 - acc: 0.8157 - val_loss: 0.9145 - val_acc: 0.7500\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7927 - acc: 0.8164 - val_loss: 0.9517 - val_acc: 0.7390\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8021 - acc: 0.8105 - val_loss: 0.9053 - val_acc: 0.7620\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.7896 - acc: 0.8168 - val_loss: 0.9228 - val_acc: 0.7480\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.7953 - acc: 0.8131 - val_loss: 0.9318 - val_acc: 0.7470\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.7893 - acc: 0.818 - 1s 71us/step - loss: 0.7891 - acc: 0.8179 - val_loss: 0.9142 - val_acc: 0.7580\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.7931 - acc: 0.8137 - val_loss: 0.9221 - val_acc: 0.7500\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7923 - acc: 0.8137 - val_loss: 0.9085 - val_acc: 0.7620\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.7908 - acc: 0.8147 - val_loss: 0.9133 - val_acc: 0.7500\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.7911 - acc: 0.8144 - val_loss: 0.9188 - val_acc: 0.7620\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.7963 - acc: 0.8128 - val_loss: 0.9199 - val_acc: 0.7510\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.7898 - acc: 0.8168 - val_loss: 0.9188 - val_acc: 0.7510\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.7922 - acc: 0.8132 - val_loss: 0.9382 - val_acc: 0.7380\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7942 - acc: 0.8168 - val_loss: 0.9079 - val_acc: 0.7610\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7874 - acc: 0.8155 - val_loss: 0.9056 - val_acc: 0.7620\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.7982 - acc: 0.8073 - val_loss: 0.9097 - val_acc: 0.7570\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7918 - acc: 0.8152 - val_loss: 0.9287 - val_acc: 0.7580\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.7938 - acc: 0.8160 - val_loss: 0.9035 - val_acc: 0.7640\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.7898 - acc: 0.8159 - val_loss: 0.9328 - val_acc: 0.7470\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.7933 - acc: 0.8153 - val_loss: 0.9295 - val_acc: 0.7400\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.7963 - acc: 0.8119 - val_loss: 0.9042 - val_acc: 0.7580\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.7952 - acc: 0.8107 - val_loss: 0.9159 - val_acc: 0.7480\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.7892 - acc: 0.8139 - val_loss: 0.9286 - val_acc: 0.7480\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.7920 - acc: 0.8145 - val_loss: 0.9095 - val_acc: 0.7540\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.7946 - acc: 0.8143 - val_loss: 0.9357 - val_acc: 0.7570\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.7911 - acc: 0.8176 - val_loss: 0.9827 - val_acc: 0.7310\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.7907 - acc: 0.8145 - val_loss: 0.9436 - val_acc: 0.7370\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7920 - acc: 0.8169 - val_loss: 0.9423 - val_acc: 0.7510\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.7995 - acc: 0.8100 - val_loss: 0.9350 - val_acc: 0.7410\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.7916 - acc: 0.8131 - val_loss: 0.9295 - val_acc: 0.7570\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.7917 - acc: 0.8161 - val_loss: 0.9185 - val_acc: 0.7490\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.7871 - acc: 0.8216 - val_loss: 0.9258 - val_acc: 0.7510\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.7877 - acc: 0.8153 - val_loss: 0.9016 - val_acc: 0.7650\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.7894 - acc: 0.8159 - val_loss: 0.9246 - val_acc: 0.7530\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.7893 - acc: 0.8141 - val_loss: 0.9282 - val_acc: 0.7520\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7888 - acc: 0.8176 - val_loss: 0.9094 - val_acc: 0.7570\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7886 - acc: 0.8169 - val_loss: 0.9138 - val_acc: 0.7600\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.7912 - acc: 0.8175 - val_loss: 0.9054 - val_acc: 0.7640\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7931 - acc: 0.8152 - val_loss: 0.9198 - val_acc: 0.7500\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7854 - acc: 0.8175 - val_loss: 0.9057 - val_acc: 0.7650\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xd8FHX++PHXezcJIKEXEYKAgkqRLhpFjMIJKCpnQ86OiHr2cpbvD8vZy6noWQ5sdyiKnl0O9RSJggQkSg8iHDU0Q+iQnvfvj5ndTJZNJUvKvp+Pxz6yM/uZmc/sbOY9nzKfEVXFGGOMAfBVdwaMMcbUHBYUjDHGBFlQMMYYE2RBwRhjTJAFBWOMMUEWFIwxxgRZUKghRMQvIntF5MiqTFvTicg7IvKQ+z5JRJaVJ20ltlNnvjNz6B3Mb6+2saBQSe4JJvAqFJEsz/SlFV2fqhaoaryqrq/KtJUhIieIyC8iskdEfhWRIZHYTihVTVbV7lWxLhGZLSJXedYd0e8sGoR+p575XUXkcxHJEJHtIvKliHSphiyaKmBBoZLcE0y8qsYD64FzPPOmhKYXkZhDn8tKewX4HGgMnAVsrN7smJKIiE9Eqvv/uAnwKXAscDiwEPjkUGagpv5/1ZDjUyG1KrO1iYg8KiLvi8h7IrIHuExEEkVkrojsFJHNIvKiiMS66WNEREWkozv9jvv5l+4Ve4qIdKpoWvfz4SLym4jsEpG/i8iP4a74PPKBdepYrarLy9jXlSIyzDMd514x9nT/KT4UkS3ufieLSNcS1jNERNZ6pvuJyEJ3n94D6nk+ayEi092r0x0i8oWItHM/ewpIBP7hltwmhPnOmrrfW4aIrBWR+0RE3M/Gisj3IvK8m+fVInJmKfs/3k2zR0SWici5IZ9f55a49ojIUhHp5c7vICKfunnYJiIvuPMfFZF/epbvLCLqmZ4tIo+ISAqwDzjSzfNydxv/E5GxIXk43/0ud4vIKhE5U0RGi8i8kHT3iMiHJe1rOKo6V1XfVNXtqpoHPA90F5EmYb6rgSKy0XuiFJGLROQX9/1J4pRSd4vIVhF5Jtw2A78VEfk/EdkCvObOP1dEFrnHbbaI9PAs09/ze5oqIv+WoqrLsSKS7Elb7PcSsu0Sf3vu5wccn4p8n9XNgkJk/RF4F+dK6n2ck+2tQEvgFGAYcF0py/8JuB9ojlMaeaSiaUWkNfAB8Bd3u2uAAWXk+yfg2cDJqxzeA0Z7pocDm1R1sTs9DegCtAGWAm+XtUIRqQd8BryJs0+fASM9SXw4J4IjgQ5AHvACgKreA6QA17slt9vCbOIV4DDgKOAM4BrgCs/nJwNLgBY4J7k3SsnubzjHswnwGPCuiBzu7sdoYDxwKU7J63xguzhXtv8BVgEdgfY4x6m8LgfGuOtMB7YCZ7vT1wJ/F5Gebh5Oxvke7wSaAqcD63Cv7qV4Vc9llOP4lGEQkK6qu8J89iPOsTrNM+9POP8nAH8HnlHVxkBnoLQAlQDE4/wG/iwiJ+D8JsbiHLc3gc/ci5R6OPv7Os7v6SOK/54qosTfnkfo8ak9VNVeB/kC1gJDQuY9CnxXxnJ3Af9238cACnR0p98B/uFJey6wtBJpxwCzPJ8JsBm4qoQ8XQak4lQbpQM93fnDgXklLHMcsAuo706/D/xfCWlbunlv6Mn7Q+77IcBa9/0ZwAZAPMv+FEgbZr39gQzP9GzvPnq/MyAWJ0Af4/n8RuBb9/1Y4FfPZ43dZVuW8/ewFDjbfT8DuDFMmlOBLYA/zGePAv/0THd2/lWL7dsDZeRhWmC7OAHtmRLSvQb81X3fG9gGxJaQtth3WkKaI4FNwEWlpHkSmOS+bwrsBxLc6TnAA0CLMrYzBMgG4kL25cGQdP/DCdhnAOtDPpvr+e2NBZLD/V5Cf6fl/O2Venxq8stKCpG1wTshIseJyH/cqpTdwMM4J8mSbPG8349zVVTRtG29+VDnV1valcutwIuqOh3nRPlf94rzZODbcAuo6q84/3xni0g8MAL3yk+cXj9Pu9Uru3GujKH0/Q7kO93Nb8C6wBsRaSgir4vIene935VjnQGtAb93fe77dp7p0O8TSvj+ReQqT5XFTpwgGchLe5zvJlR7nABYUM48hwr9bY0QkXniVNvtBM4sRx4A/oVTigHnguB9daqAKswtlf4XeEFV/11K0neBC8SpOr0A52Ij8Ju8GugGrBCRn0TkrFLWs1VVcz3THYB7AsfB/R6OwDmubTnwd7+BSijnb69S664JLChEVugQtBNxriI7q1M8fgDnyj2SNuMUswEQEaH4yS9UDM5VNKr6GXAPTjC4DJhQynKBKqQ/AgtVda07/wqcUscZONUrnQNZqUi+Xd662buBTsAA97s8IyRtacP//g4U4JxEvOuucIO6iBwFvArcgHN12xT4laL92wAcHWbRDUAHEfGH+WwfTtVWQJswabxtDA1wqlmeAA538/DfcuQBVZ3truMUnONXqaojEWmB8zv5UFWfKi2tOtWKm4GhFK86QlVXqOolOIH7WeAjEalf0qpCpjfglHqael6HqeoHhP89tfe8L893HlDWby9c3moNCwqHViOcapZ94jS2ltaeUFWmAX1F5By3HvtWoFUp6f8NPCQix7uNgb8CuUADoKR/TnCCwnBgHJ5/cpx9zgEycf7pHitnvmcDPhG5yW30uwjoG7Le/cAO94T0QMjyW3HaCw7gXgl/CDwuIvHiNMrfjlNFUFHxOCeADJyYOxanpBDwOnC3iPQRRxcRaY/T5pHp5uEwEWngnpjB6b1zmoi0F5GmwL1l5KEeEOfmoUBERgCDPZ+/AYwVkdPFafhPEJFjPZ+/jRPY9qnq3DK2FSsi9T2vWLdB+b841aXjy1g+4D2c7zwRT7uBiFwuIi1VtRDnf0WBwnKucxJwozhdqsU9tueISEOc35NfRG5wf08XAP08yy4Cerq/+wbAg6Vsp6zfXq1mQeHQuhO4EtiDU2p4P9IbVNWtwCjgOZyT0NHAApwTdThPAZNxuqRuxykdjMX5J/6PiDQuYTvpOG0RJ1G8wfQtnDrmTcAynDrj8uQ7B6fUcS2wA6eB9lNPkudwSh6Z7jq/DFnFBGC0W43wXJhN/Bkn2K0BvsepRplcnryF5HMx8CJOe8dmnIAwz/P5ezjf6fvAbuBjoJmq5uNUs3XFucJdD1zoLvYVTpfOJe56Py8jDztxTrCf4ByzC3EuBgKfz8H5Hl/EOdHOpPhV8mSgB+UrJUwCsjyv19zt9cUJPN77d9qWsp53ca6wv1HVHZ75ZwHLxemx9zdgVEgVUYlUdR5Oie1VnN/MbzglXO/v6Xr3s4uB6bj/B6qaBjwOJAMrgB9K2VRZv71aTYpX2Zq6zq2u2ARcqKqzqjs/pvq5V9K/Az1UdU115+dQEZGfgQmqerC9reoUKylEAREZJiJN3G559+O0GfxUzdkyNceNwI91PSCIM4zK4W710TU4pbr/Vne+apoaeRegqXIDgSk49c7LgJFucdpEORFJx+lnf1515+UQ6IpTjdcQpzfWBW71qvGw6iNjjDFBVn1kjDEmqNZVH7Vs2VI7duxY3dkwxpha5eeff96mqqV1RwdqYVDo2LEjqamp1Z0NY4ypVURkXdmprPrIGGOMhwUFY4wxQRYUjDHGBFlQMMYYE2RBwRhjTJAFBWOMMUEWFIwxxgRZUDDGmBpq3c51fL6i1JHTq5wFBWOMqUIbd2+koPDAp6yO/24801dOLzZv2/5tzFg9g0ItZG/u3sAznskvzCd5bTIDXh/AeVPP490l7/LMj8+wO2d3xPNf6+5oNsaYikjZkEKvNr04LPawshN7bN27FUVpE+88mXPb/m3c/c3djO07lsmLJnND/xvo0LQDmfszWZaxjJz8HJZlLOOv3/8VgK8v+5rfMn/juZTnaFyvMYu2LirXdgd1GMS6netYt6voBuRLP3Yeoy0i3HXyXRXaj4qqdaOk9u/fX22YC2NMOKpKgRbgFz9Tl06lU7NOJL6RCMD1/a7nh/U/sGnPJm464SYenfUoAE8OfpKTEk6iQAt4cvaTXNjtQq6bVvSk3F6H9yK3IJf9efuLnagPtd5tepN8ZTJN6jep1PIi8rOq9i8zXSSDgogMA14A/MDrqvpkyOdH4jwGsamb5l5VnX7AijwsKBhTdxRqIT5xarFzC3KJ88cBkJ2fTXZ+NnH+OPbl7mP5tuWs2bGGi7tfzOOzHmfL3i0MOWoIPVr3YPrK6by18C1uP+l2/u+7/6NQC9metf2Q7kdC4wTSd6cHp7u27Mr+vP1k52ez/vb1pG5KJaFxAj+u/5Fx08axN3dvMO2tJ95Kq8NaMX5m0eOtm9ZvyvDOw3lv6XssuWEJ7Ru3r3QwCKj2oOA+9vE34A9AOjAfGO0+CzWQZhKwQFVfFZFuwHRV7Vjaei0oGFMzqSo7s3fSrEGz4Ml+2/5tPPbDY9w44EZ25+xm5NSRDD16KIu2LmL+pvkA3D/ofib+PJHf9/0OQHxcPLkFueQWlOvRzOXSIKYBQzsPRRA++fWTUtOed+x5fLbiMwD+3P/PNIhtwLMpzwLw0vCXGNp5KB2adCDGF0NWfha7c3bTJr4NWXlZiAhzNszhjE5nUKiF7MvdR6N6jQ7Yxq7sXTw26zGu7XstXVp0AeDZOc9y1zd3semOTTSMa4ggLN66mFOOPKVKvoOaEBQSgYdUdag7fR+Aqj7hSTMRWK2qT7npn1XVk0tbrwUFYyJDVdmydwtHNDoiOG9e+jyem/scL5/1Mgu3LKRDkw489eNTDD16KFv2buGH9T8wa90sYnwxbNyzEYB2jdoF31eFto3aktA4gYu7Xcznv33OCW1PoGFsQx7+4WF6t+lNXkEeyzKWEeuLZWjnoUwaMYntWdv54rcvuL7/9TSt37TY+j5e/jGN6zWm1+G9aN6gOX6fn905u/GJD7/4aRDbINjgKyKAU6LZmb2TZvWbBedVNVVlT+4eGtdrHJH114SgcCEwTFXHutOXAyeq6k2eNEfgPCO1Gc4j8oao6s+lrdeCgjEVV1BYwN7cvcTHxbN572aa1W/Gp79+ytLfl/Jr5q/4xMcxzY/hyR+f5PaTbueXzb/w/brvq2TbPvFxWofTWLV9FWd3ORsRITMrk05NO/HUj08B8Ojpj9KtVTcSGifw2KzH8ImP9bvW8+kln5LQOKFK8hHtakJQuAgYGhIUBqjqzZ40d7h5eNYtKbwB9FDVwpB1jQPGARx55JH91q2rvsYeY2qCndk7ycrLokn9JqRlpNG8QXPW71rPJ8s/YVnGMvbk7uHYFsfy9uK3q2R7Zx59JikbUtiTuweAo5odRYwvhq17t5JfmE+Hph2YPHIye3P38sK8F3hh2Au0a9yOvII84vxxEbu6NuVX3qAQyS6p6UB7z3QCsCkkzTXAMABVTRGR+kBL4HdvIlWdBEwCp6QQqQwbc6gF6uHvn3k/jes1JjEhkTcWvEH7xu05PP5wPkz7kOXblvPOH99hypIp7M7ZjaIkr00uc90/bfwp7Px6/nrkFOQEp4cePZSxfcfyj9R/MGPNDACeGPwEl/e8nNYNWxPrj63QPp3W8bSibcXUq9CypvpFMijMB7qISCdgI3AJ8KeQNOuBwcA/RaQrUB/IiGCejIm4HVk7mLp0Kn2O6EPn5p1Jy0hj2e/L+GnTT6zbuY79efuZt3EeAIc3PJyt+7aWuc6LP7y4xM/i4+KDvVnG9B5Dv7b9WJ6xnL15e7ns+Mto1bAVD8x8gBv630Crhq3oe0RfCrWQrXu3sjN7J11bdQXgwm4XVsHem9ou0l1SzwIm4HQ3fVNVHxORh4FUVf3c7XH0GhAPKHC3qv63tHVam4KJpEItpKCwgFh/LNv2byOvIC9481JKego9D+/J0t+XkleQR15hHum70/l4+cf0OrwXE3+eyN7cvezL21epbY/rO45Jv0zipISTGH/qeNJ3pxMfF8/zc5+nS4suTuPq6Q8z4LUBjB80no5NOzKs87Cq3H1Th1V7m0KkWFAwVWHx1sW0aNCCZg2a8fSPT7N6x2oaxjZk1vpZrNy+koTGCazesTqYvlFco2B9emlKSjfimBG8ds5rTPttGl2ad+Gw2MOI88cxZckUvlvzHXcm3sno40dX6T4a42VBwUSl/MJ8cgty2Ze7j7SMNP6z8j/M2TCHi7pdxD9+/gd7cvZUuLtkq8NacXqn0/l8xedk52cH5/vFz7V9r6VTs04kdUwiJz+HUzucCjhDIiz9fSlJHZOqcveMqbSa0NBszEFTVVZkriDWF8tRzY5i6e9LWbl9Je8tfY8m9Zrw44Yf8YmPtIy0Utfz44Yfw85v3qA5cf44Jo6YSE5+Dvvy9jGq+yi+Wf0Nfdr04YhGRxDjK/o32bJ3C/Vj6h/Q9z1Uy8NaWkAwtZIFBVPt8gvzeXHeixzb4li2Z23nuJbH8c7id5iTPofUTRUvFQrCkKOG8M3qbwC47cTbGNBuAP3b9qdd43bMWjeLto3a0q1VN/w+f9h1nHvsuWHnB9oXjKmwlBRIToakJEhMrO7clMiCgomYwOBkMb4Y9uftZ3vWdvbk7KHbK90AOLrZ0ezO2U3G/rI7nMX4YojxxZCdn805x5xDxv4M/tTjT2RmZdKjdQ9aN2xNPX89Tkw4MbhMTn5O2C6RQzsPrbqdNAcnJQUmT3beX3HFgSfLgz2RVmb5qtpmixaQmemsB2DwYMjNhbg4mDCh6DPvNsJtO/AdbdkCbdqE/56qkLUpmCrz+77fycnPYfb62fyw7gf+nfZvtmdtR6nYb+zsLmdzVLOjeHzw4xRqYfC2//15+xGEBrENIpH92quqrkAD69m5ExYuhAsugHHjyk7fogV8+SVs2gTXXOMsU9KJzBsE+vSBm292TpQAfj+88krRNu+5B/72NygshJgYePnlos+8216woGg7ffo4J9tAnr74AlSd5c86y1m2TRto3NhZvm1buPvu4ifg00+HnBwnP3feCbvdZxiE24crrnD+Br5/cE7+OTlOvkXA54Njj4Vff3Xm+XzOugsKnPd33OFsY+5cWLzYyW/9+jDDuWeEpKSi7wigXj2YObPCx9oamk1EFWohqZtSmbxoMl+u+rJYT53SXN7zcrbu28p5x55HjC+GxIREOjTtwK7sXbRq2Iote7fQsWnHyGb+UCrrSri05UKvNgMnJO8JaPJkeOstyMtzTjAvv+zM/+gj56R+/PHFT8ILFkBaGmRnOydwgDfecE46S5Y4Jyqvu++Go4920tSvD926OesJnHBD0wN06QKrVjkntwC/3zlZfvNN8fnhDBrknCQXLjzws44d4Zhj4NtvnRNsVenYES6+GKZNc76fcHw+aN8e1q8vvg8izrTfD506OfteGp+vfHkfMMAJWp9+Wny+CDz2GNx3X9nrKLaYBQVTBXILcon1xbJxz0Zu+fKWMkeYDPhl3C/Ex8XTpUUXdmbvJNYXS8O4hhHO7SFW0onb+7n3Ki/0Sri09Q4e7Jy4A/+fPh/07OlcSQZOKIGTkYkufj/MmhWxkoK1KZgD5Bbk4hMfr8x/hVu/urXEdIG6/EFHDuKCbheQlZdFj9Y9iPXHFuuxU1ZPnVpp0iS46SbnCh2cE3RsLIwZU1SlMHZs8WJ/QQFcd51TJdG5s1OFkZ3tBI65c2H5cqdqYMcOyMoqvr3CwgOvnC0gRKdXXrE2BS8rKVQtVSUrP4vnUp7j6/99zez1s8OmO7vL2ZxzzDmc3P5kjmh0BE3qNanwmDiHRGn166FVLyVd5YeuI7R+vE8fuPFGyM8Pnwe/3zlhV2X1hjHglBhnz65UULCSgjlATn4Ocf44NuzewF+++Qvz0ueV+njBU488lRHHjODuU+4+hLmsgJQUePrpogZOcK7eCwqcK+4JE4oaIcGpBw9c2UPxk7YI/OEPsGEDrFhR9Fnv3uHr2ktTkbTGVISqc8FiJYUiVlKouPzCfL5f+z1D3h4S9vN2jdpxcfeLeeT0R2gY15C9uXuJ8cVQP6Z+5DIVuPpOS4OMDGjVymnEDO3hETihQ1Gvkfffdxoid+2yq/Ga5GDaOKqifSQwPPfBrqd5c9izxykJ1rTzYyV7HoGVFKKaqrJ251o27tnIoz88ytf/+7rY512ad2FU91Fs27+Nh5Ie4vD4w4t9Hh8XX74NhauO8Va5eK/iQ7sp/uc/xa/aly+HH35werl07x6+54kp3WGHwf79lVu2d2+n/3xqasUCrc8H557r9FKCoi6tga6kFVnPsceW3vMn0K0z3Im6TRs48siiEuP48c7FRmlE4NRTnXadn34qmnfXXc7v2NuzC5wLkp07i6/D74dLLoEPPijevhTochqul1JJGjQo3rnAq3dvOOmkiN+jADgnkNr06tevn5rw8gvyNXVjqsY8HKM8xAGve765Rzft3lT6SubMUX38cedv6LT3/cSJqrGxqj6falyc8x5URVTbtHHee18DBjhpQ+fbq/SXiOqZZ6refbeq3198fuvWzl9wvtuuXcMvX9J6vdPXX+8c15Ejne2UtFxg2UsvLf47Cfc7GjRItVUr1S5dih/7evWc/RkwwPnd+P2qDRo4v6kGDcJvLybG+fz660v/vvx+J92cOc52QvdXxEkzcmRR3idOLL6OiRPD/z+EpvOuY84c5zgF9tPnK77PgXzdfXf4/wOfz0njPcahx+cg4YxOTVkvKynUcgWFBSzLWMbYz8cGH4Qe0L9tf24ecDPnHnsuTeo1KfnpV94r+EC9u88Ho0bBe+8VvwknXH25t4eNavEqn4Cfwj/wpdbr0sWpagi3z1DUz//Pf3a+OxH405+cq9hAX3ufD/r3d+4DyM521vnBB0VtIw895Fwdjhx54E1T3hulVqxwbtJq0wY2biw6pXj7xQeuOPv0gVtuKbrDtk+fojtu/X649tqi+xreesupSvH7i3pXledqdf58Z307dhTNE4Grr4annMdwhu0YEOjV5fPBkCFOdUlBAdx2m9NOFBNTciN/QYGz/PffO8t5OxOU1HU4M7PoKt7nc6YDEhOL0iYnF32XPp9zH0Hgs8RE5zjNmuXss0jxrsPXXuuUlp94oqiaK/CZ3++kDdzMFhtbvBR9iFlQqEUKtRBVJS0jjUVbF/HMnGdYvHVxsTTnHnsuD572IN1adTuwTSDcjU9paU5vhtCifmEhTJlSNK0a3Q2ogaqGwM1b4XoshbuRLHBPwvHHH1i9FjiBBIY98J6obrzxwJOl9wQVMGOGczIKBBgRGDEC3nyzKFgH5sfFFe/O6M1TcrKTPnCMjzyyKO9XXFHxO6a96wvcwRvIQyCghdunzMziJ9NA/gsLnfVlZjrVjIGqyaQk57sL7fqbnOzc3FWe/LZo4fy+A9tq0SJ8uqQkJ0gHjlng/8jryiudv336OEEskDawz0lJznQg+I4Zc2Dam2+GZ58tOhaxscW/swizoFDDbd6zmW9Wf8P9M+9n/a71xT5rE9+GE9qeQHZ+Nq+c/Qp+8ZPYPqRbZWAYgLQ0+PHH6D6x+3zOVfvKlcXnDxoEl1564NUklP9u5MDJraQTaOjJLzHROaGXdLINFwBK2q73CtV7Apo4sehkp+pcXXt7roRuI3CyCj3hlTcvXt6TX2lj/ZS13AUXFN+3wPKfeG6iHDnSCRKBIS3q1Qt/wi5JZmbxEoC3pOBV2jEL3LcSKN1dcUX4tCWtIzRAB4g47SSHcAA9631Uw2TnZ/NcynOkpKewZscalmUsK/b5SRvgwt9bsr0BPHj8TcS1PsI56YPTEPbFF86NT+vX182eOa1aQdeuMG9eUTF99GincTow3o13WIBBg5zeJIHxd5KT4f77i6pyrrsOXn212nanyoS7t8JbteTzOSerGTNKPsFU9SielV1fuH0pz3oOZnvewepK+45KWn7QoKIqLZ8PHn20wsNQVFl+SmDDXNQyaRlpTPttGk/9+BTbs7YH5x8RfwQjMlswbs8x9O4xhJjb7zzwbtfaKNDbpW1bGD7cGavHOy6Oz1f0HpwTuPekVtoJYNKkorF/QoeUiNA/3EGJ1JDKZQ3DYYoczDF44gmnt1PgIiw21mnTqIrBCavwmFlQqAUy9mUw7bdp3PPtPcHho9s2asvpHU9n4uZ+NJw81Wl8TEmp1oaniLj++uJX6N6TdaCRrrCwqNHzyCOr/wo2EmpikDIV4y2VhbYl1SB2n0INtGTrEvIK8zjtn6dxcvuT+X7t9+QU5ABw2b6jeXVbIg1XrUPWz4Z1U8pY2yHUvDls3158XlycMxTx558Xr6YaOdKpqgn0WAk0nH3xhdPHHMI3nHnrWlu0OLCRripPlJWpI48Ub4Nsbm7E71Y1EVBW+1AtYyWFCMsryOPHDT9y+r9OP+Czbq26cVWvq/jD1nh6jb4N8fagqAlE4C9/cRpnr7uuaP7IkUVj0E+a5HS3LCx0TuCBuy1LelhIef9xatLVfCRZScEcIjWi+khEhgEvAH7gdVV9MuTz54HA2fIwoLWqljqkZm0ICqrK3PS5TFkyhZfnv1zss/OOPY+BRw5kcKfB9GrTC9/ceU7vkf/+t+oyEB/vnGQqEmRC77YcNAiefLLoBFVWPX00nMAjxb4/cwhUe1AQET/wG/AHIB2YD4xW1bD3sYvIzUAfVR1T2nprelD4YNkHjPpw1AHzR3UfxQvDXuDwJauLTgBLlhTd1FRZnTs7QWCxe79CoDEWnH71gXXXqwe33uoMH9G7tzN2UOCGq0DPnCVLSj7xG2NqtZrQpjAAWKWqq90MTQXOA0oY3ITRwIMRzE9EfbfmOwZPHhycruevx38v/y+qykkJJ1Fv/i/wl4fg9ddLvhuzJA0awAknOH3pv/zS6YLasKFzkg99PKH3anPWrIo99Ssx0YKBMVEukiWFC4FhqjrWnb4cOFFVbwqTtgMwF0hQ1QMum0VkHDAO4Mgjj+y3bl3Jwz0fKoVaSFpGGj+s+4Fbv7qV/MKiE/1vN/1G5+adi4aV8D73taIuvRTeeaeKcm2MiVY1oaQQbqCdkiLQJcCH4QICgKpOAiaBU31UNdmrvILCAi775DKmLp0KgE98+MXP/Gvn07tN7+LBYPJk+OqrigWEwFhwPuyXAAAgAElEQVQ4gdFFjTHmEIlkUEgH2numE4BNJaS9BLgxgnmpEnty9rBwy0Ju//p2ft78MwAPDHqA+0+7v9jjJwGnYfaGGyp+V3FoA68xxhxCkQwK84EuItIJ2Ihz4v9TaCIRORZoBqREMC8HbUfWDpo/3RwAv/j553n/5PJel+MTX1GiwDMEFiyA8lZxNWgACQnQrJmVDIwx1S5iQUFV80XkJuBrnC6pb6rqMhF5GGdc78/dpKOBqVqDb5hYvWM1g94aFJx+/8L3uaDbBU5p4I03nK6fOTnFH+NYFhH4xz8sCBhjapSI3tGsqtOB6SHzHgiZfiiSeThYWXlZXPrxpWzcs5Grel/FW+e95QSDwUeUPIZ+efzlLxYQjDE1jg1zUYaXfnqJuelzua7fdfxjxD9g6NDK32jWpYtVExljajQLCmWYkz4HgL8P/ztcdlnFA0KbNs6wEIfi2arGGHOQLCiUQlWZlz6PS4+/lNifUos/iaw8/H74+GMLBsaYWsOCQimS33uC+6duZlj8MlhxQcUWtq6lxphayIJCCXTOHE6+YjxJBQALS08cG1v0JK9zzikaQdQYY2oZCwol+N8nb9CpQMPell1MYBhpG+XSGFMHWFAIY3fObi7f8SazcMblCBsYRJwSQqBUYMHAGFMHWFAIY/b62fTY6txxFzYgdO0Kl19uJQNjTJ1jQSGEqvJR2kdcu6CURLfdZvcZGGPqJF/ZSaLLht0b8L3+Jv03hikldOsGEydaQDDG1FlWUgixc+ZXvDrNqToK6tgR3n3XqoqMMXWelRRC7H/soQPbEoYNs4BgjIkKFhQ88n+cRf/UzQd+0KfPoc+MMcZUAwsKHplffowQpi1hQWmtzsYYU3dYUPCY2mIzef6SnxlqjDF1nQUFjzfqp3HHvb2RkSOd5ySLQFycM8KpMcZEAet95NqVvYv4n5dwdUZ/aNsGXn0VMjPtBjVjTFSxoOBa+Z/JzHwL4gpTgVRnCIvvv7eAYIyJKlZ95Ko35X3iCj2NzHl5MHlydWbJGGMOOQsKroLNm6o7C8YYU+0iGhREZJiIrBCRVSJybwlpLhaRNBFZJiLvRjI/pdmTu6f4DJ/PGpiNMVEnYm0KIuIHXgb+AKQD80Xkc1VN86TpAtwHnKKqO0SkdaTyU6qUFAYs2lY0LeI0NFt7gjEmykSypDAAWKWqq1U1F5gKnBeS5lrgZVXdAaCqv0cwPyXK/vYr/AWe9gSfD44/vjqyYowx1SqSQaEdsMEzne7O8zoGOEZEfhSRuSIyLNyKRGSciKSKSGpGRkaVZ3R1ryNR8dy0puo8Sc0YY6JMJINCuOfThN4sHAN0AZKA0cDrItL0gIVUJ6lqf1Xt36pVqyrP6M75sxFvzmJjnfsTjDEmykQyKKQD7T3TCUBoF5904DNVzVPVNcAKnCBx6KSkcOLj/yo+Murw4daeYIyJSpEMCvOBLiLSSUTigEuAz0PSfAqcDiAiLXGqk1ZHME8HSk5GCrV4saZNm0OaBWOMqSkiFhRUNR+4CfgaWA58oKrLRORhETnXTfY1kCkiacBM4C+qmhmpPIWVlES+dxC82FjrimqMiVoRHeZCVacD00PmPeB5r8Ad7qta5BXkUYgTFMTvh5desqojY0zUivo7mrd/9QkxhZ4vIvPQFlSMMaYmifqgkNatFbl+UL/fGSbbeh0ZY6JY1AeFOQnK4Csh76H7YcYMqzoyxkS1qA8Ky7ctp0384cT546o7K8YYU+2i/nkKx378A/dN/R10PNSrZ6UFY0xUi+qSgs6Zwz3vbcBfoFBYCDk5NryFMSaqRXVQyJ7xFT7vg3X8fmtoNsZEtagOClv6HVd045rdo2CMMdEdFLbtd5+hIOIEBRsu2xgT5aI6KDR+/1NiC0BUoaDA2hOMMVEveoNCSgpHf/YDPtzqI5/P2hOMMVEverukJicj+QVFjcyFhdWZG2OMqRGit6SQlISK2NPWjDHGI3qDAlDofRCcPW3NGGOiOCgkJ+NX9x4FEbj6auuOaoyJelEbFApbNMenbiOzKvTpU91ZMsaYahe1QWH/5g0U4Ckp2HMUjDEmeoPCjoY+/HhKCi1aVHOOjDGm+kVtUNi/eX1RScHns5KCMcYQxUEho0EhhT5Qn88ZMtt6HhljTPmCgogcLSL13PdJInKLiDQtx3LDRGSFiKwSkXvDfH6ViGSIyEL3Nbbiu1AJKSmc9PS7+Atx2hMmTLCeR8YYQ/lLCh8BBSLSGXgD6AS8W9oCIuIHXgaGA92A0SLSLUzS91W1t/t6vfxZPwiTJ+PPK8APSEEBLFhwSDZrjDE1XXmDQqGq5gN/BCao6u3AEWUsMwBYpaqrVTUXmAqcV/msGmOMibTyBoU8ERkNXAlMc+fFlrFMO2CDZzrdnRfqAhFZLCIfikj7cCsSkXEikioiqRkZGeXMcimuuIK8GB8FAsTFwRVXHPw6jTGmDihvULgaSAQeU9U1ItIJeKeMZSTMPA2Z/gLoqKo9gW+Bf4VbkapOUtX+qtq/VatW5cxyKRITeeWK45jftTH8/e/WnmCMMa5yjZKqqmnALQAi0gxopKpPlrFYOuC98k8ANoWs19sP9DXgqfLk56ClpHD9278SW1AIt93mPFzHAoMxxpS791GyiDQWkebAIuAtEXmujMXmA11EpJOIxAGXAJ+HrNfbLnEusLz8WT8IycnE5Bc6vY9yc210VGOMcZW3+qiJqu4GzgfeUtV+wJDSFnAbpm8CvsY52X+gqstE5GEROddNdouILBORRTglkasqsxMV1qIFKhS1Kdg9CsYYA5T/ITsx7lX9xcD/K+/KVXU6MD1k3gOe9/cB95V3fVUiJQVuuw0pBPX77B4FY4zxKG9J4WGcK/7/qep8ETkKWBm5bEVQcjKam0sM7rOZbXgLY4wJKm9D87+Bf3umVwMXRCpTEZWUBDExFBQUoH6/VR0ZY4xHeRuaE0TkExH5XUS2ishHIpIQ6cxFimqgZ2xoD1ljjIlu5a0+egun51BbnBvQvnDn1T7JyUhBPn7AV2jPZTbGGK/yBoVWqvqWqua7r38CVXAXWTVISqIwNpY8AY2NseojY4zxKG9Q2CYil4mI331dBtTOFtrERFInP8kDZ8Cy9+xuZmOM8SpvUBiD0x11C7AZuBBn6ItaaUP3BJ48FXwnn1zdWTHGmBqlXEFBVder6rmq2kpVW6vqSJwb2WqlXdm7AGhcr3E158QYY2qWg3ny2h1VlotDbHfObsCCgjHGhDqYoBBuFNRaIRAUGsU1quacGGNMzXIwQaHWdvJvumA5D86Jwz/vp+rOijHG1Cil3tEsInsIf/IXoEFEchRpKSnccO+H+PIKYNZgmDHDeiAZY4yr1JKCqjZS1cZhXo1UtbyD6dUsycn48wuIUWzYbGOMCXEw1Ue1U1ISeTE+8n3YsNnGGBMi+oJCYiI339mVyRd0tqojY4wJEX1BAZiToEy/sJcFBGOMCRGVQWFX9i6a1GtS3dkwxpgaJyqDwu6c3XbjmjHGhBF1QaFQC9mTu4dG9ezGNWOMCRV1QSFnVjL3zoJjfqudg7waY0wkRTQoiMgwEVkhIqtE5N5S0l0oIioi/SOZH1JSqD/sbB75Dkbd+hqkpER0c8YYU9tELCiIiB94GRgOdANGi0i3MOkaAbcA8yKVl6DkZMjNI0bBn19gN64ZY0yISJYUBgCrVHW1quYCU4HzwqR7BHgayI5gXhxJSWic89S1whh76poxxoSKZFBoB2zwTKe784JEpA/QXlWnlbYiERknIqkikpqRkVH5HCUmsubfr/HAGfDDmw/YfQrGGBMikkEh3NDawcH1RMQHPA/cWdaKVHWSqvZX1f6tWh3co6G39z6WJ0+F/f17HdR6jDGmLopkUEgH2numE4BNnulGQA8gWUTWAicBn0e6sTkrPwuABjG1c5BXY4yJpEgGhflAFxHpJCJxwCXA54EPVXWXqrZU1Y6q2hGYC5yrqqkRzBPZ+U7TRf2Y+pHcjDHG1EoRCwqqmg/cBHwNLAc+UNVlIvKwiJwbqe2WJSvPLSnEWknBGGNCRfSZCKo6HZgeMu+BEtImRTIvAVZSMMaYkkXdHc3WpmCMMSWLuqBgJQVjjCmZBQVjjDFBURcUmi9Ywb2z4LDURdWdFWOMqXEi2tBc46SkMPr215Fc8M85yx7HaYwxIaKrpJCcjC+vgBgFyc21AfGMMSZEdAWFpCQKYvzkCRAXZwPiGWNMiOgKComJPPvIcJ4Z1siqjowxJozoCgpAWpcmvD60pQUEY4wJI+qCQlZelg1xYYwxJYi6oJCdn233KBhjTAmiLihk5WfZEBfGGFOCqAsKR/+awdVfboGUlOrOijHG1DhRd/Pai39bSmyBwn8GWw8kY4wJEV0lheRkYvIVfyFgN68ZY8wBoisoJCWRFyMU+MRuXjPGmDCiKygkJnLRdc347NL+VnVkjDFhRFdQAH5MKCR59EkWEIwxJoyoCwpZedYl1RhjShJVQUFV6bM2hz98kGpdUo0xJoyIBgURGSYiK0RklYjcG+bz60VkiYgsFJHZItItkvnJnf09M/4FZ7w5EwYPtsBgjDEhIhYURMQPvAwMB7oBo8Oc9N9V1eNVtTfwNPBcpPIDUDjzO+IKwFeo1iXVGGPCiGRJYQCwSlVXq2ouMBU4z5tAVXd7JhsCGsH8sPfk/uT6odDvsy6pxhgTRiSDQjtgg2c63Z1XjIjcKCL/wykp3BJuRSIyTkRSRSQ1IyOj0hna068Hg6+EBTecb11SjTEmjEgGBQkz74CSgKq+rKpHA/cA48OtSFUnqWp/Ve3fqlWrSmcoOz+bue3hf9dfZAHBGGPCiGRQSAfae6YTgE2lpJ8KjIxgfsjKywKwobONMaYEkQwK84EuItJJROKAS4DPvQlEpItn8mxgZQTzQ+y8VO6dBYcvXh3JzRhjTK0VsVFSVTVfRG4Cvgb8wJuqukxEHgZSVfVz4CYRGQLkATuAKyOVH1JS6P6nW3kkB+THeyHhRKtCMsaYEBEdOltVpwPTQ+Y94Hl/ayS3X0xyMpKbh19B8/Kd7qgWFIwxppjouaM5KYnCuBjyBDQ21rqjGmNMGNETFBIT+WbiPTxwBmz6dLKVEowxJozoCQrAhu4JPHkqSOLJ1Z0VY4ypkaIqKGTnZwPWJdUYY0oSVUEhcJ9Cg1gbOtsYY8KJqqBgJQVjjCld1AWFOH8cPomq3TbGmHKLqrPj4UtWc98s7DkKxhhTgojevFajpKTw53s/wpdXALMG2yipxhgTRvSUFJKT8ecXEqPYA3aMMaYE0RMUkpLIj/GR78MesGOMMSWInqCQmMhf7z+Vv49obVVHxhhTguhpUwCWHB3PxlbtuN0CgolSeXl5pKenk52dXd1ZMRFSv359EhISiI2NrdTyURUUcvJzqBdTr7qzYUy1SU9Pp1GjRnTs2BGRcA9HNLWZqpKZmUl6ejqdOnWq1Dqip/oIyCnIoZ7fgoKJXtnZ2bRo0cICQh0lIrRo0eKgSoLRFRSspGCMBYQ67mCPb3QFBSspGGNMqaIrKFhJwZhqlZmZSe/evenduzdt2rShXbt2wenc3NxyrePqq69mxYoVpaZ5+eWXmTJlSlVkucqNHz+eCRMmHDD/yiuvpFWrVvTu3bsaclUkqhqau67cyYUpKyEhxbqkGlMNWrRowcKFCwF46KGHiI+P56677iqWRlVRVXy+8Nesb731VpnbufHGGw8+s4fYmDFjuPHGGxk3bly15iN6gkJKCu+8spm4gi3wsQ1zYcxtX93Gwi0Lq3Sdvdv0ZsKwA6+Cy7Jq1SpGjhzJwIEDmTdvHtOmTeOvf/0rv/zyC1lZWYwaNYoHHnAe7z5w4EBeeuklevToQcuWLbn++uv58ssvOeyww/jss89o3bo148ePp2XLltx2220MHDiQgQMH8t1337Fr1y7eeustTj75ZPbt28cVV1zBqlWr6NatGytXruT1118/4Er9wQcfZPr06WRlZTFw4EBeffVVRITffvuN66+/nszMTPx+Px9//DEdO3bk8ccf57333sPn8zFixAgee+yxcn0Hp512GqtWrarwd1fVoqf6KDmZ2HzwF6oNc2FMDZSWlsY111zDggULaNeuHU8++SSpqaksWrSIb775hrS0tAOW2bVrF6eddhqLFi0iMTGRN998M+y6VZWffvqJZ555hocffhiAv//977Rp04ZFixZx7733smDBgrDL3nrrrcyfP58lS5awa9cuvvrqKwBGjx7N7bffzqJFi5gzZw6tW7fmiy++4Msvv+Snn35i0aJF3HnnnVX07Rw6ES0piMgw4AXAD7yuqk+GfH4HMBbIBzKAMaq6LiKZSUoiNwakQPDbMBfGVOqKPpKOPvpoTjjhhOD0e++9xxtvvEF+fj6bNm0iLS2Nbt26FVumQYMGDB8+HIB+/foxa9assOs+//zzg2nWrl0LwOzZs7nnnnsA6NWrF927dw+77IwZM3jmmWfIzs5m27Zt9OvXj5NOOolt27ZxzjnnAM4NYwDffvstY8aMoUED50FezZs3r8xXUa0iVlIQET/wMjAc6AaMFpFuIckWAP1VtSfwIfB0pPJDYiJDr/TzzVWDrOrImBqoYcOGwfcrV67khRde4LvvvmPx4sUMGzYsbN/7uLi44Hu/309+fn7YdderV++ANKpaZp7279/PTTfdxCeffMLixYsZM2ZMMB/hun6qaq3v8hvJ6qMBwCpVXa2qucBU4DxvAlWdqar73cm5QEKkMlOohcxOKGDu5UkWEIyp4Xbv3k2jRo1o3Lgxmzdv5uuvv67ybQwcOJAPPvgAgCVLloStnsrKysLn89GyZUv27NnDRx99BECzZs1o2bIlX3zxBeDcFLh//37OPPNM3njjDbKynEf/bt++vcrzHWmRDArtgA2e6XR3XkmuAb4M94GIjBORVBFJzcjIqFRmcguc7m52n4IxNV/fvn3p1q0bPXr04Nprr+WUU06p8m3cfPPNbNy4kZ49e/Lss8/So0cPmjRpUixNixYtuPLKK+nRowd//OMfOfHEE4OfTZkyhWeffZaePXsycOBAMjIyGDFiBMOGDaN///707t2b559/Puy2H3roIRISEkhISKBjx44AXHTRRZx66qmkpaWRkJDAP//5zyrf5/KQ8hShKrVikYuAoao61p2+HBigqjeHSXsZcBNwmqrmlLbe/v37a2pqaoXzsyt7F02fasqzZz7LHYl3VHh5Y+qC5cuX07Vr1+rORo2Qn59Pfn4+9evXZ+XKlZx55pmsXLmSmJja3ykz3HEWkZ9VtX9Zy0Zy79OB9p7pBGBTaCIRGQL8P8oREA5GToGzaispGGMA9u7dy+DBg8nPz0dVmThxYp0ICAcrkt/AfKCLiHQCNgKXAH/yJhCRPsBEYJiq/h7BvJCd7zQO2R3NxhiApk2b8vPPP1d3NmqciLUpqGo+TpXQ18By4ANVXSYiD4vIuW6yZ4B44N8islBEPo9UfnLyraRgjDFliWhZSVWnA9ND5j3geT8kktv3ClQf1Y+pf6g2aYwxtU7U3NEcLClY9ZExxpQoaoJC7E8/c+8saL34f9WdFWOMqbGiIyikpNB99C088h2ccMV9kJJS3TkyJiolJSUdcCPahAkT+POf/1zqcvHx8QBs2rSJCy+8sMR1l9VdfcKECezfvz84fdZZZ7Fz587yZP2QSk5OZsSIEQfMf+mll+jcuTMiwrZt2yKy7egICsnJSG4eMQq+vDwbDM+YikhJgSeeqJKLqdGjRzN16tRi86ZOncro0aPLtXzbtm358MMPK7390KAwffp0mjZtWun1HWqnnHIK3377LR06dIjYNqIjKCQlURgXQ56AxsbaYHjGlFdKCgweDPff7/w9yMBw4YUXMm3aNHJynDa+tWvXsmnTJgYOHBi8b6Bv374cf/zxfPbZZwcsv3btWnr06AE4Q1Bccskl9OzZk1GjRgWHlgC44YYb6N+/P927d+fBBx8E4MUXX2TTpk2cfvrpnH766QB07NgxeMX93HPP0aNHD3r06BF8CM7atWvp2rUr1157Ld27d+fMM88stp2AL774ghNPPJE+ffowZMgQtm7dCjj3Qlx99dUcf/zx9OzZMzhMxldffUXfvn3p1asXgwcPLvf316dPn+Ad0BETeKBFbXn169dPK+Pbdx7Rewej/5v2TqWWN6YuSEtLq9gCjz+u6vergvP38ccPOg9nnXWWfvrpp6qq+sQTT+hdd92lqqp5eXm6a9cuVVXNyMjQo48+WgsLC1VVtWHDhqqqumbNGu3evbuqqj777LN69dVXq6rqokWL1O/36/z581VVNTMzU1VV8/Pz9bTTTtNFixapqmqHDh00IyMjmJfAdGpqqvbo0UP37t2re/bs0W7duukvv/yia9asUb/frwsWLFBV1YsuukjffvvtA/Zp+/btwby+9tprescdd6iq6t1336233nprsXS///67JiQk6OrVq4vl1WvmzJl69tlnl/gdhu5HqHDHGUjVcpxjo6OkAGzq0YEnT4XCxBPLTmyMcSQlQVwc+P3O3yooZXurkLxVR6rK//3f/9GzZ0+GDBnCxo0bg1fc4fzwww9cdtllAPTs2ZOePXsGP/vggw/o27cvffr0YdmyZWEHu/OaPXs2f/zjH2nYsCHx8fGcf/75wWG4O3XqFHzwjnfoba/09HSGDh3K8ccfzzPPPMOyZcsAZyht71PgmjVrxty5cxk0aBCdOnUCat7w2lETFIJ3NNvNa8aUX2KiM9T8I49U2ZDzI0eOZMaMGcGnqvXt2xdwBpjLyMjg559/ZuHChRx++OFhh8v2CjdM9Zo1a/jb3/7GjBkzWLx4MWeffXaZ69FSxoALDLsNJQ/PffPNN3PTTTexZMkSJk6cGNyehhlKO9y8miRqgoLdvGZMJSUmwn33VdmQ8/Hx8SQlJTFmzJhiDcy7du2idevWxMbGMnPmTNatK/15W4MGDWLKlCkALF26lMWLFwPOsNsNGzakSZMmbN26lS+/LBp8uVGjRuzZsyfsuj799FP279/Pvn37+OSTTzj11FPLvU+7du2iXTtnEOh//etfwflnnnkmL730UnB6x44dJCYm8v3337NmzRqg5g2vHT1BwW5eM6bGGD16NIsWLeKSSy4Jzrv00ktJTU2lf//+TJkyheOOO67Uddxwww3s3buXnj178vTTTzNgwADAeYpanz596N69O2PGjCk27Pa4ceMYPnx4sKE5oG/fvlx11VUMGDCAE088kbFjx9KnT59y789DDz0UHPq6ZcuWwfnjx49nx44d9OjRg169ejFz5kxatWrFpEmTOP/88+nVqxejRo0Ku84ZM2YEh9dOSEggJSWFF198kYSEBNLT0+nZsydjx44tdx7LK2JDZ0dKZYfO/uzXz3h78du8e8G7xPnjyl7AmDrIhs6ODjV16Owa5bzjzuO8484rO6ExxkSxqKk+MsYYUzYLCsZEmdpWZWwq5mCPrwUFY6JI/fr1yczMtMBQR6kqmZmZ1K9f+V6WUdOmYIwh2HMlIyOjurNiIqR+/fokJCRUenkLCsZEkdjY2OCdtMaEY9VHxhhjgiwoGGOMCbKgYIwxJqjW3dEsIhlA6YOilKwlEJnHFdVcts/RwfY5OhzMPndQ1VZlJap1QeFgiEhqeW7zrktsn6OD7XN0OBT7bNVHxhhjgiwoGGOMCYq2oDCpujNQDWyfo4Ptc3SI+D5HVZuCMcaY0kVbScEYY0wpLCgYY4wJioqgICLDRGSFiKwSkXurOz9VRUTai8hMEVkuIstE5FZ3fnMR+UZEVrp/m7nzRURedL+HxSLSt3r3oPJExC8iC0RkmjvdSUTmufv8vojEufPrudOr3M87Vme+K0tEmorIhyLyq3u8E+v6cRaR293f9VIReU9E6te14ywib4rI7yKy1DOvwsdVRK50068UkSsPJk91PiiIiB94GRgOdANGi0i36s1VlckH7lTVrsBJwI3uvt0LzFDVLsAMdxqc76CL+xoHvHros1xlbgWWe6afAp5393kHcI07/xpgh6p2Bp5309VGLwBfqepxQC+cfa+zx1lE2gG3AP1VtQfgBy6h7h3nfwLDQuZV6LiKSHPgQeBEYADwYCCQVIqq1ukXkAh87Zm+D7ivuvMVoX39DPgDsAI4wp13BLDCfT8RGO1JH0xXm15AgvvPcgYwDRCcuzxjQo858DWQ6L6PcdNJde9DBfe3MbAmNN91+TgD7YANQHP3uE0DhtbF4wx0BJZW9rgCo4GJnvnF0lX0VedLChT9uALS3Xl1iltc7gPMAw5X1c0A7t/WbrK68l1MAO4GCt3pFsBOVc13p737Fdxn9/Ndbvra5CggA3jLrTJ7XUQaUoePs6puBP4GrAc24xy3n6nbxzmgose1So93NAQFCTOvTvXDFZF44CPgNlXdXVrSMPNq1XchIiOA31X1Z+/sMEm1HJ/VFjFAX+BVVe0D7KOoSiGcWr/PbvXHeUAnoC3QEKf6JFRdOs5lKWkfq3TfoyEopAPtPdMJwKZqykuVE5FYnIAwRVU/dmdvFZEj3M+PAH5359eF7+IU4FwRWQtMxalCmgA0FZHAQ6O8+xXcZ/fzJsD2Q5nhKpAOpKvqPHf6Q5wgUZeP8xBgjapmqGoe8DFwMnX7OAdU9LhW6fGOhqAwH+ji9lqIw2ms+rya81QlRESAN4Dlqvqc56PPgUAPhCtx2hoC869wezGcBOwKFFNrC1W9T1UTVLUjzrH8TlUvBWYCF7rJQvc58F1c6KavVVeQqroF2CAix7qzBgNp1OHjjFNtdJKIHOb+zgP7XGePs0dFj+vXwJki0swtYZ3pzquc6m5kOUQNOWcBvwH/A/5fdeenCvdrIE4xcTGw0H2dhVOXOgNY6f5t7qYXnJ5Y/wOW4PTsqPb9OIj9TwKmue+PAn4CVgH/Buq584AdnloAAAI/SURBVOu706vcz4+q7nxXcl97A6nusf4UaFbXjzPwV+BXYCnwNlCvrh1n4D2cNpM8nCv+aypzXIEx7r6vAq4+mDzZMBfGGGOCoqH6yBhjTDlZUDDGGBNkQcEYY0yQBQVjjDFBFhSMMcYEWVAwxiUiBSKy0POqshF1RaSjdyRMY2qqmLKTGBM1slS1d3VnwpjqZCUFY8ogImtF5CkR+cl9dXbndxCRGe7Y9jNE5Eh3/uEi8omILHJfJ7ur8ovIa+4zAv4rIg3c9LeISJq7nqnVtJvGABYUjPFqEFJ9NMrz2W5VHQC8hDPWEu77yaraE5gCvOjOfxH4XlV74YxRtMyd3wV4WVW7AzuBC9z59wJ93PVcH6mdM6Y87I5mY1wisldV48PMXwucoaqr3QEIt6hqCxHZhjPufZ47f7OqthSRDCBBVXM86+gIfKPOg1MQkXuAWFV9VES+AvbiDF/xqarujfCuGlMiKykYUz5awvuS0oST43lfQFGb3tk4Y9r0A372jAJqzCFnQcGY8hnl+Zvivp+DM1IrwKXAbPf9DOAGCD5LunFJKxURH9BeVWfiPDioKXBAacWYQ8WuSIwp0kBEFnqmv1LVQLfUeiIyD+dCarQ77xbgTRH5C86T0a52598KTBKRa3BKBDfgjIQZjh94R0Sa4IyC+byq7qyyPTKmgqxNwZgyuG0K/VV1W3XnxZhIs+ojY4wxQVZSMMYYE2QlBWOMMUEWFIwxxgRZUDDGGBNkQcEYY0yQBQVjjDFB/x8xsLuwKlTt1AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'r.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 45us/step\n",
      "1500/1500 [==============================] - 0s 51us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7771187589327494, 0.8226666666984558]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9170150950749715, 0.7619999998410543]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jbard\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 1s 190us/step - loss: 1.9826 - acc: 0.1400 - val_loss: 1.9484 - val_acc: 0.1610\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 1s 106us/step - loss: 1.9543 - acc: 0.1607 - val_loss: 1.9343 - val_acc: 0.1910\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 1.9371 - acc: 0.1756 - val_loss: 1.9241 - val_acc: 0.2010\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 1.9290 - acc: 0.1829 - val_loss: 1.9155 - val_acc: 0.2140\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 1.9183 - acc: 0.1960 - val_loss: 1.9070 - val_acc: 0.2270\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.9086 - acc: 0.2041 - val_loss: 1.8977 - val_acc: 0.2320\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.8970 - acc: 0.2140 - val_loss: 1.8864 - val_acc: 0.2460\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.8943 - acc: 0.2191 - val_loss: 1.8749 - val_acc: 0.2560\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.8806 - acc: 0.2313 - val_loss: 1.8616 - val_acc: 0.2650\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.8675 - acc: 0.2440 - val_loss: 1.8469 - val_acc: 0.2830\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.8585 - acc: 0.2533 - val_loss: 1.8305 - val_acc: 0.2930\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 1.8459 - acc: 0.2568 - val_loss: 1.8140 - val_acc: 0.3100\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 1.8360 - acc: 0.2619 - val_loss: 1.7973 - val_acc: 0.3220\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 1.8219 - acc: 0.2719 - val_loss: 1.7774 - val_acc: 0.3350\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 1s 97us/step - loss: 1.8043 - acc: 0.2783 - val_loss: 1.7572 - val_acc: 0.3510\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 1.7863 - acc: 0.2935 - val_loss: 1.7349 - val_acc: 0.3590\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 1.7684 - acc: 0.2997 - val_loss: 1.7127 - val_acc: 0.3850\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 1s 106us/step - loss: 1.7579 - acc: 0.3075 - val_loss: 1.6903 - val_acc: 0.3880\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 1s 103us/step - loss: 1.7389 - acc: 0.3084 - val_loss: 1.6685 - val_acc: 0.4130\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 1s 109us/step - loss: 1.7261 - acc: 0.3215 - val_loss: 1.6467 - val_acc: 0.4180\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 1.7089 - acc: 0.3340 - val_loss: 1.6221 - val_acc: 0.4350\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 1s 109us/step - loss: 1.6917 - acc: 0.3303 - val_loss: 1.5984 - val_acc: 0.4480\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 1.6720 - acc: 0.3365 - val_loss: 1.5766 - val_acc: 0.4620\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 1.6608 - acc: 0.3532 - val_loss: 1.5543 - val_acc: 0.4650\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 1.6465 - acc: 0.3543 - val_loss: 1.5343 - val_acc: 0.4740\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.6223 - acc: 0.3713 - val_loss: 1.5097 - val_acc: 0.4810\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 1.6118 - acc: 0.3659 - val_loss: 1.4884 - val_acc: 0.4800\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 1.5985 - acc: 0.3791 - val_loss: 1.4685 - val_acc: 0.5000\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 1.5766 - acc: 0.3848 - val_loss: 1.4474 - val_acc: 0.5000\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 1.5695 - acc: 0.3961 - val_loss: 1.4273 - val_acc: 0.5100\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.5380 - acc: 0.4067 - val_loss: 1.4050 - val_acc: 0.5240\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.5359 - acc: 0.4092 - val_loss: 1.3867 - val_acc: 0.5340\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 1.5256 - acc: 0.4115 - val_loss: 1.3697 - val_acc: 0.5470\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 1.5094 - acc: 0.4160 - val_loss: 1.3517 - val_acc: 0.5560\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 1.4960 - acc: 0.4261 - val_loss: 1.3323 - val_acc: 0.5630\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.4720 - acc: 0.4303 - val_loss: 1.3109 - val_acc: 0.5730\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 1.4639 - acc: 0.4396 - val_loss: 1.2950 - val_acc: 0.5800\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 1.4425 - acc: 0.4537 - val_loss: 1.2782 - val_acc: 0.5940\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 1.4455 - acc: 0.4460 - val_loss: 1.2613 - val_acc: 0.6070\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 1.4208 - acc: 0.4577 - val_loss: 1.2454 - val_acc: 0.6090\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 1.4300 - acc: 0.4511 - val_loss: 1.2336 - val_acc: 0.6140\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 1.4019 - acc: 0.4703 - val_loss: 1.2147 - val_acc: 0.6140\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 1.3772 - acc: 0.4729 - val_loss: 1.1957 - val_acc: 0.6310\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.3849 - acc: 0.4828 - val_loss: 1.1850 - val_acc: 0.6350\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.3501 - acc: 0.4897 - val_loss: 1.1632 - val_acc: 0.6410\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.3537 - acc: 0.4835 - val_loss: 1.1516 - val_acc: 0.6480\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.3441 - acc: 0.4909 - val_loss: 1.1396 - val_acc: 0.6520\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 1.3292 - acc: 0.4985 - val_loss: 1.1259 - val_acc: 0.6510\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.3203 - acc: 0.4996 - val_loss: 1.1143 - val_acc: 0.6600\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.3229 - acc: 0.4953 - val_loss: 1.1047 - val_acc: 0.6600\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 1.3057 - acc: 0.5063 - val_loss: 1.0916 - val_acc: 0.6690\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.2844 - acc: 0.5152 - val_loss: 1.0752 - val_acc: 0.6680\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.2642 - acc: 0.5212 - val_loss: 1.0623 - val_acc: 0.6690\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 1.2659 - acc: 0.5200 - val_loss: 1.0520 - val_acc: 0.6730\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 1.2696 - acc: 0.5196 - val_loss: 1.0454 - val_acc: 0.6830\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 1s 113us/step - loss: 1.2467 - acc: 0.5376 - val_loss: 1.0338 - val_acc: 0.6850\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 1s 120us/step - loss: 1.2430 - acc: 0.5361 - val_loss: 1.0210 - val_acc: 0.6850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 1s 107us/step - loss: 1.2211 - acc: 0.5369 - val_loss: 1.0097 - val_acc: 0.6960\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 1.2247 - acc: 0.5348 - val_loss: 0.9991 - val_acc: 0.6980\n",
      "Epoch 60/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.2161 - acc: 0.5412 - val_loss: 0.9915 - val_acc: 0.6940\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 1s 110us/step - loss: 1.2011 - acc: 0.5496 - val_loss: 0.9817 - val_acc: 0.6990\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 1s 101us/step - loss: 1.2014 - acc: 0.5515 - val_loss: 0.9756 - val_acc: 0.6980\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 1.1914 - acc: 0.5467 - val_loss: 0.9642 - val_acc: 0.7040\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 1s 106us/step - loss: 1.1785 - acc: 0.5596 - val_loss: 0.9548 - val_acc: 0.7050\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 1s 100us/step - loss: 1.1894 - acc: 0.5528 - val_loss: 0.9521 - val_acc: 0.7040\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 1.1732 - acc: 0.5543 - val_loss: 0.9424 - val_acc: 0.7080\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 1.1571 - acc: 0.5681 - val_loss: 0.9328 - val_acc: 0.7090\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 1s 104us/step - loss: 1.1563 - acc: 0.5611 - val_loss: 0.9249 - val_acc: 0.7120\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.1550 - acc: 0.5728 - val_loss: 0.9176 - val_acc: 0.7130\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 1s 103us/step - loss: 1.1305 - acc: 0.5768 - val_loss: 0.9079 - val_acc: 0.7150\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 1s 99us/step - loss: 1.1344 - acc: 0.5729 - val_loss: 0.9016 - val_acc: 0.7150\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 1.1381 - acc: 0.5789 - val_loss: 0.8968 - val_acc: 0.7100\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 1s 105us/step - loss: 1.1158 - acc: 0.5771 - val_loss: 0.8878 - val_acc: 0.7140\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 1s 105us/step - loss: 1.1168 - acc: 0.5819 - val_loss: 0.8822 - val_acc: 0.7150\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 1.1167 - acc: 0.5781 - val_loss: 0.8752 - val_acc: 0.7130\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 1.0939 - acc: 0.5855 - val_loss: 0.8681 - val_acc: 0.7180\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 1s 110us/step - loss: 1.1012 - acc: 0.5888 - val_loss: 0.8618 - val_acc: 0.7170\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 1.0869 - acc: 0.5907 - val_loss: 0.8574 - val_acc: 0.7190\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 1s 103us/step - loss: 1.0842 - acc: 0.5931 - val_loss: 0.8504 - val_acc: 0.7200\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 1s 104us/step - loss: 1.0960 - acc: 0.5827 - val_loss: 0.8492 - val_acc: 0.7200\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 1.0762 - acc: 0.5964 - val_loss: 0.8434 - val_acc: 0.7190\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 1.0661 - acc: 0.6001 - val_loss: 0.8344 - val_acc: 0.7240\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 1s 106us/step - loss: 1.0567 - acc: 0.6132 - val_loss: 0.8290 - val_acc: 0.7230\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 1s 116us/step - loss: 1.0675 - acc: 0.6008 - val_loss: 0.8252 - val_acc: 0.7220\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 1.0476 - acc: 0.6121 - val_loss: 0.8210 - val_acc: 0.7260\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 1.0472 - acc: 0.6063 - val_loss: 0.8145 - val_acc: 0.7240\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 1.0246 - acc: 0.6159 - val_loss: 0.8068 - val_acc: 0.7270\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 1.0267 - acc: 0.6215 - val_loss: 0.8037 - val_acc: 0.7230\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 1.0328 - acc: 0.6152 - val_loss: 0.8015 - val_acc: 0.7260\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 1.0357 - acc: 0.6088 - val_loss: 0.7976 - val_acc: 0.7300\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 1s 101us/step - loss: 1.0329 - acc: 0.6115 - val_loss: 0.7954 - val_acc: 0.7250\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 1.0189 - acc: 0.6180 - val_loss: 0.7888 - val_acc: 0.7280\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 1s 113us/step - loss: 1.0126 - acc: 0.6271 - val_loss: 0.7860 - val_acc: 0.7310\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 1s 105us/step - loss: 1.0040 - acc: 0.6245 - val_loss: 0.7827 - val_acc: 0.7330\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.0012 - acc: 0.6324 - val_loss: 0.7745 - val_acc: 0.7350\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 0.9994 - acc: 0.6233 - val_loss: 0.7706 - val_acc: 0.7370\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 0.9983 - acc: 0.6291 - val_loss: 0.7690 - val_acc: 0.7340\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 1s 107us/step - loss: 0.9910 - acc: 0.6343 - val_loss: 0.7656 - val_acc: 0.7390\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 1s 103us/step - loss: 1.0076 - acc: 0.6239 - val_loss: 0.7622 - val_acc: 0.7400\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 0.9787 - acc: 0.6404 - val_loss: 0.7589 - val_acc: 0.7380\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 0.9926 - acc: 0.6309 - val_loss: 0.7592 - val_acc: 0.7360\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 1s 97us/step - loss: 0.9769 - acc: 0.6332 - val_loss: 0.7531 - val_acc: 0.7390\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 0.9687 - acc: 0.6344 - val_loss: 0.7491 - val_acc: 0.7420\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 0.9749 - acc: 0.6357 - val_loss: 0.7453 - val_acc: 0.7430\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 1s 108us/step - loss: 0.9561 - acc: 0.6396 - val_loss: 0.7421 - val_acc: 0.7400\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 0.9669 - acc: 0.6425 - val_loss: 0.7413 - val_acc: 0.7420\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 1s 105us/step - loss: 0.9588 - acc: 0.6493 - val_loss: 0.7358 - val_acc: 0.7430\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 1s 103us/step - loss: 0.9425 - acc: 0.6407 - val_loss: 0.7322 - val_acc: 0.7440\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 1s 108us/step - loss: 0.9560 - acc: 0.6393 - val_loss: 0.7321 - val_acc: 0.7480\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 1s 106us/step - loss: 0.9467 - acc: 0.6535 - val_loss: 0.7311 - val_acc: 0.7460\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 0.9471 - acc: 0.6496 - val_loss: 0.7298 - val_acc: 0.7460\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 0.9422 - acc: 0.6491 - val_loss: 0.7246 - val_acc: 0.7510\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.9422 - acc: 0.6493 - val_loss: 0.7231 - val_acc: 0.7500\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 0.9375 - acc: 0.6573 - val_loss: 0.7213 - val_acc: 0.7470\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 1s 97us/step - loss: 0.9281 - acc: 0.6532 - val_loss: 0.7166 - val_acc: 0.7500\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 0.9259 - acc: 0.6577 - val_loss: 0.7153 - val_acc: 0.7490\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - ETA: 0s - loss: 0.9211 - acc: 0.664 - 1s 94us/step - loss: 0.9215 - acc: 0.6641 - val_loss: 0.7129 - val_acc: 0.7490\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.9234 - acc: 0.6608 - val_loss: 0.7122 - val_acc: 0.7490\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 0.9278 - acc: 0.6601 - val_loss: 0.7091 - val_acc: 0.7520\n",
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.9167 - acc: 0.6543 - val_loss: 0.7066 - val_acc: 0.7510\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.9059 - acc: 0.6635 - val_loss: 0.7028 - val_acc: 0.7510\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.9064 - acc: 0.6613 - val_loss: 0.7022 - val_acc: 0.7520\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.9084 - acc: 0.6641 - val_loss: 0.6994 - val_acc: 0.7530\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.9041 - acc: 0.6660 - val_loss: 0.6995 - val_acc: 0.7520\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.9054 - acc: 0.6668 - val_loss: 0.6981 - val_acc: 0.7500\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.8945 - acc: 0.6755 - val_loss: 0.6925 - val_acc: 0.7550\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.8996 - acc: 0.6673 - val_loss: 0.6914 - val_acc: 0.7550\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 0.8947 - acc: 0.6724 - val_loss: 0.6884 - val_acc: 0.7590\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 0.8930 - acc: 0.6732 - val_loss: 0.6870 - val_acc: 0.7540\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.8709 - acc: 0.6777 - val_loss: 0.6830 - val_acc: 0.7550\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8870 - acc: 0.6707 - val_loss: 0.6839 - val_acc: 0.7550\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 0.8902 - acc: 0.6701 - val_loss: 0.6844 - val_acc: 0.7540\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 0.8717 - acc: 0.6745 - val_loss: 0.6796 - val_acc: 0.7540\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 0.8831 - acc: 0.6685 - val_loss: 0.6799 - val_acc: 0.7550\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 1s 99us/step - loss: 0.8656 - acc: 0.6807 - val_loss: 0.6773 - val_acc: 0.7570\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 0.8798 - acc: 0.6789 - val_loss: 0.6772 - val_acc: 0.7560\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.8743 - acc: 0.6720 - val_loss: 0.6758 - val_acc: 0.7560\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8632 - acc: 0.6785 - val_loss: 0.6736 - val_acc: 0.7570\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.8666 - acc: 0.6793 - val_loss: 0.6709 - val_acc: 0.7560\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8706 - acc: 0.6777 - val_loss: 0.6693 - val_acc: 0.7570\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.8760 - acc: 0.6801 - val_loss: 0.6720 - val_acc: 0.7540\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8607 - acc: 0.6771 - val_loss: 0.6658 - val_acc: 0.7560\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8405 - acc: 0.6857 - val_loss: 0.6659 - val_acc: 0.7560\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8559 - acc: 0.6800 - val_loss: 0.6654 - val_acc: 0.7580\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8483 - acc: 0.6845 - val_loss: 0.6661 - val_acc: 0.7540\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8550 - acc: 0.6836 - val_loss: 0.6611 - val_acc: 0.7550\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8478 - acc: 0.6885 - val_loss: 0.6608 - val_acc: 0.7560\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 0.8411 - acc: 0.6891 - val_loss: 0.6582 - val_acc: 0.7600\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8479 - acc: 0.6831 - val_loss: 0.6551 - val_acc: 0.7600\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.8558 - acc: 0.6783 - val_loss: 0.6554 - val_acc: 0.7570\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8527 - acc: 0.6839 - val_loss: 0.6562 - val_acc: 0.7600\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8305 - acc: 0.6876 - val_loss: 0.6537 - val_acc: 0.7600\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8304 - acc: 0.6941 - val_loss: 0.6507 - val_acc: 0.7620\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8204 - acc: 0.6907 - val_loss: 0.6481 - val_acc: 0.7600\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8357 - acc: 0.6960 - val_loss: 0.6513 - val_acc: 0.7600\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8384 - acc: 0.6879 - val_loss: 0.6493 - val_acc: 0.7600\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8326 - acc: 0.6940 - val_loss: 0.6454 - val_acc: 0.7630\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 0.8226 - acc: 0.7015 - val_loss: 0.6453 - val_acc: 0.7620\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 0.8273 - acc: 0.6936 - val_loss: 0.6448 - val_acc: 0.7620\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 0.8189 - acc: 0.6921 - val_loss: 0.6443 - val_acc: 0.7610\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8143 - acc: 0.6953 - val_loss: 0.6415 - val_acc: 0.7630\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.8230 - acc: 0.6927 - val_loss: 0.6411 - val_acc: 0.7620\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8184 - acc: 0.6952 - val_loss: 0.6407 - val_acc: 0.7630\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.8020 - acc: 0.7013 - val_loss: 0.6400 - val_acc: 0.7630\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 0.8079 - acc: 0.6975 - val_loss: 0.6385 - val_acc: 0.7630\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 0.8108 - acc: 0.6900 - val_loss: 0.6397 - val_acc: 0.7640\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 0.8027 - acc: 0.6997 - val_loss: 0.6365 - val_acc: 0.7640\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8015 - acc: 0.7019 - val_loss: 0.6360 - val_acc: 0.7630\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8078 - acc: 0.7016 - val_loss: 0.6336 - val_acc: 0.7650\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8059 - acc: 0.6991 - val_loss: 0.6365 - val_acc: 0.7630\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.7945 - acc: 0.7032 - val_loss: 0.6312 - val_acc: 0.7630\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 0.8077 - acc: 0.6969 - val_loss: 0.6307 - val_acc: 0.7650\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8045 - acc: 0.6932 - val_loss: 0.6312 - val_acc: 0.7630\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 0.7826 - acc: 0.7017 - val_loss: 0.6295 - val_acc: 0.7650\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 0.7869 - acc: 0.7033 - val_loss: 0.6284 - val_acc: 0.7660\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.7851 - acc: 0.6995 - val_loss: 0.6276 - val_acc: 0.7610\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.7851 - acc: 0.7057 - val_loss: 0.6287 - val_acc: 0.7640\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.7856 - acc: 0.7108 - val_loss: 0.6264 - val_acc: 0.7620\n",
      "Epoch 179/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.7879 - acc: 0.7024 - val_loss: 0.6268 - val_acc: 0.7640\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.7730 - acc: 0.7141 - val_loss: 0.6257 - val_acc: 0.7660\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.7802 - acc: 0.7096 - val_loss: 0.6231 - val_acc: 0.7660\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.7790 - acc: 0.7081 - val_loss: 0.6231 - val_acc: 0.7660\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.7909 - acc: 0.7020 - val_loss: 0.6231 - val_acc: 0.7660\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.7638 - acc: 0.7191 - val_loss: 0.6215 - val_acc: 0.7660\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.7649 - acc: 0.7075 - val_loss: 0.6189 - val_acc: 0.7670\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.7750 - acc: 0.7072 - val_loss: 0.6197 - val_acc: 0.7670\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.7649 - acc: 0.7116 - val_loss: 0.6184 - val_acc: 0.7640\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.7788 - acc: 0.7112 - val_loss: 0.6176 - val_acc: 0.7670\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.7635 - acc: 0.7103 - val_loss: 0.6174 - val_acc: 0.7660\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.7548 - acc: 0.7124 - val_loss: 0.6148 - val_acc: 0.7690\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.7617 - acc: 0.7157 - val_loss: 0.6166 - val_acc: 0.7690\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.7523 - acc: 0.7205 - val_loss: 0.6154 - val_acc: 0.7700\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.7579 - acc: 0.7181 - val_loss: 0.6156 - val_acc: 0.7660\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.7549 - acc: 0.7164 - val_loss: 0.6128 - val_acc: 0.7680\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.7610 - acc: 0.7149 - val_loss: 0.6128 - val_acc: 0.7670\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.7526 - acc: 0.7169 - val_loss: 0.6139 - val_acc: 0.7690\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.7538 - acc: 0.7181 - val_loss: 0.6126 - val_acc: 0.7700\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.7504 - acc: 0.7248 - val_loss: 0.6105 - val_acc: 0.7740\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.7548 - acc: 0.7107 - val_loss: 0.6111 - val_acc: 0.7720\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.7506 - acc: 0.7179 - val_loss: 0.6112 - val_acc: 0.7700\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 47us/step\n",
      "1500/1500 [==============================] - 0s 49us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.47081585088570915, 0.8266666666984558]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6240548561414083, 0.7639999995231629]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 2s 71us/step - loss: 1.9052 - acc: 0.1929 - val_loss: 1.8708 - val_acc: 0.2233\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 1.8199 - acc: 0.2702 - val_loss: 1.7655 - val_acc: 0.3320\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 1.6846 - acc: 0.3905 - val_loss: 1.5963 - val_acc: 0.4557\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 1.4988 - acc: 0.5019 - val_loss: 1.3930 - val_acc: 0.5587\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 1.3038 - acc: 0.5838 - val_loss: 1.2064 - val_acc: 0.6160\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 1.1391 - acc: 0.6375 - val_loss: 1.0640 - val_acc: 0.6470\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 1.0124 - acc: 0.6708 - val_loss: 0.9547 - val_acc: 0.6777\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.9169 - acc: 0.6965 - val_loss: 0.8774 - val_acc: 0.6983\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.8454 - acc: 0.7148 - val_loss: 0.8180 - val_acc: 0.7150\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 2s 51us/step - loss: 0.7916 - acc: 0.7266 - val_loss: 0.7744 - val_acc: 0.7260\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 2s 53us/step - loss: 0.7507 - acc: 0.7368 - val_loss: 0.7429 - val_acc: 0.7353\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.7185 - acc: 0.7471 - val_loss: 0.7181 - val_acc: 0.7350\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.6925 - acc: 0.7542 - val_loss: 0.6988 - val_acc: 0.7437\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.6709 - acc: 0.7608 - val_loss: 0.6814 - val_acc: 0.7470\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.6529 - acc: 0.7668 - val_loss: 0.6694 - val_acc: 0.7533\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.6372 - acc: 0.7711 - val_loss: 0.6559 - val_acc: 0.7560\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 2s 51us/step - loss: 0.6232 - acc: 0.7769 - val_loss: 0.6447 - val_acc: 0.7613\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.6104 - acc: 0.7801 - val_loss: 0.6373 - val_acc: 0.7640\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 2s 53us/step - loss: 0.5991 - acc: 0.7856 - val_loss: 0.6322 - val_acc: 0.7670\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 2s 53us/step - loss: 0.5890 - acc: 0.7895 - val_loss: 0.6214 - val_acc: 0.7700\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.5794 - acc: 0.7911 - val_loss: 0.6146 - val_acc: 0.7723\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.5706 - acc: 0.7951 - val_loss: 0.6091 - val_acc: 0.7717\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.5617 - acc: 0.7984 - val_loss: 0.6031 - val_acc: 0.7723\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.5539 - acc: 0.8020 - val_loss: 0.6023 - val_acc: 0.7737\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.5468 - acc: 0.8039 - val_loss: 0.5939 - val_acc: 0.7797\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.5397 - acc: 0.8082 - val_loss: 0.5900 - val_acc: 0.7783\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.5329 - acc: 0.8100 - val_loss: 0.5859 - val_acc: 0.7807\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.5267 - acc: 0.8125 - val_loss: 0.5863 - val_acc: 0.7817\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.5208 - acc: 0.8145 - val_loss: 0.5778 - val_acc: 0.7813\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.5145 - acc: 0.8175 - val_loss: 0.5766 - val_acc: 0.7837\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 2s 67us/step - loss: 0.5090 - acc: 0.8184 - val_loss: 0.5729 - val_acc: 0.7833\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 2s 65us/step - loss: 0.5037 - acc: 0.8212 - val_loss: 0.5707 - val_acc: 0.7850\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 2s 67us/step - loss: 0.4984 - acc: 0.8233 - val_loss: 0.5669 - val_acc: 0.7843\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.4934 - acc: 0.8254 - val_loss: 0.5648 - val_acc: 0.7883\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.4886 - acc: 0.8274 - val_loss: 0.5621 - val_acc: 0.7883\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.4845 - acc: 0.8278 - val_loss: 0.5596 - val_acc: 0.7883\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.4797 - acc: 0.8293 - val_loss: 0.5624 - val_acc: 0.7900\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.4755 - acc: 0.8313 - val_loss: 0.5570 - val_acc: 0.7897\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.4712 - acc: 0.8335 - val_loss: 0.5552 - val_acc: 0.7893\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.4670 - acc: 0.8353 - val_loss: 0.5550 - val_acc: 0.7950\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.4636 - acc: 0.8363 - val_loss: 0.5523 - val_acc: 0.7950\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.4599 - acc: 0.8376 - val_loss: 0.5521 - val_acc: 0.7950\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.4562 - acc: 0.8388 - val_loss: 0.5500 - val_acc: 0.7983\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.4524 - acc: 0.8409 - val_loss: 0.5477 - val_acc: 0.7963\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.4490 - acc: 0.8417 - val_loss: 0.5462 - val_acc: 0.7973\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.4454 - acc: 0.8434 - val_loss: 0.5483 - val_acc: 0.8000\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.4425 - acc: 0.8450 - val_loss: 0.5495 - val_acc: 0.7997\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.4392 - acc: 0.8470 - val_loss: 0.5473 - val_acc: 0.7973\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.4362 - acc: 0.8469 - val_loss: 0.5458 - val_acc: 0.7983\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.4330 - acc: 0.8478 - val_loss: 0.5458 - val_acc: 0.8000\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.4303 - acc: 0.8491 - val_loss: 0.5459 - val_acc: 0.8007\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.4277 - acc: 0.8501 - val_loss: 0.5437 - val_acc: 0.8007\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.4244 - acc: 0.8512 - val_loss: 0.5420 - val_acc: 0.8033\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.4220 - acc: 0.8525 - val_loss: 0.5450 - val_acc: 0.8003\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.4192 - acc: 0.8534 - val_loss: 0.5429 - val_acc: 0.8010\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.4165 - acc: 0.8548 - val_loss: 0.5444 - val_acc: 0.8030\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.4140 - acc: 0.8559 - val_loss: 0.5439 - val_acc: 0.8017\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.4114 - acc: 0.8570 - val_loss: 0.5444 - val_acc: 0.8040\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 2s 53us/step - loss: 0.4091 - acc: 0.8575 - val_loss: 0.5431 - val_acc: 0.8030\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.4069 - acc: 0.8588 - val_loss: 0.5440 - val_acc: 0.8010\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.4045 - acc: 0.8598 - val_loss: 0.5410 - val_acc: 0.8023\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.4019 - acc: 0.8596 - val_loss: 0.5449 - val_acc: 0.8023\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.4000 - acc: 0.8618 - val_loss: 0.5468 - val_acc: 0.7997\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3976 - acc: 0.8625 - val_loss: 0.5440 - val_acc: 0.8020\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3953 - acc: 0.8627 - val_loss: 0.5464 - val_acc: 0.8043\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3929 - acc: 0.8639 - val_loss: 0.5442 - val_acc: 0.8050\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3910 - acc: 0.8650 - val_loss: 0.5429 - val_acc: 0.8030\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3892 - acc: 0.8654 - val_loss: 0.5467 - val_acc: 0.8010\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3873 - acc: 0.8653 - val_loss: 0.5476 - val_acc: 0.8030\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.3850 - acc: 0.8670 - val_loss: 0.5450 - val_acc: 0.8013\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3832 - acc: 0.8672 - val_loss: 0.5457 - val_acc: 0.8027\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3810 - acc: 0.8679 - val_loss: 0.5459 - val_acc: 0.8043\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3791 - acc: 0.8699 - val_loss: 0.5483 - val_acc: 0.8027\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3776 - acc: 0.8701 - val_loss: 0.5481 - val_acc: 0.8023\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3754 - acc: 0.8704 - val_loss: 0.5493 - val_acc: 0.8037\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3736 - acc: 0.8717 - val_loss: 0.5498 - val_acc: 0.8023\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3719 - acc: 0.8714 - val_loss: 0.5513 - val_acc: 0.8033\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3701 - acc: 0.8723 - val_loss: 0.5497 - val_acc: 0.8037\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3679 - acc: 0.8727 - val_loss: 0.5487 - val_acc: 0.8027\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3668 - acc: 0.8728 - val_loss: 0.5532 - val_acc: 0.8040\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3650 - acc: 0.8738 - val_loss: 0.5518 - val_acc: 0.8047\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3635 - acc: 0.8743 - val_loss: 0.5550 - val_acc: 0.8050\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3616 - acc: 0.8758 - val_loss: 0.5555 - val_acc: 0.8020\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3599 - acc: 0.8759 - val_loss: 0.5556 - val_acc: 0.8057\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3584 - acc: 0.8768 - val_loss: 0.5556 - val_acc: 0.8047\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3571 - acc: 0.8764 - val_loss: 0.5539 - val_acc: 0.8043\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3548 - acc: 0.8776 - val_loss: 0.5575 - val_acc: 0.8053\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3537 - acc: 0.8781 - val_loss: 0.5579 - val_acc: 0.8027\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3518 - acc: 0.8784 - val_loss: 0.5571 - val_acc: 0.8043\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3507 - acc: 0.8797 - val_loss: 0.5601 - val_acc: 0.8060\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3490 - acc: 0.8796 - val_loss: 0.5621 - val_acc: 0.8063\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3475 - acc: 0.8800 - val_loss: 0.5614 - val_acc: 0.8073\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3461 - acc: 0.8812 - val_loss: 0.5604 - val_acc: 0.8023\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3446 - acc: 0.8817 - val_loss: 0.5668 - val_acc: 0.8043\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3430 - acc: 0.8811 - val_loss: 0.5637 - val_acc: 0.8027\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3414 - acc: 0.8826 - val_loss: 0.5637 - val_acc: 0.8033\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3404 - acc: 0.8835 - val_loss: 0.5682 - val_acc: 0.8043\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3389 - acc: 0.8835 - val_loss: 0.5666 - val_acc: 0.8057\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3376 - acc: 0.8836 - val_loss: 0.5686 - val_acc: 0.8057\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3361 - acc: 0.8843 - val_loss: 0.5660 - val_acc: 0.8050\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3349 - acc: 0.8835 - val_loss: 0.5683 - val_acc: 0.8027\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3333 - acc: 0.8853 - val_loss: 0.5698 - val_acc: 0.8050\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3319 - acc: 0.8858 - val_loss: 0.5712 - val_acc: 0.8033\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3305 - acc: 0.8858 - val_loss: 0.5719 - val_acc: 0.8073\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3290 - acc: 0.8871 - val_loss: 0.5759 - val_acc: 0.8033\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 2s 51us/step - loss: 0.3284 - acc: 0.8868 - val_loss: 0.5736 - val_acc: 0.8017\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3266 - acc: 0.8881 - val_loss: 0.5753 - val_acc: 0.8050\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3255 - acc: 0.8882 - val_loss: 0.5779 - val_acc: 0.8027\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 2s 51us/step - loss: 0.3239 - acc: 0.8891 - val_loss: 0.5810 - val_acc: 0.8057\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3227 - acc: 0.8887 - val_loss: 0.5800 - val_acc: 0.8033\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3214 - acc: 0.8904 - val_loss: 0.5809 - val_acc: 0.8040\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3204 - acc: 0.8902 - val_loss: 0.5817 - val_acc: 0.8043\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3189 - acc: 0.8906 - val_loss: 0.5915 - val_acc: 0.8020\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3182 - acc: 0.8903 - val_loss: 0.5817 - val_acc: 0.8007\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3166 - acc: 0.8919 - val_loss: 0.5852 - val_acc: 0.8043\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3158 - acc: 0.8923 - val_loss: 0.5861 - val_acc: 0.8000\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3141 - acc: 0.8925 - val_loss: 0.5880 - val_acc: 0.8020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 2s 53us/step - loss: 0.3130 - acc: 0.8931 - val_loss: 0.5933 - val_acc: 0.8037\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.3117 - acc: 0.8926 - val_loss: 0.5914 - val_acc: 0.8037\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 2s 53us/step - loss: 0.3107 - acc: 0.8928 - val_loss: 0.5914 - val_acc: 0.8047\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 43us/step\n",
      "4000/4000 [==============================] - 0s 49us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.30549447350068526, 0.8967272727272727]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5640837223529815, 0.81075]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
